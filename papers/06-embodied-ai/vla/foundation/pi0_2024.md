# π₀：基于 Flow Matching 的通用机器人 VLA 基础模型——原理详解

> 论文：*π₀: A Vision-Language-Action Flow Model for General Robot Control*
> 机构：Physical Intelligence（π）
> 发布时间：2024年10月
> 🔗 [arXiv](https://arxiv.org/abs/2410.24164) | [PDF](https://arxiv.org/pdf/2410.24164) | [项目主页](https://physicalintelligence.company/blog/pi0)

---

## 一、论文要解决什么问题？

### 1.1 机器人学习的核心困境

当前机器人学习面临三大瓶颈：

1. **数据稀缺**：高质量机器人操作数据的采集极其昂贵，单个任务通常只有几十到几百条轨迹
2. **泛化不足**：专门为某个任务训练的策略，换个场景或换个物体就失效
3. **鲁棒性差**：策略在遇到训练分布外的情况时，缺乏恢复和纠错能力

NLP 和计算机视觉早已证明了一个有效的解决范式：**先在海量多样数据上预训练基础模型，再针对具体任务微调**。比如识别鸟类照片，与其只用鸟的图片训练，不如先在整个 ImageNet 上预训练，再微调——泛化性反而更好。

那么问题来了：**能不能用同样的范式来构建机器人基础模型？**

### 1.2 现有 VLA 模型的局限

之前的 VLA 模型（如 RT-2、OpenVLA）的做法是：把动作离散化成 token，用自回归的方式生成动作——本质上就像"用语言模型说出动作"。这种设计存在根本性的缺陷：

- **无法生成高频动作**：自回归逐 token 解码太慢，通常只能以 2-10 Hz 控制机器人
- **无法表达 action chunk**：精细灵巧的操作（如叠衣服）需要一次预测未来 50 步动作的能力
- **丢失连续性**：将连续动作离散化会损失精度，对高精度操作致命

### 1.3 π₀ 的核心思路

π₀ 提出了一个新的范式：**Flow Matching VLA**——用 flow matching（扩散的一种变体）替代自回归来生成连续动作。具体来说：

1. **继承 VLM 的知识**：以预训练的视觉-语言模型 PaliGemma（3B 参数）为骨架，继承互联网规模的语义知识
2. **添加动作专家**：增加一个独立的 action expert 模块（300M 参数），通过 flow matching 生成连续动作分布
3. **大规模跨构型预训练**：在超过 10,000 小时的机器人数据上训练，覆盖 7 种不同构型的机器人和 68 个任务
4. **预训练 + 后训练**：借鉴 LLM 的训练范式，先预训练获得广泛能力，再后训练获得具体任务的精通

这使得 π₀ 能以 **50 Hz** 输出高频动作，完成折衣服、组装纸箱、打包鸡蛋等此前 VLA 模型无法完成的高度灵巧任务。

---

## 二、预备知识

### 2.1 Flow Matching 基础

Flow matching 是一种生成模型方法，可以理解为扩散模型的"直线版"。核心思想是学习一个向量场，将纯噪声"推"向真实数据。

**前向过程**（加噪）：对真实动作 $\mathbf{A}_t$ 加入噪声 $\epsilon \sim \mathcal{N}(0, \mathbf{I})$，沿直线插值：

$$\mathbf{A}_t^\tau = \tau \mathbf{A}_t + (1 - \tau)\epsilon, \quad \tau \in [0, 1]$$

- $\tau = 0$：纯噪声
- $\tau = 1$：完全干净的真实动作
- 中间值：噪声和真实动作的线性混合

**训练目标**：让网络学习去噪向量场 $\mathbf{u}(\mathbf{A}_t^\tau | \mathbf{A}_t) = \epsilon - \mathbf{A}_t$，训练损失为：

$$\mathcal{L}^\tau(\theta) = \mathbb{E}_{p(\mathbf{A}_t | o_t), q(\mathbf{A}_t^\tau | \mathbf{A}_t)} \left[\|\mathbf{v}_\theta(\mathbf{A}_t^\tau, o_t) - \mathbf{u}(\mathbf{A}_t^\tau | \mathbf{A}_t)\|^2\right]$$

**推理过程**（去噪）：从纯噪声 $\mathbf{A}_t^0 \sim \mathcal{N}(0, \mathbf{I})$ 出发，用欧拉积分逐步去噪：

$$\mathbf{A}_t^{\tau + \delta} = \mathbf{A}_t^\tau + \delta \cdot \mathbf{v}_\theta(\mathbf{A}_t^\tau, o_t)$$

π₀ 使用 10 步积分（$\delta = 0.1$），即 10 次前向传播即可从噪声生成干净的动作序列。

**与扩散模型的区别**：扩散模型走的是曲线路径（需要更多步），flow matching 走直线（更高效）。在高分辨率图像和视频生成中，flow matching 已展现出优异的实证性能。

### 2.2 Action Chunking

传统的机器人策略是逐步预测单个动作 $a_t$，而 action chunking 是一次预测未来 $H$ 步的动作序列：

$$\mathbf{A}_t = [a_t, a_{t+1}, \dots, a_{t+H-1}]$$

π₀ 使用 $H = 50$，意味着每次推理一次性给出未来 50 步的动作计划。

**为什么需要 action chunking？**

- **时间一致性**：避免逐步预测导致的动作抖动
- **高频控制**：生成 50 步动作后，机器人可以 50 Hz 执行，不需要每步都等推理
- **多模态表达**：对同一个任务可能有多种合理的执行策略，action chunk 能更自然地表达这种多模态性

### 2.3 Transfusion 架构思想

π₀ 的架构灵感来源于 Transfusion——一种在同一个 Transformer 中同时使用两种训练目标的方法：

- **离散 token**（文本、语言指令）→ 用交叉熵损失
- **连续 token**（动作、状态）→ 用 flow matching 损失

这样一个模型就能同时处理离散的语言理解和连续的动作生成，不需要将动作强行离散化。

---

## 三、方法论详解

### 3.1 模型架构

π₀ 的架构由两大组件组成：**VLM 骨架** + **动作专家**。

#### 3.1.1 输入表示

观测 $o_t$ 包含三类信息：

$$o_t = [I_t^1, \dots, I_t^n, \ell_t, q_t]$$

- $I_t^i$：第 $i$ 个摄像头的 RGB 图像（2-3 个视角）
- $\ell_t$：语言指令的 token 序列
- $q_t$：机器人本体感知状态（关节角度向量）

图像通过 ViT 编码器映射到与语言 token 相同的嵌入空间。本体感知状态 $q_t$ 通过线性投影层映射。

#### 3.1.2 双专家设计（VLM 骨架 + Action Expert）

这是 π₀ 最关键的架构创新。整个模型是一个 Transformer，但使用**两套权重**：

| 组件 | 处理的 token | 参数量 | 初始化方式 |
| --- | --- | --- | --- |
| VLM 骨架 | 图像 $I_t^i$ + 语言 $\ell_t$ | 3B（PaliGemma） | 从预训练 VLM 加载 |
| 动作专家 | 本体感知 $q_t$ + 噪声动作 $\mathbf{A}_t^\tau$ | 300M | 随机初始化 |

两套权重**仅通过 self-attention 层交互**——在注意力计算时，动作 token 可以看到图像和语言 token 的 key/value，从而获取视觉和语义信息。但各自的 FFN 层完全独立。

**为什么不共享权重？**

论文发现，如果让机器人特有的 token（动作、本体感知）走 VLM 的全部权重，会干扰 VLM 在预训练中学到的表征。用独立的权重既保护了 VLM 的知识，又让动作生成有自己的专属容量。这类似于 **Mixture of Experts（MoE）** 的思想——两个专家，一个处理视觉-语言，一个处理动作。

#### 3.1.3 注意力掩码设计

π₀ 使用**分块因果注意力掩码**，将输入序列分为三个块：

1. **块 1**：$[I_t^1, \dots, I_t^n, \ell_t]$ — 图像 + 语言（块内双向注意力）
2. **块 2**：$[q_t]$ — 本体感知状态
3. **块 3**：$[a_t^\tau, \dots, a_{t+H-1}^\tau]$ — 噪声动作（块内双向注意力）

规则：每个块只能看到自己和之前的块，不能看到后面的块。

设计意图：

- 块 1 是 VLM 预训练时见过的输入格式，不让它看到后面的新输入类型，**最小化分布偏移**
- 块 2（本体感知）在 flow matching 的多步积分中不变，将其隔离后可以**缓存 key/value**，避免重复计算
- 块 3（动作 token）可以看到完整的输入序列，获取所有必要信息

#### 3.1.4 Flow Matching 时间步编码

每个噪声动作 $a_t^\tau$ 通过一个 MLP 映射到 Transformer 的嵌入维度，同时融入 flow matching 时间步 $\tau$ 的信息：

$$\text{embedding} = W_3 \cdot \text{swish}(W_2 \cdot \text{concat}(W_1 \cdot a_t^\tau, \phi(\tau)))$$

其中 $\phi(\tau)$ 是正弦位置编码函数，将标量 $\tau$ 映射到高维空间。

#### 3.1.5 时间步采样策略

论文发现，对机器人动作预测来说，**低 $\tau$（高噪声）** 的时间步更重要。原因是：

- 在图像生成中，观测（文本标签）对可能的输出（图像）约束很弱，有很多合理图像
- 在动作预测中，观测（机器人视角画面）对可能的动作约束很强，合理动作的范围窄得多
- 因此模型需要在高噪声区域（$\tau$ 接近 0）花更多精力学习如何"从噪声中找到正确方向"

π₀ 使用 shifted Beta 分布来采样 $\tau$：

$$p(\tau) = \text{Beta}\left(\frac{s - \tau}{s}; 1.5, 1\right), \quad s = 0.999$$

这个分布偏向低 $\tau$ 值，且在 $\tau > s$ 时直接截断不采样（因为这些极低噪声的时间步对推理不重要）。

### 3.2 推理流程

推理时的计算流程：

1. **图像编码**（3 个摄像头）：14 ms
2. **观测前向传播**（VLM 骨架处理图像 + 语言）：32 ms
3. **10 步 flow matching 积分**（动作专家，每步约 2.7 ms）：27 ms
4. **总计**：~73 ms（本地推理）

**关键优化**：在 10 步积分中，只有动作 token 在变化，观测 token 的 key/value 被缓存复用。因此 10 步积分只需 27 ms，而非 320 ms。

**动作执行策略**：生成一个 50 步的 action chunk 后，直接开环执行一部分（50 Hz 机器人执行 25 步 = 0.5 秒，20 Hz 机器人执行 16 步 = 0.8 秒），然后再次推理。论文尝试过时序集成（temporal ensembling）但发现反而损害性能，因此选择简单的开环执行。

### 3.3 跨构型训练

π₀ 在 7 种不同构型的机器人上联合训练：

| 构型 | 手臂 | 自由度 | 特点 |
| --- | --- | --- | --- |
| UR5e | 单臂 | 7 | 2 个摄像头，平行夹爪 |
| 双臂 UR5e | 双臂 | 14 | 3 个摄像头 |
| Franka | 单臂 | 8 | 2 个摄像头 |
| 双臂 Trossen | 双臂 | 14 | 基于 ALOHA 平台 |
| 双臂 ARX | 双臂 | 14 | 3 个摄像头 |
| 移动 Trossen/ARX | 双臂+移动底盘 | 16 | 非全向底盘 |
| 移动 Fibocom | 双臂+全向底盘 | 17 | 3 维底盘控制 |

**统一动作空间**：所有机器人的动作向量统一为最大维度（18 维），低维度的机器人用零填充。图像不足 3 个的用 mask 屏蔽。

**数据权重平衡**：由于各任务数据量差异很大，对每个任务-机器人组合的权重设为 $n^{0.43}$（$n$ 是样本数），抑制过采样的大数据集，提升小数据集的影响。

### 3.4 预训练与后训练

这是 π₀ 借鉴 LLM 训练范式的核心设计：

**预训练阶段**：

- 数据：约 10,000 小时的机器人数据（自有 903M 步 + OXE 开源数据 9.1%）
- 目标：让模型获得广泛的物理操作能力，能处理多种机器人和多种任务
- 语言标签：混合使用任务名称和细粒度片段标注（约 2 秒一个子轨迹标签）
- 结果：base 模型能 zero-shot 执行多种任务，但不一定精通

**后训练阶段**：

- 数据：少量高质量的任务特定数据（简单任务 5 小时，复杂任务 100+ 小时）
- 目标：让模型在特定任务上达到精通水平
- 类比：就像 LLM 的 alignment——预训练给知识，后训练给行为模式

**为什么需要两阶段？**

- 只用高质量数据：模型脆弱，一旦犯错就不会恢复（因为训练数据中没有错误和纠正）
- 只用预训练数据：模型能做但不精通，动作不够流畅和高效
- 两者结合：预训练数据提供多样性和纠错能力，后训练数据提供流畅和高效的执行策略

---

## 四、实验设置

### 4.1 评估体系

π₀ 的实验分为四个层次，层层递进：

1. **Base 模型直接评估**：预训练后不做后训练，直接用语言命令执行任务
2. **语言指令跟随**：评估模型理解和执行细粒度语言命令的能力
3. **新任务微调**：在新任务上用少量数据微调，评估迁移效率
4. **复杂多阶段任务**：挑战叠衣服、组装纸箱等需要数十分钟的超长任务

### 4.2 对比方法

| 方法 | 类型 | 参数量 | 动作生成方式 |
| --- | --- | --- | --- |
| OpenVLA | VLA | 7B | 自回归离散化 |
| Octo | 非 VLA | 93M | 扩散 |
| ACT | 任务专用 | — | action chunking |
| Diffusion Policy | 任务专用 | — | 扩散 |
| π₀-small | 非 VLA 基线 | 470M | flow matching（无 VLM 初始化） |

### 4.3 评估指标

每个任务设计了细粒度的里程碑式评分标准（满分 1.0），按子目标给予部分分数。每个任务每种方法评估 10 次。

---

## 五、实验结果

### 5.1 Base 模型 Zero-Shot 表现

预训练后直接用语言命令评估 5 个任务：折 T 恤、整理桌面（易/难）、装袋杂货、取吐司。

**关键结果**：

- π₀ 在所有任务和所有对比中大幅领先
- 即使只训练 160k 步（"计算对等"版本），仍优于所有基线
- OpenVLA 几乎无法完成这些任务——自回归离散化不支持 action chunk，是根本性的架构缺陷
- Octo 支持 action chunk 但表征容量有限，表现也不佳

### 5.2 语言指令跟随

在整理桌面、摆放餐具、装袋杂货三个任务上，对比 π₀ 与无 VLM 初始化的 π₀-small：

| 评估方式 | π₀ | π₀-small |
| --- | --- | --- |
| 平坦命令（仅任务描述） | 中等 | 中等 |
| 人类专家分步命令 | 显著提升 | 几乎无提升 |
| 高级 VLM 策略命令 | 明显提升 | 无提升 |

**关键发现**：π₀ 的语言跟随准确率远高于 π₀-small，这直接来源于 VLM 预训练。π₀-small 即使收到详细的分步指令也无法有效利用——语言理解能力是 VLM 骨架带来的核心优势。

### 5.3 新任务微调

在 5 个新任务上用不同数据量（1/5/10 小时）微调，对比多种方法：

**关键发现**：

1. **π₀ 总体最优**：在几乎所有任务和数据量条件下胜出
2. **预训练的价值**：π₀（从预训练初始化）vs π₀（从头训练），前者通常高出 2 倍成功率
3. **与任务相似度相关**：和预训练数据越相似的任务（如叠毛巾 vs 预训练中的叠衬衫），预训练的增益越大
4. **少数据优势明显**：1 小时数据下，π₀ 远超所有基线；5-10 小时时差距缩小但仍领先
5. **专用模型反而更强**：令人意外的是，ACT 和 Diffusion Policy（从头在任务数据上训练）比 OpenVLA 和 Octo（预训练 + 微调）更强，说明利用预训练对先前方法来说仍是一个未解决的挑战

### 5.4 复杂多阶段任务

这是 π₀ 最令人印象深刻的实验——长达 5-20 分钟的超复杂任务：

| 任务 | 描述 | 预训练+微调 | 从头训练 | Zero-Shot |
| --- | --- | --- | --- | --- |
| 叠衣服 | 从随机皱褶状态取出、展平、折叠 | 最优 | 差 | 中等 |
| 移动叠衣 | 移动底盘 + 叠衣 | 最优 | 差 | 较差 |
| 整理桌面 | 分拣 12 种物品到正确容器 | 最优 | 差 | 中等 |
| 组装纸箱 | 折叠纸板、关合翻盖 | 中优 | 很差 | — |
| 打包鸡蛋 | 6 个鸡蛋放入蛋托并关盖 | 中优 | 较差 | — |
| 打包外卖 | 食物装入外卖盒并关盖 | 中优 | 很差 | — |

**关键发现**：

- 预训练 + 微调的完整配方始终最优，尤其在最难的任务上优势最大
- 从头训练在这些复杂任务上几乎无法工作，说明预训练提供的广泛操作基础不可或缺
- π₀ 展示了端到端学习文献中**最长的灵巧操作任务**

---

## 六、用类比总结 π₀ 的核心原理

想象你在培养一个全能厨师：

**传统 VLA（OpenVLA）**：厨师只会按食谱一个字一个字地"念"出动作——"拿、起、刀、切、下、去"。说话太慢（低频控制），还经常念串了（离散化误差），对需要连贯手法的复杂菜品（灵巧操作）无能为力。

**专用模型（ACT/Diffusion Policy）**：为每道菜从零开始培训一个专门的厨师。炒蛋专家、煎牛排专家……每个人很厉害，但换一道菜就得重新培训。而且因为只见过正确操作，一旦失手（比如鸡蛋掉了）就不知道怎么补救。

**π₀ 的做法**：先让厨师在全世界各种厨房实习（VLM 预训练获取常识 + 大规模跨构型预训练获取操作基础），见识过各种菜系、各种厨具、各种失误和补救方式。然后针对目标餐厅的特色菜做一段精细培训（后训练）。这样培养出的厨师既有广博的基础功（能快速上手新菜品），又有精湛的拿手菜（后训练的核心任务），还能在出错时随机应变（预训练数据中见过各种恢复行为）。

而 flow matching 就像是这位厨师的"行云流水"技能——不是一个动作一个动作地想，而是**一气呵成地规划整套手法**（50 步 action chunk），手起刀落干净利落（50 Hz 高频控制）。

---

## 七、局限性与未来方向

### 7.1 数据组成的理解不足

论文将所有可用数据混合在一起，但并不清楚哪些类型的数据贡献最大。不同任务和不同机器人之间是否存在正向迁移，还是某些数据实际上在"拖后腿"？这仍是一个开放问题。

### 7.2 任务性能的不可预测性

并非所有任务都能可靠地工作。目前缺乏一种方法来预测：对于某个目标任务，需要多少以及什么类型的数据才能达到接近完美的表现。

### 7.3 跨领域迁移的未知性

π₀ 验证了在灵巧操作域内的跨构型迁移，但能否进一步扩展到自动驾驶、四足运动、导航等更异质的领域，有待未来探索。

### 7.4 模型规模与推理效率

π₀ 使用 3.3B 参数模型实现了 ~73 ms 的推理时间（RTX 4090），对实时控制来说足够。但随着任务复杂度提升，模型需要更大规模，如何在保持实时性的同时提升表达能力，是一个持续的工程挑战。
