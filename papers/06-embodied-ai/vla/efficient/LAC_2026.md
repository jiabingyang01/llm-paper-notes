# LAC：通过自适应视觉 Token 缓存加速 VLA 模型——原理详解

> 论文：*Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching*
> 机构：哈尔滨工业大学、哈尔滨工业大学（深圳）、美团机器人研究院、中南大学、HKUST(GZ)
> 发布时间：2026年1月
> 🔗 [arXiv](https://arxiv.org/abs/2602.00686) | [PDF](https://arxiv.org/pdf/2602.00686)

---

## 一、论文要解决什么问题？

### 1.1 VLA 模型的推理瓶颈

VLA（Vision-Language-Action）模型在机器人操控任务中展现了强大的泛化能力，但它们的**计算开销极大**，严重阻碍了实际部署。核心矛盾在于：

- 机器人实时控制需要**低延迟**（通常要求数十毫秒级别的响应）
- VLA 模型（如 OpenVLA 7B、CogAct）每一步都要对**完整的视觉输入**重新编码，即使连续帧之间绝大部分区域（如静态背景）并没有变化

这种对**视觉冗余**的重复计算是推理延迟的主要来源。

### 1.2 现有加速方法的局限

已有的加速策略主要分为两类，但都存在根本性缺陷：

**Token 剪枝/合并方法**（SparseVLM、FastV 等）：
- 直接丢弃"不重要"的 token，但判断标准是**与任务无关的启发式规则**
- 可能丢弃对操控任务至关重要的空间细节
- 引入的剪枝逻辑本身也有计算开销，对于机器人短动作序列，往往得不偿失

**静态缓存方法**（VLA-Cache 等）：
- 使用固定的基于规则的策略决定哪些 token 可以缓存复用
- **无法适应动态变化的场景**——比如机器人刚开始时篮子是静止的，但马上就要与之交互
- 判断依据（如注意力分数）与实际任务需求**不对齐**：模型注意力高的地方不一定是任务真正需要的

### 1.3 LAC 的核心洞察

LAC 的核心思路可以用一句话概括：**将推理加速从"人工设计规则"变成"可学习的策略优化问题"**。

具体来说，LAC 认为：

> **模型注意到的地方 ≠ 任务需要的地方**。最优的缓存策略应该直接由任务损失函数驱动，而非依赖间接代理指标。

LAC 提出了两个轻量级协作模块：
1. **Cached Token Selector**（缓存 Token 选择器）：学习哪些 token 需要重新计算、哪些可以复用
2. **Cache Ratio Predictor**（缓存比例预测器）：学习当前场景应该缓存多大比例的 token

这两个模块通过任务损失函数 $\nabla \mathcal{L}_{\text{VLA}}$ 进行端到端优化，确保每一个计算分配决策都直接服务于最终的任务成功率。

**最终效果**：在 LIBERO 基准上实现 **1.76× 推理加速**，同时成功率**提升 1.9 个百分点**（75.0% → 76.9%）——加速的同时性能反而更好了。

---

## 二、预备知识

### 2.1 VLA 模型中的 KV 缓存

在自回归 Transformer 模型中，KV（Key-Value）缓存是标准的推理加速技术。对于输入序列 $X = [x_1, x_2, \dots, x_T]$，自注意力机制计算：

$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

$$\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V$$

推理时，只需计算新 token 的 $k_{\text{new}}$ 和 $v_{\text{new}}$，追加到缓存中：

$$K_t = [K_{t-1}, k_{\text{new}}], \quad V_t = [V_{t-1}, v_{\text{new}}]$$

**问题**：这种缓存只作用于**同一上下文窗口内**的解码步。而 VLA 模型在**每个时间步**都会收到新的视觉帧，需要重新编码完整的视觉输入。连续帧之间的大量冗余完全没有被利用。

### 2.2 视觉 Token 的时间冗余

机器人操控场景有一个重要特性：**连续帧之间高度相似**。

- 背景、桌面、固定物体在短时间内几乎不变
- 真正在变化的只有**机器人末端执行器**及其**交互对象**附近的小片区域

这意味着，如果能识别出哪些视觉 token 对应的区域没有变化，就可以直接复用上一帧的 KV 表示，跳过对这些 token 的编码计算，从而大幅降低计算量。

### 2.3 光流作为运动先验

LAC 使用**光流（Optical Flow）** 作为运动信号来源，具体采用轻量级的 RAFT-small 模型。

光流提供了每个像素级别的运动向量，能直接告诉模型"哪里在动、哪里没动"。相比使用语言指令来引导注意力分配，光流有两个优势：

1. **直接性**：无需复杂的语义对齐，直接从像素运动判断
2. **高效性**：避免了为了语言-视觉对齐引入额外的大型编码器

---

## 三、方法论详解

### 3.1 整体框架

LAC 的设计哲学是**即插即用**（plug-and-play）：VLA 骨架网络保持冻结，只训练两个轻量级决策模块。整体流程如下：

1. 接收当前帧 $I_t$ 和上一帧之间的光流 $O_t$，拼接为运动感知表示 $V_t = [I_t; O_t]$
2. **Cached Token Selector** 根据 $V_t$ 为每个 token 打分，评估其重要性
3. **Cache Ratio Predictor** 根据 $V_t$ 评估全局场景动态性，决定总体缓存比例
4. 生成二值掩码 $M_t \in \{0, 1\}^N$，指导每个 token 是重新计算还是从缓存复用
5. 仅对"活跃"token 执行前向计算，"缓存"token 直接复用上一帧的 KV 状态

### 3.2 Cached Token Selector（缓存 Token 选择器）

选择器 $f_{\text{sel}}$ 是一个轻量级 CNN，输入运动感知表示 $V_t$，输出每个 token 的显著性分数：

$$S_t = f_{\text{sel}}(V_t; \theta_{\text{sel}})$$

其中 $S_t = \{s_t^{(1)}, \dots, s_t^{(N)}\}$，每个 $s_t^{(i)} \in [0, 1]$。

- **高分** $s_t^{(i)}$ → 该 token 对应的区域正在发生变化（如机械臂在移动），需要**重新计算**
- **低分** $s_t^{(i)}$ → 该 token 对应的区域是静态的（如背景），可以**缓存复用**

### 3.3 Cache Ratio Predictor（缓存比例预测器）

选择器给出了 token 间的**相对排序**，而预测器决定**整体计算预算**。它从一个离散候选集 $\mathcal{R} = \{r_1, \dots, r_C\}$ 中选择最优缓存比例：

$$L_t = f_{\text{pred}}(V_t; \theta_{\text{pred}})$$

其中 $L_t = \{l_t^{(1)}, \dots, l_t^{(C)}\}$ 是对每个候选比例的 logit 值。推理时选择最高 logit 对应的比例。

- **静态场景** → 预测器学会选择**高缓存比例**，最大化效率
- **动态场景** → 预测器学会选择**低缓存比例**，确保任务性能

### 3.4 两阶段训练流程

直接从零学习离散缓存策略是不稳定的。LAC 采用两阶段训练来解决冷启动问题：

#### Stage I：注意力对齐初始化

先用 VLA 骨架网络的注意力图作为"教师信号"，训练 Cached Token Selector 模仿 VLA 的注意力分布：

$$\mathcal{L}_{\text{align}} = \text{MSE}(f_{\text{sel}}(V_t; \theta_{\text{sel}}), S_{\text{VLA}})$$

虽然 VLA 的注意力分数不是最优策略，但它提供了一个合理的**视觉显著性先验**，为后续的端到端优化提供稳定的起点。

**为什么需要这一步？** 论文消融实验显示，跳过 Stage I 直接端到端训练，成功率从 85.6% 骤降至 79.2%——在稀疏监督下从零学习离散选择策略极其不稳定。

#### Stage II：端到端联合优化

冻结 VLA 骨架，联合优化选择器和预测器。总损失函数：

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{VLA}} + \lambda \mathcal{L}_{\text{ratio}}$$

其中 $\mathcal{L}_{\text{VLA}}$ 是任务损失（动作预测），$\mathcal{L}_{\text{ratio}}$ 是鼓励更高缓存比例的正则项：

$$\mathcal{L}_{\text{ratio}} = -\mathbb{E}_{\tilde{p}_t}[r] = -\sum_{j=1}^{C} \tilde{p}_t^{(j)} r_j$$

$\lambda$ 控制效率与性能之间的权衡。梯度通过两个模块反向传播，使缓存策略直接与下游控制目标对齐。

### 3.5 可微分的离散决策学习

LAC 面临的核心技术挑战是：缓存决策本质上是**离散的**（缓存/不缓存），而离散操作不可微分，阻碍了端到端优化。

LAC 采用 **Gumbel-Softmax + 直通估计器（STE）** 的"硬前向、软反向"策略来解决：

**对于 Cache Ratio Predictor**：

前向传播使用硬 argmax 做确定性选择：
$$p_t = \text{one\_hot}(\arg\max_j(\tilde{p}_t^{(j)}))$$

反向传播时，通过添加 Gumbel 噪声的软概率传递梯度：
$$\tilde{p}_t^{(j)} = \frac{\exp((l_t^{(j)} + g_j)/\tau)}{\sum_{k=1}^{C} \exp((l_t^{(k)} + g_k)/\tau)}$$

其中 $g_j \sim \text{Gumbel}(0, 1)$，$\tau$ 是温度参数。

**对于 Cached Token Selector**：

前向传播使用硬 top-k 操作生成二值掩码 $M_t$。反向传播使用陡峭 sigmoid 构造软掩码：

$$\tilde{M}_t^{(i)} = \sigma\left(\frac{s_t^{(i)} - \theta_k}{\tau_s}\right)$$

其中 $\theta_k$ 是第 $k$ 个 token 的分数阈值，$\tau_s$ 是温度参数。

### 3.6 推理流程

训练完成后，推理流程简洁高效：

1. **确定性决策**：用 argmax 替代训练时的随机采样，确保推理稳定
2. **高效前向传播**：
   - **活跃 token**：重新编码，计算新的 K/V 状态
   - **缓存 token**：直接复用上一帧的 K/V 状态
   - 合并后的 KV 状态送入动作解码器
3. **随机恢复机制**：以小概率 $p_{\text{recover}}$ 随机重新计算一部分"缓存"token，防止长时间持续缓存导致的误差累积

**最大的计算节省**发生在每个时间步**第一个动作 token 的生成**——此时需要处理全部视觉 token。后续 token 照常自回归解码，不额外增加开销。

---

## 四、实验结果

### 4.1 实验设置

**基准模型**：OpenVLA（7B，自回归动作解码）和 CogAct（扩散动作解码器）

**评估基准**：
- **LIBERO**：包含 Spatial、Object、Goal、Long 四个子任务套件
- **SIMPLER**：包含 Visual Matching 和 Variant Aggregation 两种配置
- **真实机器人**：Franka 机械臂，4 个操控任务

**对比方法**：
| 方法 | 类型 | 策略 |
| --- | --- | --- |
| SparseVLM | Token 剪枝 | 基于模块化剪枝误差的结构化稀疏 |
| FastV | Token 剪枝 | 第 2 层后丢弃一半 token |
| VLA-Cache | Token 缓存 | 基于规则的静态 KV 缓存 |
| LAC（本文） | Token 缓存 | 可学习的自适应缓存策略 |

### 4.2 LIBERO 基准结果

| 方法 | Spatial | Object | Goal | Long | 平均 | FLOPs(T)↓ | CUDA Time(ms)↓ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Baseline (OpenVLA) | 84.4% | **86.6%** | 75.6% | 53.2% | 75.0% | 1.864 | 51.91 |
| SparseVLM | 79.8% | 67.0% | 72.6% | 39.4% | 64.7% | 1.407 | 83.39 |
| FastV | 83.4% | 84.0% | 74.2% | 51.6% | 73.3% | 1.864 | 53.28 |
| VLA-Cache | 83.8% | 85.8% | 76.4% | 52.8% | 74.7% | **1.355** | 31.83 |
| **LAC（本文）** | **85.6%** | 86.2% | **76.6%** | **59.2%** | **76.9%** | 1.392 | **29.51** |

**关键发现**：

1. **LAC 是唯一在提速的同时提升性能的方法**——成功率 +1.9pp，推理速度 1.76×
2. **SparseVLM 反而更慢了**——虽然理论 FLOPs 降低，但剪枝逻辑引入的额外开销对短动作序列得不偿失，CUDA 时间反升至 83.39ms
3. **LAC 在最难的 Long 子任务上优势最大**（59.2% vs 53.2%），说明可学习策略在长期任务中的适应性更强
4. **VLA-Cache 虽然也快，但性能略有损失**——静态规则无法适应动态场景变化

### 4.3 SIMPLER 基准结果

| SIMPLER | 方法 | PickCan | MoveNear | Drawer | DrawerApple | 平均 | FLOPs(T)↓ | CUDA Time(ms)↓ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Visual Matching | Baseline (CogAct) | 91.3% | **85.0%** | 71.8% | 50.9% | 74.8% | 1.847 | 54.29 |
| | VLA-Cache | 92.0% | 83.3% | 70.5% | 51.6% | 74.4% | **1.496** | 39.63 |
| | **LAC** | **92.3%** | 84.2% | **72.7%** | **52.8%** | **75.5%** | 1.511 | **37.82** |
| Variant Aggregation | Baseline (CogAct) | 89.6% | 80.8% | 28.3% | 46.6% | 61.3% | 1.807 | 53.54 |
| | VLA-Cache | **91.7%** | 79.3% | **32.5%** | 45.8% | 62.3% | **1.493** | 39.11 |
| | **LAC** | 92.1% | **81.5%** | 31.2% | **47.1%** | **63.0%** | 1.506 | **37.90** |

LAC 在以 CogAct（扩散动作解码器）为基座的 SIMPLER 上同样有效，证明其对**不同动作解码架构的通用性**。FLOPs 降低 17.4%，推理加速 1.42×。

### 4.4 真实机器人结果

| 方法 | KnockCrisp | PickMango | CoverBanana | KnockBottle | 平均 | FLOPs(T)↓ | CUDA Time(ms)↓ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Baseline (OpenVLA) | 48.0% | **16.0%** | **24.0%** | 44.0% | 33.0% | 1.893 | 37.38 |
| VLA-Cache | 48.0% | 12.0% | 20.0% | 40.0% | 30.0% | **1.534** | 33.36 |
| **LAC** | **52.0%** | 12.0% | **24.0%** | **64.0%** | **38.0%** | 1.569 | **32.47** |

LAC 在真实机器人上成功率提升 5.0 个百分点，同时降低推理延迟。KnockBottle 任务上提升尤为显著（44% → 64%），说明 LAC 学到的注意力分配在瓶子这种需要精确定位的任务上特别有效。

### 4.5 消融实验

| 配置 | 成功率(%) | FLOPs(T)↓ | Time(ms)↓ |
| --- | --- | --- | --- |
| 仅 Selector | 82.20 | 1.283 | 28.48 |
| + Reuse Predictor | 83.40 | 1.325 | 29.04 |
| + Recovery（完整 LAC） | **85.60** | 1.377 | 29.32 |

- **Token Selector 是核心**：仅选择器就达到 82.2%，超过多数基线
- **Reuse Predictor 提供自适应能力**：+1.2pp，FLOPs 略增是因为它在关键时刻自适应降低缓存比例以确保任务成功
- **随机恢复机制锦上添花**：+2.2pp，通过随机刷新缓存 token 防止误差累积

**额外消融**：

| 变体 | 成功率(%) |
| --- | --- |
| 无 Stage I 初始化 | 79.2 |
| 语言引导策略（替代光流） | 83.0 |
| **完整 LAC** | **85.6** |

- 跳过注意力对齐初始化→性能骤降 6.4pp，验证了两阶段训练的必要性
- 用语言引导替代光流→性能降 2.6pp，因为轻量级语言模块难以精确对齐视觉特征，而光流提供的运动信号更直接高效

---

## 五、用类比总结 LAC 的核心原理

想象你在用一台摄像机监控一个仓库：

**传统 VLA**：每一帧都对整个画面从头分析一遍——门口的墙壁、天花板的灯、地上的叉车……即使只有一个工人在搬箱子，其他 99% 的区域都没变，也要全部重新处理。

**基于规则的缓存（VLA-Cache）**：提前写好规则——"画面中没动的区域就别看了"。但规则是死的。比如规则说"篮子没动就不看"，结果机械臂正准备把东西放进篮子，错过了关键交互区域。

**LAC 的做法**：培训一个"智能助手"，让它学会**根据任务目标**判断应该关注哪里。这个助手用光流（运动检测）快速扫一眼"哪里在动"，再结合从任务反馈中学到的经验，做出精准的注意力分配决策。它不仅知道"机械臂在动所以要看"，还知道"接下来要放东西的目标区域也要提前关注"。

而且这个助手是**通过实际做任务的成败来学习**的（端到端训练），而非靠人给它写规则。所以它总能比任何人工规则做得更好、更快。

---

## 六、局限性与未来方向

### 6.1 对光流质量的依赖

LAC 的决策模块以光流为核心输入，在极端视觉条件（如严重光照变化、大量遮挡）下，光流估计精度可能下降，从而影响 token 选择质量。

### 6.2 高动态场景下加速收益递减

当场景发生快速全局变化（如相机快速运动）时，LAC 会自适应地选择重新计算大部分 token 以保证准确性，此时加速收益自然减少。这是一个合理的设计权衡——性能优先于速度。

### 6.3 未来方向

- **扩展到 Flow Matching VLA**：LAC 目前验证了在自回归 VLA（OpenVLA）和扩散解码 VLA（CogAct）上的效果，但尚未在 π₀ 等 Flow Matching 架构上验证。π₀ 的推理瓶颈在于 10 步积分中重复的 Transformer 前向传播，LAC 的 token 缓存策略可能与 π₀ 的 KV 缓存机制互补
- **端到端训练选择器与 VLA 骨架**：当前 LAC 冻结 VLA 骨架只训练决策模块，未来可以探索联合微调的效果
- **多模态运动先验**：结合光流 + 深度信息 + 语义分割，提供更鲁棒的运动感知

---

## 七、个人思考

### 7.1 "加速提性能"的反直觉现象

LAC 最令人惊喜的发现是**加速和提性能可以同时实现**。这看似矛盾，但背后的逻辑很清晰：冗余的视觉计算不仅浪费算力，还可能引入噪声干扰决策。通过学到的策略精准地"忽略"无关区域，模型反而能更专注于任务关键信息。

### 7.2 与 π₀ 的对比视角

从推理效率角度看，π₀ 和 LAC 解决的是**不同层面**的问题：

- **π₀** 通过 action chunking + KV 缓存解决了 flow matching 积分步之间的重复计算（同一时间步内的效率）
- **LAC** 解决的是**时间步之间**的视觉冗余（跨帧效率）

两者理论上可以**叠加使用**：在 π₀ 的 10 步积分中，观测 token 的 KV 已经被缓存；而 LAC 可以进一步减少每个时间步首次编码观测时的 token 数量。

### 7.3 可学习加速作为通用范式

LAC 的思路——将加速策略本身变成可学习的组件——具有广泛的启发意义。这不仅适用于 token 缓存，也可以推广到层跳跃（layer skipping）、动态量化、自适应分辨率等其他加速维度。核心思想是：**让模型自己学会在哪里值得花计算量**。

---

## 参考

- [OpenVLA](https://arxiv.org/abs/2406.09246) — 开源 VLA 基座模型
- [CogAct](https://arxiv.org/abs/2411.19650) — 扩散解码 VLA 基础模型
- [VLA-Cache](https://arxiv.org/abs/2502.02175) — 基于规则的 VLA token 缓存
- [π₀](/papers/06-embodied-ai/vla/foundation/pi0_2024) — Flow Matching VLA 基础模型
- [RAFT](https://arxiv.org/abs/2003.12039) — 光流估计模型
- [Gumbel-Softmax](https://arxiv.org/abs/1611.01144) — 离散变量的可微分松弛
