# VLA-Cache：通过自适应 Token 缓存加速 VLA 操控——原理详解

> 论文：*VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive Token Caching*
> 机构：University of Sydney、Shanghai Jiao Tong University
> 发布时间：2025年2月（NeurIPS 2025）
> 🔗 [arXiv](https://arxiv.org/abs/2502.02175) | [Project](https://vla-cache.github.io)

---

## 一、论文要解决什么问题？

### 1.1 VLA 模型的推理瓶颈

VLA（Vision-Language-Action）模型在机器人操控中展现了强大的多模态推理能力，但**计算开销极大**，难以满足实时控制要求。核心矛盾在于：

- 机器人闭环控制需要**低延迟**的动作生成（通常需要数十毫秒级别的响应）
- VLA 模型在**每个时间步**都要对完整视觉输入重新编码，即使连续帧之间绝大部分区域（如静态背景）并没有变化
- 计算瓶颈主要在**语言模型解码器**（如 LLaMA、Gemma），它占据了 VLA 推理时间的大部分

### 1.2 现有加速方法的局限

已有的加速策略存在两类根本性问题：

**通用加速方法**（量化、蒸馏、早退等）：
- 需要**架构修改或重新训练**
- 缺乏对 VLA 任务特性的针对性设计
- 难以在速度和精度之间取得最佳平衡

**VLM Token 剪枝/合并方法**（FastV、SparseVLM、ToMe 等）：
- 在**单帧内**减少冗余，完全忽视了跨帧时间连续性
- 对 VLA 的短动作序列（如 7 个 token），加速收益微乎其微
- 剪枝/合并会破坏**空间保真度**，这对精确操控至关重要

### 1.3 VLA-Cache 的核心洞察

VLA-Cache 发现了 VLA 任务中一个重要但被忽视的特性：

> **机器人操控场景中，连续帧之间存在大量视觉冗余。**大部分视觉 token（背景、桌面、固定物体）在相邻时间步之间几乎没有变化，但 VLA 模型每一步都从头重新编码所有 token。

VLA-Cache 的核心思路：**利用跨帧时间冗余，选择性地缓存和复用静态视觉 token 的 KV 表示，跳过冗余计算。** 但关键挑战在于——**不是所有视觉上静态的 token 都可以安全复用**。某些区域（如夹爪附近、目标物体）虽然像素变化小，但在语义上对动作生成至关重要。

**最终效果**：在 LIBERO 上实现 **1.7× CUDA 延迟加速**和 **15% 控制频率提升**，成功率几乎无损。且**无需训练**，即插即用。

---

## 二、预备知识

### 2.1 VLA 模型中的 KV 缓存

在自回归 Transformer 中，KV 缓存是标准的推理加速技术。给定输入序列 $X$，自注意力机制计算：

$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

$$\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V$$

推理时，新 token 的 $k_{\text{new}}$ 和 $v_{\text{new}}$ 追加到缓存：

$$K_i = \text{Concat}(K_{i-1}, k_{\text{new}}), \quad V_i = \text{Concat}(V_{i-1}, v_{\text{new}})$$

**问题**：这种 KV 缓存只在**单次查询内部**的解码步之间有效。VLA 模型在每个新时间步都接收新的视觉帧，需要从头编码所有视觉 token，完全没有利用跨帧冗余。

### 2.2 跨帧时间冗余

在闭环机器人操控中，连续帧之间具有高度的视觉连续性：

- **静态区域**（背景、桌面、远处物体）在短时间内几乎不变
- **动态区域**（机械臂末端执行器、被操控物体）仅占画面的一小部分

这种时间冗余意味着：如果能识别出哪些 token 对应静态区域，就可以直接复用上一帧的 KV 表示，避免重复计算。

### 2.3 注意力模式的层间差异

VLA-Cache 还发现了一个重要规律：**VLA 解码器中不同层的注意力分布存在显著差异**。

- **浅层**：注意力分散，广泛关注所有 token
- **中间层**：注意力波动
- **深层**：注意力更集中，聚焦于少数关键 token

这一观察（与 FastV 在 VLM 中的发现一致）暗示：**不同层可以采用不同的 token 复用比例**——注意力越集中的层，可以复用更多 token。

---

## 三、方法论详解

VLA-Cache 包含两个核心机制和一个层级优化策略，构成一个**训练无关、即插即用**的加速框架。

### 3.1 Dynamic Token Selection（动态 Token 选择）

#### 3.1.1 Static Token Selection（静态 Token 选择）

给定图像 $I \in \mathbb{R}^{H \times W \times 3}$，将其划分为 $N \times N$ 个不重叠的 patch，每个 patch 大小为 $p \times p$。对当前帧和上一帧的对应 patch 计算余弦相似度：

$$\text{Sim}(P_t^{i,j}, P_{t-1}^{i,j}) = \frac{P_t^{i,j} \cdot P_{t-1}^{i,j}}{\|P_t^{i,j}\|_2 \cdot \|P_{t-1}^{i,j}\|_2}$$

当相似度超过阈值 $\tau$ 时，该 patch 被认为是视觉静态的。进一步用 Top-k 筛选最稳定的 token：

$$\mathcal{P}_{\text{static}} = \text{Top-k}\left(\{P_t^{i,j} \mid \text{Sim}(P_t^{i,j}, P_{t-1}^{i,j}) \geq \tau\}\right)$$

这一步的计算开销极低（$O(H^2)$），远小于 Transformer 前向传播。

#### 3.1.2 Evicting Task-Relevant Tokens（剔除任务相关 Token）

直接复用所有视觉静态 token 是危险的。论文在 Table 1 中展示了一个关键消融：

| 方法 | 成功率(%) | 延迟(ms) |
| --- | --- | --- |
| OpenVLA（基线） | 84.4 | 51.56 |
| + 仅静态 Token 复用 | **74.2**（-10.2pp） | 31.03 |
| + 剔除任务相关 Token | 82.6 | 31.03 |
| + 层自适应策略 | 83.8 | 32.22 |

**成功率从 84.4% 暴跌到 74.2%！** 原因是某些区域（如夹爪、目标物体）虽然像素变化小，但在语义上对动作生成至关重要——VLA 模型需要追踪物体状态和交互过程。

为此，VLA-Cache 利用**解码器的文本-视觉交叉注意力分数**来识别任务相关 token。对每个解码器层 $l$，提取文本到视觉的注意力矩阵：

$$A^l_{\text{vis-text}} = A^l[:, v_{\text{start}}:v_{\text{end}}, t_{\text{start}}:t_{\text{end}}]$$

跨多头取均值 $A^l_{\text{avg}} = \text{Mean}_{\text{heads}}(A^l_{\text{vis-text}})$，再跨多层取均值得到最终任务相关性分数：

$$S_{\text{task-relevance}} = \text{Mean}_{l \in \mathcal{L}}(A^l_{\text{avg}})$$

用阈值 $\tau_{\text{task}}$ 筛选出高任务相关性的 token：

$$\mathcal{P}_{\text{task-relevant}} = \{P_t^{i,j} \mid S_{\text{task-relevance}}[i,j] \geq \tau_{\text{task}}\}$$

最终，从静态 token 集合中**剔除**任务相关 token，得到安全可复用的 token 集合：

$$\mathcal{P}_{\text{reuse}} = \mathcal{P}_{\text{static}} \setminus \mathcal{P}_{\text{task-relevant}}$$

这一机制将成功率从 74.2% 恢复到 82.6%，同时保持了计算收益。

### 3.2 Adaptive Token Caching（自适应 Token 缓存）

基于注意力分布的层间差异，VLA-Cache 提出**层自适应复用策略**，为每一层动态调整 token 复用比例。

首先，使用注意力熵 $\mathcal{E}^l$ 量化第 $l$ 层的注意力集中程度。定义相邻层间的熵比：

$$R^l = \frac{\mathcal{E}^{l-1} - \mathcal{E}^l}{\mathcal{E}^{l-1}}$$

$R^l > 0$ 表示第 $l$ 层的注意力比上一层更集中。累积这些比率，得到每层的复用比例：

$$\alpha^l = \min\left(k \sum_{j=1}^{l} R^j, \; 1\right)$$

其中 $k$ 是控制注意力集中度影响的超参数。**累积熵缩减越大的层，允许复用更高比例的 token**——因为注意力越集中，意味着模型只依赖少数关键 token，其余 token 可以安全复用。

### 3.3 跨帧 KV 缓存实现

在每个时间步 $t$，对视觉 token 序列 $H_t$ 进行选择性计算：

$$K_t(i) = \begin{cases} K_{t-1}(i), & i \in \mathcal{P}_{\text{reuse}} \\ W_K H_t(i), & \text{otherwise} \end{cases}, \quad V_t(i) = \begin{cases} V_{t-1}(i), & i \in \mathcal{P}_{\text{reuse}} \\ W_V H_t(i), & \text{otherwise} \end{cases}$$

**实现细节**：

- **位置编码和注意力掩码**：维护 `cache_position` 数组标记需要重新计算的 token，静态 token 保留其上一帧的位置编码
- **旋转嵌入（RoPE）**：仅对重新计算的 token 施加新的旋转嵌入，跳过的 token 保留之前的编码状态
- **动态缓存更新**：新计算的 token 更新 $\{K_t^l, V_t^l\}$，复用的 token 继承上一帧的 $\{K_{t-1}^l, V_{t-1}^l\}$。由于 Transformer 的置换不变性，这种部分更新产生正确的注意力结果

**最大的计算节省**发生在每个时间步**第一个动作 token 的生成**——此时需要处理所有视觉 token。后续 token 照常自回归解码，不额外增加开销。

### 3.4 计算复杂度分析

**标准 VLA 每层的 FLOP 成本**：

$$\text{FLOPs} \approx 4LD^2 + 2L^2D + 2LDM$$

**VLA-Cache 的 FLOP 节省**：有效 token 数减少到 $L_r = \alpha \times \mathcal{P}_{\text{final}}$，每层节省：

$$\Delta\text{FLOPs}_{\text{layer}} \approx 4L_rD^2 + 2L_r^2D + 2L_rDM$$

**额外开销**：

| 组件 | 复杂度 |
| --- | --- |
| 静态 Token 选择（patch 相似度） | $O(H^2)$ |
| 任务相关性过滤（注意力聚合） | $O(L_tL_vD)$ |
| 层自适应熵计算 | $O(L^2D)$ |

所有额外开销远小于 Transformer 前向传播成本，确保了实际的推理加速。

---

## 四、实验结果

### 4.1 实验设置

**基准模型**：OpenVLA（7B，自回归动作解码）、CogAct（扩散动作解码器）、OpenVLA-OFT（高频动作分块）

**评估基准**：
- **LIBERO**：Spatial、Object、Goal、Long 四个子任务套件
- **SIMPLER**：Visual Matching 和 Variant Aggregation 两种配置
- **真实机器人**：Kinova Jaco2 机械臂，4 个操控任务

**对比方法**：SparseVLM（Token 剪枝）、FastV（Token 剪枝）

**硬件**：NVIDIA RTX 4090 GPU，所有实验在 BF16 精度下进行

### 4.2 LIBERO 基准结果

| 方法 | Spatial | Object | Goal | Long | 平均 | FLOPs(T)↓ | Latency(ms)↓ | 控制频率(Hz)↑ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OpenVLA | 84.4% | 86.6% | 75.6% | 53.2% | 75.0% | 1.864 | 51.91 | 4.23 |
| + SparseVLM | 79.8% | 67.0% | 72.6% | 39.4% | 64.7% | 1.407 | 83.39 | 3.72 |
| + FastV | 83.4% | 84.0% | 74.2% | 51.6% | 73.3% | 1.864 | 53.28 | 4.19 |
| **+ VLA-Cache** | **83.8%** | **85.8%** | **76.4%** | **52.8%** | **74.7%** | **1.355** | **31.83** | **4.59** |
| OpenVLA-OFT | 97.8% | 97.6% | 97.6% | 94.2% | 96.8% | 4.013 | 79.05 | 65.10 |
| **+ VLA-Cache** | **98.3%** | 97.5% | **98.3%** | **95.4%** | **97.4%** | **3.097** | **62.59** | **78.98** |

**关键发现**：

1. **VLA-Cache 在 OpenVLA 上实现 1.63× 加速，FLOPs 降低 27.3%，成功率仅降 0.3pp**
2. **SparseVLM 反而更慢了**——剪枝逻辑引入的额外开销导致 CUDA 延迟反升至 83.39ms（比基线慢 60%），在短动作序列上得不偿失
3. **FastV 几乎没有加速效果**——VLA 动作输出只有 7 个 token，它针对长序列生成设计的策略此处无效
4. **在 OpenVLA-OFT 上仍有叠加收益**——控制频率从 65.10 Hz 提升到 78.98 Hz（+14 Hz），说明 VLA-Cache 与高频架构兼容，且能直接加速语言解码器瓶颈

### 4.3 SIMPLER 基准结果

| SIMPLER | 方法 | PickCan | MoveNear | Drawer | DrawerApple | 平均 | FLOPs(T)↓ | Latency(ms)↓ | 控制频率(Hz)↑ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Matching | CogACT | 91.3% | **85.0%** | **71.8%** | 50.9% | 74.8% | 1.847 | 54.29 | 12.42 |
| | + VLA-Cache | **92.0%** | 83.3% | 70.5% | **51.6%** | 74.4% | **1.496** | **39.63** | **14.66** |
| Aggregation | CogACT | 89.6% | **80.8%** | 28.3% | **46.6%** | 61.3% | 1.807 | 53.54 | 12.36 |
| | + VLA-Cache | **91.7%** | 79.3% | **32.5%** | 45.8% | **62.3%** | **1.493** | **39.11** | **14.48** |

VLA-Cache 在以 CogAct（扩散动作头）为基座的 SIMPLER 上同样有效：FLOPs 降低约 20%，推理加速 1.37×，成功率基本持平。证明了 VLA-Cache 对**不同动作解码架构**的通用性。

### 4.4 真实机器人结果

| 方法 | PickPot | PlaceCube | PutSausage | WipeTable | 平均 | FLOPs(T)↓ | Latency(ms)↓ | 控制频率(Hz)↑ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OpenVLA | **95.0%** | 83.3% | 80.0% | 70.0% | 82.1% | 1.814 | 64.16 | 4.02 |
| + VLA-Cache | 90.0% | **90.0%** | **85.0%** | **73.3%** | **84.6%** | **1.303** | **51.85** | **4.21** |

**关键发现**：

1. **VLA-Cache 在真实机器人上平均成功率反而提升 2.5pp**（82.1% → 84.6%），可能因为过滤冗余 token 减少了对决策的干扰
2. 在 PlaceCube（+6.7pp）、PutSausage（+5.0pp）、WipeTable（+3.3pp）三个任务上超过基线
3. FLOPs 降低 28.2%，延迟降低 19.2%

### 4.5 动态背景下的鲁棒性

| 方法 | 成功率(%) | FLOPs(T)↓ | Latency(ms)↓ |
| --- | --- | --- | --- |
| OpenVLA（基线） | 95 | 1.800 | 68.03 |
| + 背景噪声 | 80 | 1.807 | 68.22 |
| + 背景噪声 + VLA-Cache | 80 | **1.275** | **50.59** |

在 PickPot 任务中引入动态背景（人手、移动物体）后，基线成功率从 95% 降至 80%。VLA-Cache 在相同噪声条件下保持 80% 成功率，同时 FLOPs 降低 42%、延迟降低 35%。说明 VLA-Cache 能有效过滤瞬态/无关 token，在动态环境中保持效率和稳定性。

### 4.6 消融实验：Token 复用/剪枝数量

| #Tokens | 方法 | 成功率(%) | FLOPs(T)↓ | Latency(ms)↓ |
| --- | --- | --- | --- | --- |
| 0 | Baseline | 84.4 | 1.888 | 52.37 |
| 50 | SparseVLM / FastV / **VLA-Cache** | 79.8 / 84.6 / **85.4** | 1.358 / 1.888 / **1.611** | 88.08 / 53.10 / **33.43** |
| 100 | SparseVLM / FastV / **VLA-Cache** | 74.6 / 83.4 / **83.8** | 1.097 / 1.888 / **1.295** | 61.01 / 45.72 / **31.29** |
| 200 | SparseVLM / FastV / **VLA-Cache** | 44.4 / 72.8 / 68.3 | **0.735** / 1.888 / 0.823 | 57.42 / 45.19 / **30.29** |

- VLA-Cache 在适度复用率（50-100 token）下**同时实现最优性能和最低延迟**
- 复用 50 个 token 时，VLA-Cache 成功率甚至**超过基线**（85.4% vs 84.4%），说明过滤冗余 token 反而有利于决策
- 过度复用（200 token）时所有方法性能都下降，但 VLA-Cache 的下降最为温和

### 4.7 注意力 vs 物体掩码作为任务相关性代理

| 方法 | 成功率(%) | FLOPs(T)↓ | Latency(ms)↓ | 控制频率(Hz)↑ |
| --- | --- | --- | --- | --- |
| OpenVLA-OFT | 97.8 | 3.99 | 78.35 | 65.44 |
| + VLA-Cache（注意力） | **98.3** | **3.04** | **61.12** | **81.67** |
| + VLA-Cache（物体掩码） | 87.4 | 3.15 | 87.49 | 64.78 |

使用 Efficient Track Anything 生成的物体掩码作为替代方案，成功率暴跌至 87.4%，且延迟反而增加。原因是物体掩码只提供空间定位，无法捕捉**细粒度的上下文相关信号**，尤其在背景杂乱或小型目标物体的场景下。而注意力分数是模型内部推理过程动态产生的，天然与任务目标对齐。

---

## 五、用类比总结 VLA-Cache 的核心原理

想象你在看一段监控摄像机的视频流：

**传统 VLA**：每一帧都把整个画面从头到尾重新分析一遍——墙壁、地板、灯光、固定家具……即使只有一个人在画面中走动，也要把所有东西都重新处理。

**VLA-Cache 的做法**：像一个聪明的监控员——

1. **第一步：找不动的地方**。快速扫一眼新帧和旧帧，用余弦相似度计算出哪些区域没变——"背景墙没动，地板没动，桌子没动"，这些直接沿用上次的分析结果。

2. **第二步：检查"看似没动但很重要"的地方**。查看模型的注意力分数——"虽然夹爪位置像素变化不大，但模型一直在关注它"。这类区域不能偷懒，必须重新分析。

3. **第三步：分层调节**。浅层注意力分散，需要看得全面一些；深层注意力集中，可以更激进地复用缓存。

最终效果：**监控员只重新分析画面中真正变化且任务关键的区域**，其余区域直接引用上一帧的结论——既快又准。

---

## 六、局限性与未来方向

### 6.1 高动态场景下加速收益递减

在背景或物体大幅运动的场景中，不可复用的 token 数量增加，加速收益降低。这是所有基于时间冗余的方法的固有限制。

### 6.2 模型架构覆盖有限

VLA-Cache 目前验证了在 OpenVLA、CogAct、OpenVLA-OFT 三种基于 LLaMA2 解码器的架构上的效果。对于采用不同骨架（如 Gemma2 in π₀）或更复杂架构的 VLA 系统，适用性尚需验证。

### 6.3 未来方向

- **扩展到更多 VLA 架构**：特别是 π₀ 等 Flow Matching 架构，其 10 步积分过程中可能存在更多跨步冗余
- **与可学习加速方法结合**：VLA-Cache 的规则式设计虽然无需训练，但可与 LAC 等可学习方法互补——用 VLA-Cache 做粗筛，用学习模块做精调
- **与扩散/Transformer 风格模型的集成**：对 RDT、DiT 等具有帧间冗余的模型提供通用加速

---

## 七、个人思考

### 7.1 Training-Free 的价值与代价

VLA-Cache 最大的优势是**零训练开销**——直接应用于任何 VLA 模型，无需修改架构或重新训练。这对于快速迭代和部署极为友好。但代价是**依赖手工设计的规则**（相似度阈值 $\tau$、任务相关性阈值 $\tau_{\text{task}}$、Top-k 值），这些超参数需要针对不同场景手动调节（如真实机器人上 $\tau$ 从 0.996 降至 0.85）。

### 7.2 与 LAC 的互补关系

VLA-Cache 和 [LAC](/papers/06-embodied-ai/vla/efficient/LAC_2026) 解决同一问题但采取了截然不同的路线：

| 维度 | VLA-Cache | LAC |
| --- | --- | --- |
| 核心策略 | 基于规则（patch 相似度 + 注意力分数） | 可学习（CNN 选择器 + 比例预测器） |
| 训练需求 | 无需训练 | 需要两阶段训练 |
| 运动信号 | patch 级余弦相似度 | 光流（RAFT-small） |
| 任务相关性 | 解码器注意力分数 | 任务损失梯度驱动 |
| 层策略 | 基于熵的自适应复用比例 | 统一策略 |
| 加速倍率 | 1.63×（OpenVLA） | 1.76×（OpenVLA） |
| 性能影响 | -0.3pp | +1.9pp |

两者理论上可以叠加：用 VLA-Cache 的无训练方案做**快速部署基线**，再用 LAC 的学习方案做**进一步优化**。

### 7.3 跨帧缓存与 π₀ 的 KV 缓存的区别

VLA-Cache 的跨帧 KV 缓存和 π₀ 在 flow matching 积分中的 KV 缓存解决的是**不同层面**的冗余：

- **π₀ 的 KV 缓存**：在同一时间步的 10 次去噪积分中，观测 token 的 KV 只算一次（**步内冗余**）
- **VLA-Cache 的跨帧缓存**：在相邻时间步之间，复用静态 token 的 KV（**步间冗余**）

两者正交，理论上可以叠加使用，实现更大幅度的加速。

### 7.4 "加速反而提性能"现象

VLA-Cache 在真实机器人上成功率反而提升（82.1% → 84.6%），与 LAC 的观察一致。这再次印证了一个重要直觉：**冗余视觉信息不仅浪费算力，还可能干扰模型决策**。通过主动过滤无关 token，模型能更专注于任务关键区域，从而做出更准确的动作预测。

---

## 参考

- [OpenVLA](https://arxiv.org/abs/2406.09246) — 开源 VLA 基座模型
- [CogAct](https://arxiv.org/abs/2411.19650) — 扩散解码 VLA 基础模型
- [OpenVLA-OFT](https://arxiv.org/abs/2502.19645) — 高频动作分块 VLA
- [LAC](/papers/06-embodied-ai/vla/efficient/LAC_2026) — 可学习自适应 VLA Token 缓存
- [π₀](/papers/06-embodied-ai/vla/foundation/pi0_2024) — Flow Matching VLA 基础模型
- [FastV](https://arxiv.org/abs/2403.06764) — VLM 推理加速
- [SparseVLM](https://arxiv.org/abs/2410.04417) — VLM Token 稀疏化
