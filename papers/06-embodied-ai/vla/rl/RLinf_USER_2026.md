# RLinf-USER：面向真实世界在线策略学习的统一可扩展系统——原理详解

> 论文：*RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI*
> 机构：清华大学、Infinigence AI、北京理工大学、浙江大学、中关村科学院、上海人工智能实验室
> 发布时间：2026年2月
> 🔗 [arXiv](https://arxiv.org/abs/2602.07837) | [代码](https://github.com/RLinf/RLinf)
> **分类标签**：`真实世界 RL` `机器人学习系统` `云-边协同` `异步训练` `VLA`

---

## 一句话总结

USER（**U**nified and extensible **S**yst**E**m for **R**eal-world online policy learning）将物理机器人提升为与 GPU 同等地位的一等硬件资源，通过统一硬件抽象层、自适应云-边通信平面、全异步训练流水线和持久化缓存感知缓冲区，构建了一个面向真实世界在线策略学习的统一可扩展系统，支持从 CNN/MLP 到 Flow Policy 到大规模 VLA 的多种策略，涵盖 RL 和模仿学习。

---

## 一、问题与动机

### 1.1 仿真训练的根本局限

当前具身智能的主流范式是**先在仿真中训练，再迁移到真实世界**。然而：

- 动力学差异（dynamics gap）、感知差异（sensing gap）、交互差异导致 **sim-to-real 迁移后性能显著下降**
- 这促使越来越多的研究转向**直接在物理世界中学习策略**

### 1.2 真实世界在线学习的系统级挑战

与仿真不同，真实世界学习面临三大系统级瓶颈：

| 挑战 | 描述 |
| --- | --- |
| **机器人调度** | 机器人通常被视为"外部环境"，无法与 GPU/TPU 一起统一调度和管理 |
| **云-边通信** | 大规模 VLA 需要云端训练 + 边缘推理/执行，跨网络域的通信面临带宽不对称和延迟 |
| **数据效率** | 物理世界无法加速，同步流水线导致机器人频繁等待训练完成；现有缓冲区是内存驻留的短生命周期设计，不支持持久化和跨阶段数据复用 |

### 1.3 现有系统的不足

| 系统 | 局限 |
| --- | --- |
| SERL | 主要支持单机器人、小模型设置 |
| Qt-Opt | 单机器人或小规模场景 |
| ROS2 / Zenoh | 提供连接性但缺乏学习编排能力 |
| SOP | 面向同构机器人，缺乏异构支持 |
| Reverb / Flashbax | 高性能但纯内存设计，不支持持久化、恢复和跨阶段复用 |

**核心洞察**：真实世界策略学习不仅是算法问题，**本质上是一个系统问题**——需要统一的抽象和可扩展的基础设施。

---

## 二、系统架构设计

USER 的系统架构由两大核心组件构成：**统一硬件抽象层**和**自适应通信平面**。

### 2.1 统一硬件抽象层（HAL）

核心思想：**将物理机器人与 GPU/TPU 视为同等地位的一等硬件资源**，统一管理和调度。

#### 节点与硬件抽象

USER 将部署建模为一个**节点集群**，每个节点输出一组硬件单元（hardware unit）：

- **硬件单元**是调度器的最小可分配实体——一个 GPU 设备，或一个物理机器人（可能绑定相机、Space Mouse 等外设）
- 每个单元由轻量级类型描述符（硬件类型 + 型号）和配置元数据描述
- **节点天然异构**：一个节点可同时包含不同类型的机器人和 GPU

三类典型节点：

| 节点类型 | 硬件 | 功能 |
| --- | --- | --- |
| **Rollout Node** | GPU（如 RTX 4090） | 策略推理 |
| **Robot Node** | 仅 CPU + 机器人 | 边缘动作执行 |
| **Training Node** | 大规模加速器（如 A800） | 集中式训练 |

节点可组织为**节点组（node group）**，每组包含同构硬件单元，一个节点可属于多个组。

#### 硬件注册与发现

HAL 使用**可插拔的 checker 接口**：

1. 每种硬件提供一个 HAL checker，定义：类型标识符、发现方法、附加元数据
2. 集群初始化时，在每个节点启动轻量级硬件探针进程
3. 发现方式：
   - **自动发现**：PCIe/USB 连接的设备（GPU、相机、Space Mouse）
   - **配置驱动**：IP 绑定的机器人（需显式配置和安全检查）
4. 物理机器人的发现包含**可选验证**：网络可达性、相机存在性、健康检查

#### 硬件调度

通过统一的**基于 rank 的放置接口**调度：

- 每个组件选择节点组 + 资源 rank 集合
- 调度器确定性地将进程 rank 映射到资源 rank
- 每个进程显式绑定到分配的硬件（如限制可见 GPU 或注入机器人端点配置）

这使得**异构部署**在一个作业中成为可能——例如在一组 GPU 上训练，同时将不同的 rollout 进程绑定到不同类型的机器人。

### 2.2 自适应通信平面

解决真实世界学习中跨异构网络域的通信挑战。

#### 基于隧道的云-边网络

- 真实部署跨越不同网络域（NAT、校园网、工厂 VLAN），天然隔离
- USER 基于 **UDP 隧道技术**构建扁平化 TCP/IP 底层，使所有节点可建立双向连接
- 控制平面使用 **Ray** 管理集群成员和 worker 放置
- 数据平面通过 TCP 握手引导点对点通信组
- 所有控制/数据流量绑定到隧道接口，避免误路由到慢速或防火墙链路

#### 分布式数据通道

传统集中式通信（数据先发到云端再分发到边缘）在多机器人高频交互场景下产生大量跨域流量。USER 提供**分布式数据通道**：

- 具名 FIFO 生产者-消费者队列，由轻量级通道服务托管
- 通过异步 `put` / `get` API 访问
- **基于数据键（如机器人 ID）分片**，将流量本地化到边缘或云区域
- 支持多生产者多消费者，机器人和 rollout 节点无需直接同步耦合

效果：跨域部署下单 episode 生成时间**减少约 3×**。

#### SM 感知的权重同步

NCCL 集合操作作为 CUDA kernel 执行，**占用 GPU 的流式多处理器（SM）**。USER 通过可调配置**限制 NCCL CTA（协作线程阵列）的最大数量**，防止后台权重同步独占 GPU 执行资源：

- 节流 NCCL 的 SM 占用
- 保持异步权重更新下稳定的低延迟 rollout

---

## 三、学习框架设计

### 3.1 全异步流水线

在真实世界策略学习中，**数据收集而非计算**是主要瓶颈。物理交互无法加速，同步流水线中训练延迟会级联传播，迫使机器人暂停。

USER 组织学习为**全异步流水线**，四个阶段完全解耦：

```
数据生成 ──→ 数据传输 ──→ 训练 ──→ 权重同步
   ↑                                    │
   └────────────────────────────────────┘
```

- **数据生成侧**：多个 Env Worker 通过 Rollout Worker 在物理机器人上执行策略，持续流式输出观测和动作，不被训练阻塞。人类操作员可通过遥操作介入提供纠正或示范。Reward Worker 分配监督信号
- **训练侧**：Learning Worker 异步从缓冲区采样 mini-batch 更新参数
- 更新的权重定期同步回 Rollout Worker，形成闭环

#### 同步 vs 异步对比

| 模型 + 算法 | 指标 | 同步 | 异步 | 加速比 |
| --- | --- | --- | --- | --- |
| π₀ + HG-DAgger | 生成周期（s/episode） | 45.07 | 37.54 | 1.20× |
| π₀ + HG-DAgger | 训练周期（s/update） | 45.01 | 7.90 | **5.70×** |
| CNN + SAC | 生成周期（s/episode） | 20.29 | 13.11 | 1.55× |
| CNN + SAC | 训练周期（s/update） | 0.64 | 0.14 | **4.61×** |

异步流水线将 Peg Insertion 任务的收敛时间从 **8000+ 秒缩短到约 1500 秒**。

#### 权重同步间隔的影响

消融实验表明：

- 间隔 = 1（每次训练步都同步）：频繁的 episode 内权重更新导致策略非平稳，收敛慢甚至发散
- 间隔 = 32 或 64：确保稳定更新，收敛更快
- 较大的同步间隔保证了策略在一个 episode 内的一致性

### 3.2 持久化缓存感知缓冲区

真实世界学习面临长时间跨度、非平稳策略、网络故障、重启和人工干预等挑战。短生命周期的内存缓冲区无法满足需求。

USER 采用**持久化、基于索引的缓冲区**：

```
┌──── Memory ────┐   ┌──── Disk ────┐
│  All Indices   │   │              │
│                │   │  persist →   │
│  Cached Data   │   │ Persistent   │
│                │   │    Data      │
└────────────────┘   └──────────────┘
     ↑ Sample Window Size ↑
   Generate              Train
```

核心设计：

1. **存储与内存解耦**：轨迹异步写入磁盘，缓冲区仅存储轻量索引（含策略版本、时间戳、episode ID 等元数据）
2. **有界内存缓存 + FIFO 替换**：新样本先入缓存，满时旧条目被驱逐但索引保留在磁盘上
3. **透明重加载**：被驱逐的样本在被请求时透明地从磁盘重载
4. **支持时间和策略感知采样**：可按策略版本或时间范围采样

与纯内存缓冲区（Reverb、GEAR）对比：

| 设计 | 吞吐量 | 容量 | 持久化 | 崩溃恢复 |
| --- | --- | --- | --- | --- |
| 纯内存 | 最高 | 受限于 RAM | ✗ | ✗ |
| 纯磁盘 | < 50% | 无限 | ✓ | ✓ |
| **USER（缓存感知）** | **高**（接近纯内存） | **无限** | **✓** | **✓** |

缓存比例 $r = c/s$（缓存大小/缓冲区大小）越大，采样吞吐量越高。

### 3.3 可扩展的策略、算法和奖励

USER 对模型架构和学习算法保持**不可知**，通过统一接口支持异构组件共享同一执行和数据管道。

#### 策略层

| 策略类型 | 代表模型 | 特点 |
| --- | --- | --- |
| **轻量策略** | CNN/MLP（ResNet 风格） | 低延迟，适合实时控制 |
| **Flow Policy** | Flow Matching 策略 | 通过连续概率流表示动作 |
| **大规模 VLA** | π₀ / π₀.₅ | 多模态推理 + 连续动作输出 |

所有策略通过**统一 rollout 抽象**部署。

#### 算法层

| 算法 | 类型 | 适用场景 |
| --- | --- | --- |
| **SAC** | Off-policy RL | 样本高效的在线学习 |
| **SAC-Flow** | Off-policy RL（Flow 策略专用） | Flow 策略的 RL 训练 |
| **RLPD** | Off-policy RL + 离线数据 | 利用示范数据加速 |
| **HG-DAgger** | 交互式模仿学习 | VLA 大模型在线微调 |

Training worker 通过标准化的采样和更新 API 与策略交互，优化层可互换。

#### 奖励层

三种奖励来源，模块化设计：

1. **规则奖励**：基于末端执行器位姿的固定位置操作任务
2. **人工标注**：操作员通过脚踏板等接口标注二值成功/失败信号
3. **学习的奖励模型**：预训练 ResNet18 分类器，使用约 1600 帧数据（成功:失败 ≈ 1:3）训练

实验表明，学习的奖励模型在 Peg Insertion 任务上**达到与人工标注相当的性能**。

---

## 四、实验结果

### 4.1 多任务 RL 训练

在 5 个真实世界操作任务上验证框架的可扩展性：

| 任务 | 模型 | 算法 | 奖励类型 | 关键结果 |
| --- | --- | --- | --- | --- |
| Peg Insertion | CNN | SAC/RLPD | 规则（密集/稀疏） | ~2000s 收敛到近完美 |
| Peg Insertion | Flow | SAC Flow | 规则（稀疏） | 与 CNN 相当 |
| Charger Plugging | CNN/Flow | SAC/RLPD/SAC-Flow | 规则 | 亚毫米精度，~1500s 收敛 |
| Cap Tightening | CNN | RLPD | 人工（稀疏） | 快速收敛 |
| Pick-and-Place | CNN | RLPD | 人工（稀疏） | 动态更丰富，需较长训练 |
| Pick-and-Place | π₀ | HG-DAgger | / | SFT 39/60 → 在线训练后 **58/60** |
| Table Clean-up | π₀ | HG-DAgger | / | SFT 9/20 → 在线训练后 **16/20** |

HG-DAgger 微调 π₀ 在短时域 Pick-and-Place 上 **~30 分钟内仅用 ~200 个在线样本**达到 96% 成功率。

### 4.2 统一硬件抽象的优势

**多机器人并行训练**：
- 两台 Franka 同时执行不同任务，约 2500s 两个任务均收敛
- 与单机器人基线收敛速度匹配，数据效率通过并行收集得到提升

**异构机器人训练**：
- 7-DoF Franka + 6-DoF 低成本 ARX 机械臂
- 训练统一 CNN 策略完成多色按钮按压任务
- 两种机器人均成功收敛（约 2 小时），证明跨构型学习的可行性

### 4.3 分布式通道的通信效果

| 部署场景 | 分布式通道 | 总生成时间（s/episode） |
| --- | --- | --- |
| 跨域（千公里级） | 启用 | **21.98** |
| 跨域 | 未启用 | 69.27 |
| 同域 | 启用 | **17.30** |
| 同域 | 未启用 | 18.70 |

跨域部署中，分布式通道将 episode 生成时间**减少约 3×**。关键发现：跨域部署下边缘侧两节点间的通信效率**接近全同域设置**——USER 有效利用了本地高带宽链路。

### 4.4 持久化缓冲区验证

- 缓存比例 $r$ 越大，采样吞吐量越高
- 纯内存缓冲区吞吐最高但容量受限
- 纯磁盘缓冲区容量无限但吞吐不足内存的一半
- **USER 的设计在两者间取得平衡**，接近内存级吞吐的同时保持无限容量

---

## 五、局限性与未来方向

1. **算法覆盖有限**：目前仅集成 SAC、RLPD、SAC-Flow 和 HG-DAgger，缺少 PPO/GRPO 等 on-policy 算法（已在 RLinf-VLA 中实现）
2. **VLA 微调仅限 HG-DAgger**：未展示 VLA 的 RL 在线训练（如 π₀ + SAC），仅使用了模仿学习方式
3. **真实世界实验规模**：实验任务相对简单（Peg Insertion、Charger 等），未涉及开放世界泛化或长时域复杂任务
4. **安全性**：真实世界 RL 涉及试错探索，论文依赖阻抗控制器保障安全，但缺乏系统化的安全约束框架
5. **自动重置**：部分任务仍依赖人工重置（Cap Tightening、Pick-and-Place），限制了全自动长时间运行

---

## 六、个人思考

### 6.1 与 RLinf 系列的关系

USER 是 RLinf 系列的**第三篇系统论文**，定位清晰区分：

| 系统 | 定位 | 核心场景 |
| --- | --- | --- |
| [RLinf](RLinf_2025.md) | 通用大规模 RL 训练系统 | 仿真环境 + 大模型 |
| [RLinf-VLA](RLinf_VLA_2025.md) | VLA+RL 统一训练框架 | 仿真环境 + VLA 模型 |
| **RLinf-USER** | 真实世界在线策略学习系统 | **物理机器人 + 多种策略** |

三者共享同一代码库（github.com/RLinf/RLinf），体现了从仿真到真实、从算法到系统的逐步深入。

### 6.2 "机器人即加速器"的抽象

将机器人视为与 GPU 同等的一等硬件资源，这一抽象极具前瞻性。它意味着：

- 未来可以像管理 GPU 集群一样管理机器人集群
- 调度器可以统一分配计算和物理执行资源
- 为"机器人农场"级别的大规模真实世界学习奠定基础

这与当前 LLM 训练中 GPU 集群管理的成熟实践形成呼应，是具身智能走向工程化的关键一步。

### 6.3 异步流水线的权衡

全异步设计带来显著加速（训练吞吐 4.6×–5.7×），但也引入了策略延迟问题：rollout 使用的策略可能落后于最新训练的策略几个更新步。消融实验表明，权重同步间隔过小（如 1）反而导致发散，说明一定程度的"策略滞后"在真实世界中是可接受甚至有益的——类似于 off-policy RL 中的经验重放机制。

### 6.4 与 SOP 的定位差异

SOP（Scalable Online Post-training）同样面向 VLA 在线后训练，但聚焦于**同构机器人**的大规模 VLA 在线后训练。USER 的差异化在于：

- **异构机器人**支持（不同 DoF、不同构型）
- **云-边协同**能力（跨域网络隧道）
- **多范式统一**（RL + IL + Human-in-the-loop）
- **持久化缓冲区**支持长时间运行

两者代表了真实世界策略学习系统的两个演进方向：规模化（SOP）vs 通用化（USER）。

### 6.5 持久化缓冲区的工程价值

持久化缓存感知缓冲区看似简单，但对真实世界 RL 至关重要：

- 真实世界实验可能运行数天甚至数周，期间不可避免遇到网络中断、机器人故障、训练暂停
- 传统内存缓冲区在崩溃后丢失所有数据，昂贵的真实世界数据需要重新收集
- USER 的设计保证了**数据不丢失 + 可恢复 + 可跨训练阶段复用**

这是将 RL 从"实验室玩具"推向"工程可部署"的关键基础设施。

---

## 参考

- **RLinf**：同一团队的通用 RL 训练系统，提供底层 M2Flow 调度框架
- **RLinf-VLA**：同一团队的 VLA+RL 统一训练框架，面向仿真环境
- **SERL**：面向真实世界机器人 RL 的先驱系统，但主要支持单机器人/小模型
- **SAC-Flow**：同一团队的 Flow 策略 RL 算法，USER 框架内集成
- **π₀ / π₀.₅**：USER 中集成的大规模 VLA 模型
- **SOP**：同期的 VLA 在线后训练系统，面向同构机器人规模化
- **HG-DAgger**：交互式模仿学习算法，USER 用于 VLA 在线微调
- **Reverb**：Google 的高性能经验回放框架，USER 的持久化缓冲区解决了其内存受限问题
