# TACO：以反探索原则驱动 VLA 测试时扩展——基于伪计数的动作验证框架

> 论文：*Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach*
> 作者：Siyuan Yang, Yang Zhang, Haoran He, Ling Pan, Xiu Li, Chenjia Bai, Xuelong Li
> 机构：中国电信人工智能研究院、中国科学技术大学、清华大学、香港科技大学
> 发布时间：2025年12月
> 🔗 [arXiv](https://arxiv.org/abs/2512.02834) | [项目主页](https://vla-anti-exploration.github.io/) | [代码](https://github.com/breez3young/TACO/)
> 分类标签：`Test-Time Scaling` `Anti-Exploration` `Pseudo-Count` `VLA`

---

## 一句话总结

TACO 将 offline RL 中的反探索（anti-exploration）原则应用到 VLA 推理阶段，通过轻量级的 Coin Flipping Network 估计动作伪计数，在多个候选动作中选择最"in-support"的那个，无需修改模型参数即可显著提升推理稳定性和成功率。

---

## 一、问题与动机

### 1.1 VLA 推理时的脆弱性

VLA 模型通过 flow matching 或 diffusion 目标训练，能从大规模多模态数据中学到丰富的行为模式。但论文发现了一个被严重忽视的问题：**推理时的噪声敏感性**。

具体而言，对于同一个经过 SFT 微调的 VLA（如 $\pi_0$ 或 RDT），仅仅更换去噪过程的初始随机噪声向量，成功率就可能从 **0% 剧烈波动到 80%**（见 Figure 1）。这意味着 VLA 的推理过程极其脆弱，性能高度依赖于随机采样的噪声。

### 1.2 不稳定性的根源：冗余动作模态

论文将这种脆弱性归因于两个层面的**分布偏移（distribution shift）**：

**(i) 预训练遗留的冗余模态：** VLA 在预训练阶段吸收了来自多种数据源的广泛动作模态（不同机器人构型、不同操作风格、不同场景）。SFT 只用了少量下游任务数据，很难将输出分布快速收窄到下游任务所需的成功模态。因此，微调后的策略分布仍保留了大量与任务成功无关的多余模态。

**(ii) SFT 数据本身的多模态性：** 微调数据通常由多个人类遥操作者或脚本策略收集，不同操作者有不同的执行风格，其中一些可能是次优甚至不可取的策略。

数学上，微调后的策略 $\pi_\theta$ 近似了整个数据集分布：

$$\pi_\theta \approx p_D = w_1 \pi^* + \sum_{k>1} w_k p_k$$

其中 $\pi^*$ 是期望的成功模态，$p_{k>1}$ 是次优或不相关模态。当采样到指向非成功模态 $p_{k>1}$ 的噪声时，就会生成失败的动作。

### 1.3 为什么不用 RL 来解决？

一个自然的想法是用 RL 对策略进行后训练来消除冗余模态。但对于 flow matching / diffusion 类 VLA，RL 更新面临根本性困难：

- 去噪过程涉及复杂的采样动力学，无法直接对整个去噪链做标准的策略梯度更新
- 梯度需要回传通过多步去噪过程，计算代价极高
- 修改模型参数可能破坏预训练获得的泛化能力

### 1.4 TACO 的核心思路

TACO 采用了一种巧妙的替代方案：**不改模型参数，而是在推理时从多个候选动作中选最好的**。这本质上是一种 test-time scaling（TTS）策略——通过增加推理时的计算来换取性能提升。

选择标准来自 offline RL 的**反探索（anti-exploration）原则**：优先选择在 SFT 数据集中出现频率最高的动作（即最"in-support"的动作），因为高频动作更可能对应成功模态。

---

## 二、预备知识

### 2.1 问题设定

语言条件化的机器人操作任务，模仿学习设定。给定预收集的 SFT 数据集 $\mathcal{D}_\text{sft} = \{\tau_1, \tau_2, \dots, \tau_n\}$，每条轨迹 $\tau = (\mathbf{o}_{1:T}, l, \mathbf{a}_{1:T})$ 包含语言指令 $l$、观测序列（多视角 RGB 图像 + 本体感觉状态）和动作序列。

### 2.2 Coin Flipping Network（CFN）

CFN 是 TACO 的核心工具，用于估计伪计数（pseudo-count）。其原理源自一个优雅的统计推断：

**基本思想：** 每次遇到一个状态 $s$，就给它配一个随机掷硬币向量 $\mathbf{c}_i \sim \{-1, 1\}^d$。构建数据集 $\mathcal{D}_\text{cfn} = \{(s_i, \mathbf{c}_i)\}$，然后训练一个网络 $f_\phi$ 做回归：

$$\min_\phi \mathbb{E}_{(s_i, \mathbf{c}_i) \sim \mathcal{D}_\text{cfn}} \left[\|f_\phi(s_i) - \mathbf{c}_i\|^2\right]$$

**关键推导：** 如果状态 $s$ 在数据集中出现了 $m$ 次，最优解满足 $f_\phi^*(s) = \frac{1}{m}\sum_{i=1}^{m} \mathbf{c}_i$。由于每个 $c_{ij}$ 是公平硬币翻转，根据大数定律：

$$\frac{1}{d}\|f_\phi(s)\|^2 = \frac{1}{d}\sum_{j=1}^{d} \mathbb{E}\left[\left(\frac{\sum_{i=1}^{m} c_{ij}}{m}\right)^2\right] = \frac{1}{m}$$

用大白话说：出现次数越多的状态，其对应的硬币翻转向量越趋向互相抵消（正负相消），网络输出的 L2 范数就越小。因此 $\|f_\phi(s)\|^2/d \approx 1/N(s)$，即**范数的平方与访问次数成反比**。

---

## 三、核心方法

TACO 分为两个阶段：Stage 1 训练伪计数估计器，Stage 2 在推理时用它做动作选择。

### 3.1 从 Offline RL 到 Test-Time Scaling 的理论推导

这是 TACO 最优雅的部分——将 offline RL 的反探索目标严格简化为"选最高频动作"。

**起点：** Offline RL 中的反探索目标：

$$\mathbf{a}^* = \arg\max_\mathbf{a} [Q(s, \mathbf{a}) - b(s, \mathbf{a})]$$

其中 $Q$ 是动作价值函数，$b$ 是反探索惩罚（对 OOD 动作施加高代价）。

**简化步骤一：** 将任务建模为 contextual bandit（简化但合理的假设），执行动作块 $\mathbf{a}_{1:H}$ 后得到二值奖励（成功=1，失败=0）。此时 $Q$-函数等价于成功概率：

$$\mathbf{a}^*_{1:H} = \arg\max_{\mathbf{a}_{1:H}} [P(r=1|\mathbf{o}, l, \mathbf{a}_{1:H}) - b(\mathbf{o}, l, \mathbf{a}_{1:H})]$$

**简化步骤二：** 核心假设——**SFT 数据集是示范性的**（demonstrative），即数据密度与任务成功率正相关：

$$P(r=1|\mathbf{o}, l, \mathbf{a}_{1:H}) \propto p_{\mathcal{D}_\text{sft}}(\mathbf{o}, l, \mathbf{a}_{1:H})$$

直觉理解：在 SFT 数据集中出现频率高的动作更可能是成功的，低频动作更可能是次优的。

**简化步骤三：** 设反探索惩罚为基于计数的 bonus $b(\mathbf{o}, l, \mathbf{a}_{1:H}) = 1/N_{\mathcal{D}_\text{sft}}(\mathbf{o}, l, \mathbf{a}_{1:H})$，代入后整个目标等价于：

$$\mathbf{a}^*_{1:H} = \arg\max_{\mathbf{a}_{1:H}} [N_{\mathcal{D}_\text{sft}}(\mathbf{o}, l, \mathbf{a}_{1:H})]$$

**结论：** 反探索目标被严格简化为——在候选动作中选择**访问计数最大**的那个。这为 TACO 的 test-time scaling 提供了坚实的理论基础：它不是一个临时的 heuristic，而是 offline RL 反探索目标的等价实现。

### 3.2 耦合伪计数估计器（Coupled Pseudo-Count Estimator）

#### 3.2.1 核心设计：利用 VLA 内部表征

CFN 需要一个好的输入表征 $z = \text{Enc}(\mathbf{o}, l, \mathbf{a})$。TACO 做了一个关键的设计选择：**不单独训练编码器，而是直接复用 VLA 自身的内部表征 $h_\theta$**。

具体而言，CFN $f_\phi$ 被实例化为一个轻量级 MLP head，接收 VLA 最后隐藏层的第一个 action token 作为输入。这种**耦合设计**有两大优势：

- 充分利用 VLA 在大规模预训练中学到的丰富视觉-语言-动作理解能力
- 几乎不增加额外计算，因为内部表征是 VLA 推理的副产品

#### 3.2.2 High-Fidelity Feature Search

耦合设计对 diffusion/flow 类 VLA 引入了一个棘手问题：这类模型训练时只见过**加噪动作** $\{a_\sigma\}$，从未见过干净动作 $\{a\}$。直接喂入干净动作提取的 $h_\theta$ 可能不在 VLA 的特征空间内，产生无意义的表征。

TACO 提出了一个巧妙的搜索过程。对每个数据点 $(\mathbf{o}, l, \mathbf{a}) \in \mathcal{D}_\text{sft}$：

1. 用 $N$ 个不同噪声水平 $\{\sigma_i\}_{i=1}^N$ 查询 VLA：

$$\{(\mathbf{a}^{(i)}_\text{pre}, h^{(i)}_\theta)\}_{i=1}^N = \{\text{VLA}(\mathbf{o}, l, \mathbf{a}_{\sigma_i})\}_{i=1}^N$$

2. 选择预测动作最接近真值的那个噪声水平对应的特征：

$$i^\star = \arg\min_{i \in \{1,...,N\}} \|\mathbf{a}^{(i)}_\text{pre} - \mathbf{a}\|_2$$

选出的特征 $h^{(i^\star)}_\theta$ 同时满足两个条件：
- **In-distribution**：它是从加噪输入生成的，属于 VLA 正常工作的特征空间
- **High-fidelity**：它对应的预测动作最接近真值，因此最能代表干净动作的语义

#### 3.2.3 CFN 训练

在获得高保真特征集 $\mathcal{D}_h = \{h^{(i^\star)}_\theta\}$ 后，按标准 CFN 训练流程（Eq. 1）训练 MLP head $f_\phi$。训练完成后，任意特征 $h_\theta$ 的伪计数为：

$$\hat{N}_{\mathcal{D}_\text{sft}}(\mathbf{o}, l, \hat{\mathbf{a}}) = \hat{N}_{\mathcal{D}_\text{sft}}(h_\theta) \propto \frac{1}{\|f_\phi(h_\theta)\|^2}$$

**CFN 网络架构细节：**

CFN 是一个轻量 MLP，包含四个组件：

1. **Feature Scaling**：所有输入特征乘以常数因子 10，放大特征间距离
2. **Input Projection**：FC 层 + GELU + LayerNorm
3. **MLP Block**：两个 FC 层（先升 4x 再降回），残差连接 + LayerNorm + Dropout
4. **Output Projection**：映射到目标输出维度（$d=20$）

此外，引入了**随机先验初始化**机制：将 CFN 输出分解为可训练部分 $\hat{f}_\phi(h)$ 和冻结的随机先验 $f_\text{prior}(h)$，使未见过的特征区域初始化伪计数为 1。

### 3.3 Test-Time Scaling 推理流程

推理时采用 **generate-then-verify** 的两阶段流程：

**生成阶段：**

1. 给定观测 $\mathbf{o}_t$ 和语言指令 $l$，先计算一次 context KV cache：$\mathcal{C}_\text{KV} = \text{ContextForward}(\mathbf{o}_t, l)$
2. 采样 $M$ 个不同初始噪声 $\{\epsilon_i\}_{i=1}^M \sim \mathcal{N}(0, I)$
3. 利用共享的 KV cache，**批并行**执行去噪过程，得到 $M$ 个候选动作块及其内部表征：

$$(\hat{\mathbf{a}}^{(i)}_{t:t+H}, h^{(i)}_\theta) = \pi_\theta(\epsilon_i, \mathcal{C}_\text{KV})$$

**验证阶段：**

4. CFN 对每个候选评分：$\hat{N}_{\mathcal{D}_\text{sft}}(\mathbf{o}_t, l, \hat{\mathbf{a}}^{(i)}_{t:t+H}) = 1/\|f_\phi(h^{(i)}_\theta)\|^2$
5. 选择伪计数最大（范数最小）的动作执行：

$$\hat{\mathbf{a}}^*_{t:t+H} = \hat{\mathbf{a}}^{(i^*)}_{t:t+H}, \quad i^* = \arg\max_{i \in \{1,...,M\}} \frac{1}{\|f_\phi(h^{(i)}_\theta)\|^2}$$

### 3.4 KV Cache 优化：让 TTS 变得实际可用

朴素地采样 $M$ 个候选会引入 $O(M)$ 倍的计算开销。TACO 的关键洞察是：VLA transformer backbone 中最昂贵的计算只依赖于共享的上下文 token $(\mathbf{o}, l)$，与动作噪声无关。因此：

- 只计算一次 context 的 Key/Value cache
- 所有 $M$ 个去噪过程共享这个 cache

当 $M=32$ 时，这种优化将推理时间减少了 **73.2%**（相比朴素多 batch 并行推理）。

---

## 四、实验结果

### 4.1 实验设置

**四个仿真基准 + 真实世界：**

- **RoboTwin 1.0**：双臂夹持器操作，8 个任务
- **RoboTwin 2.0**：大规模双臂操作，40 个任务
- **LIBERO-long**：长时域决策，10 个任务
- **Simpler-WindowX**：真实到仿真评估，4 个任务
- **真实世界**：RealMan75 双臂平台，5 个任务

**基线策略：** 主要在 $\pi_0$（flow matching VLA）和 $\pi_{0.5}$（$\pi_0$ 的增强版）上评估，也包括 OpenVLA（自回归 VLA）。

### 4.2 仿真实验主要结果

**RoboTwin 1.0（$\pi_0$ 为基线）：**

| 任务 | $\pi_0$ | $\pi_0$ + TACO | 提升 |
| --- | --- | --- | --- |
| Block Handover | 41.0 | 62.0 | +21 |
| Container Place | 25.0 | 40.0 | +15 |
| Dual Bottles Pick Easy | 60.0 | 70.0 | +10 |
| Shoe Place | 42.0 | 50.0 | +8 |
| **平均** | **32.2** | **41.3** | **+9.1** |

**RoboTwin 2.0（$\pi_{0.5}$ 为基线，40 个任务）：**

| | RDT | $\pi_{0.5}$ | $\pi_{0.5}$ + TACO |
| --- | --- | --- | --- |
| **平均成功率** | 34.6 | 59.3 | **64.0 (+4.7)** |

其中多个任务提升超过 10 个百分点：Beat Block Hammer (+10)、Move Can Pot (+15)、Place Phone Stand (+10)、Stack Blocks Two (+10)、Stamp Seal (+12)、Handover Block (+12)。

**Simpler-WindowX（$\pi_0$ 为基线）：**

| 任务 | RT-1-X | Octo | RoboVLM | SpatialVLA | $\pi_0$ | $\pi_0$ + TACO |
| --- | --- | --- | --- | --- | --- | --- |
| Spoon on Towel | 0.0 | 12.5 | 29.2 | 16.7 | 36.0 | **52.0** (+16) |
| Carrot on Plate | 4.2 | 8.3 | 25.0 | 25.0 | 42.0 | **52.0** (+10) |
| Eggplant in Basket | 0.0 | 43.1 | 58.3 | 100.0 | 80.0 | **88.0** (+8) |
| **平均** | 1.1 | 16.0 | 31.3 | 42.7 | 48.0 | **55.5** (+7.5) |

**LIBERO-long（$\pi_{0.5}$ 和 OpenVLA 为基线）：**

| 方法 | 平均成功率 |
| --- | --- |
| $\pi_{0.5}$ | 94.8 |
| $\pi_{0.5}$ + TACO | **96.6** (+1.8) |
| OpenVLA (reproduced) | 54.0 |
| OpenVLA + TACO | **60.0** (+6.0) |
| OpenVLA + RoboMonkey (7B VLM verifier) | 56.5 (+6.7) |

值得注意的是，TACO 仅用一个轻量 MLP 伪计数器就达到了与 RoboMonkey（训练了一个 7B 参数的 VLM 验证器）相当的提升效果。

### 4.3 真实世界实验

在 RealMan75 双臂平台上，5 个任务，每步同时采样 30 个候选动作块：

| 任务 | $\pi_0$ | $\pi_0$ + TACO | 提升 |
| --- | --- | --- | --- |
| Receive Book | 70.0 | 80.0 | +10 |
| Storage Charger | 15.0 | 40.0 | +25 |
| Paper and Pen | 15.0 | 40.0 | +25 |
| Laptop | 40.0 | 55.0 | +15 |
| Pick Books | 60.0 | 65.0 | +5 |
| **平均** | **40.0** | **56.0** | **+16** |

在长时域任务（Paper and Pen, Laptop）上提升尤其显著。论文观察到基线策略主要以两种方式失败：(1) 抓取位置不精确；(2) 遥操作数据质量不佳导致的振荡行为（如夹持器反复开合）。TACO 有效抑制了这些次优模态。

### 4.4 推理效率

在单张 RTX 4090 上评估 $\pi_0$：

| 采样数 $M$ | 朴素并行 | KV Cache 优化 | 加速比 |
| --- | --- | --- | --- |
| 1 | ~0.1s | ~0.1s | - |
| 8 | ~0.7s | ~0.3s | ~2.3x |
| 32 | ~2.8s | ~0.75s | ~3.7x (73.2%) |

实际部署中，由于动作块长度为 20 步，在 30Hz 控制频率下每次推理有约 0.67s 的时间预算，采样 30 个候选在 KV cache 优化后完全可行。

### 4.5 消融实验

在 RoboTwin 1.0 上基于 $\pi_0$ 的消融：

| 变体 | Block Handover | Container Place | Diverse Bottles Pick | 平均 |
| --- | --- | --- | --- | --- |
| **$\pi_0$ + TACO（完整）** | **62.0** | **40.0** | **27.0** | **43.0** |
| w/o CFN（直接拟合动作误差） | 52.0 | 32.0 | 26.0 | 36.7 |
| w/o Feature Scaling | 51.0 | 37.0 | 30.0 | 39.3 |
| w/o Internal Feature（用 CNN+MLP 编码器） | 48.0 | 33.0 | 23.0 | 34.7 |

**关键发现：**

1. **CFN vs 直接拟合动作误差**：直接拟合"特征→动作误差"的映射需要同时建模好坏所有特征，学习难度大且容易过拟合。CFN 只需区分高频/低频特征，任务更简单。

2. **Internal Feature vs 独立编码器**：使用 CNN+MLP 独立编码器并与 CFN 联合训练时，提取的特征趋于高度相似，伪计数难以区分，且无法利用 VLA 对场景和动作的理解能力。

3. **Feature Scaling 的必要性**：不做特征缩放会降低特征间的可区分度。

**伪计数器的替代选择（RND vs CFN）：**

| 方法 | Block Handover | Container Place | Diverse Bottles Pick | 平均 |
| --- | --- | --- | --- | --- |
| $\pi_0$ | 41.0 | 25.0 | 21.0 | 29.0 |
| $\pi_0$ + TACO (RND) | 54.0 | 33.0 | **30.0** | 39.0 |
| $\pi_0$ + TACO (CFN) | **62.0** | **40.0** | 27.0 | **43.0** |

CFN 在平均性能上优于 RND，两者都显著优于基线。

---

## 五、局限性与未来方向

### 5.1 依赖 SFT 数据分布的假设

TACO 的理论推导依赖"SFT 数据密度与成功率正相关"的假设。当数据集中包含大量低质量或失败轨迹时，高频动作不一定对应最优行为。这限制了 TACO 在数据质量不均匀场景下的适用性。

### 5.2 无法评估纯合成动作

TACO 只能在 VLA 生成的候选动作中选择，无法评估完全由外部扰动产生的新动作。如果所有候选都不够好，TACO 只能选出"矮子里面的将军"。

### 5.3 依赖 VLA 的表征质量

伪计数估计器直接建立在 VLA 的内部表征之上，如果 VLA 的表征质量差（如预训练不充分），CFN 的判别能力也会受限。

### 5.4 推理时间与实时性的权衡

虽然 KV cache 优化大幅降低了开销，但在高频控制（>30Hz）或资源受限的嵌入式平台上，同时采样几十个候选仍可能面临延迟挑战。

---

## 六、个人思考

### 6.1 与 offline RL 后训练方法的互补性

TACO 和 RISE、$\pi_{0.6}^*$ 等 RL 后训练方法解决的是同一个问题——VLA 微调后的分布偏移——但从截然不同的角度切入：

- **RL 后训练**（如 RISE、RECAP）：修改策略参数，在训练时消除冗余模态
- **TACO**：保持参数不变，在推理时过滤冗余模态

两者不是竞争关系，而是**天然互补**的。经过 RL 后训练的 VLA 仍可能在推理时因噪声采样到次优模态，此时 TACO 可以作为"最后一道防线"。反过来，TACO 也不能替代 RL 后训练，因为它无法让策略学到新的行为。

### 6.2 与 RISE 的有趣对比

RISE 和 TACO 都使用了某种形式的"验证器"来评估候选动作：
- RISE 用世界模型（动力学 + 价值）在训练时想象并评估
- TACO 用伪计数估计器在推理时实时评估

TACO 的优势在于**零训练时间成本和推理时的即插即用性**，但它的评估信号只有"是否 in-support"（频率信息），没有 RISE 那种"执行后价值如何变化"的因果推理能力。

### 6.3 理论简洁性

论文从 offline RL 的反探索目标出发，通过三步简化（contextual bandit → 数据密度假设 → 计数 bonus）严格推导出"选最高频动作"这个极简目标，整个推导链条清晰且有说服力。这种"把复杂问题简化为简单操作"的思路值得借鉴。

### 6.4 CFN 作为通用工具

CFN 本身是一个通用的访问频率估计器，不限于 VLA 领域。它在 LLM 偏好优化中也有应用（Bai et al., 2025），说明这种基于伪计数的方法有广泛的适用性。

---

## 参考

- **$\pi_0$** (Black et al., 2024)：TACO 的主要评估基线，flow matching VLA
- **$\pi_{0.5}$** (Black et al., 2025)：TACO 在 RoboTwin 2.0 和 LIBERO 上的评估基线
- **RECAP / $\pi_{0.6}^*$** (Black et al., 2025)：基于优势条件化的离线 RL 方法，与 TACO 互补
- **RISE** (2026)：通过想象空间 RL 消除分布偏移，与 TACO 解决同一问题但方法论不同
- **CFN** (Lobel et al., 2023)：Coin Flipping Network 原论文，TACO 的核心估计工具
- **RoboMonkey** (Kwok et al., 2025)：另一个 VLA test-time scaling 方法，使用 7B VLM 做验证器
- **Anti-Exploration** (Rezaeifar et al., 2022)：offline RL 中的反探索原则，TACO 的理论基础
