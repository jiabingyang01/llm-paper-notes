# π₀.₆*：通过经验学习的 VLA——RECAP 方法详解

> 论文：*π₀.₆*: a VLA That Learns From Experience*
> 机构：Physical Intelligence
> 发布时间：2025年11月
> 🔗 [arXiv](https://arxiv.org/abs/2511.14759) | [项目主页](https://pi.website/blog/pistar06)

---

## 一句话总结

提出 RECAP（RL with Experience and Corrections via Advantage-conditioned Policies），一种基于**优势条件化（advantage conditioning）** 的离线 RL 框架，将自主 rollout、专家干预和演示数据统一整合，通过分布式价值函数估计优势并条件化策略训练，在咖啡制作、叠衣服、组装箱子等长时域复杂任务上**吞吐量翻倍、失败率减半**。

---

## 一、问题与动机

### 1.1 模仿学习的天花板

VLA 模型的标准训练方式是模仿学习（IL）：从专家演示中学习。这种方式存在根本性限制：

- **复合误差（compounding errors）**：策略在部署时偏离训练分布后，误差逐步累积，最终导致任务失败
- **性能上界 = 演示数据质量**：模仿学习最好也只能做到和演示数据一样好，无法超越演示者的水平
- **缺乏自我纠错能力**：训练数据全是"正确"操作，策略从未见过失败情况，不知道如何从错误中恢复

对于需要**精确力控**（如拧咖啡手柄）、**长时程序列**（如叠衣服 10+ 步骤）、**可变形物体操作**（如折叠各种布料）的复杂任务，这些缺陷尤为致命。

### 1.2 现有 RL 方法面临的挑战

虽然 RL 是突破模仿学习天花板的自然选择，但将 RL 应用于大规模 VLA 面临三重挑战：

1. **可扩展性**：需要能高效处理大容量 flow matching / diffusion 模型，现有的 PPO 等在线方法在大模型上不稳定
2. **异构数据利用**：需要统一利用演示、自主 rollout（好的和差的）、专家干预等多源异构数据
3. **真实世界反馈**：奖励信号模糊、稀疏甚至随机——一个 5 分钟的 episode 只在最后得到一个成功/失败标签

### 1.3 RECAP 的核心洞察

RECAP 的关键创新在于将 RL 问题转化为**条件化监督学习**：

- 训练一个**分布式价值函数**来评估每个状态-动作对的质量
- 将价值函数的输出**二值化为优势指标** $I_t$（"这个动作是否比平均水平好？"）
- 策略以这个优势指标为额外条件输入进行训练——在**所有数据**上训练（不丢弃差数据），但通过优势条件区分好坏
- 推理时将优势条件设为"正面"，引导策略输出最优动作

这种方式完美绕开了 PPO 等策略梯度方法在 flow matching 模型上的困难，且天然支持离线多源数据。

---

## 二、预备知识

### 2.1 强化学习基础

标准 RL 设定中，策略 $\pi(\mathbf{a}_t|\mathbf{o}_t)$ 根据观测选择动作。轨迹 $\tau = (\mathbf{o}_0, \mathbf{a}_0, \cdots, \mathbf{o}_T)$ 的分布由策略和环境动力学共同决定：

$$\rho_\pi(\tau) = p(\mathbf{o}_0) \prod_{t=0}^{T-1} \pi(\mathbf{a}_t|\mathbf{o}_t) p(\mathbf{o}_{t+1}|\mathbf{o}_t, \mathbf{a}_t)$$

目标是最大化累积回报 $J(\pi) = \mathbb{E}_{\tau \sim \rho_\pi}\left[\sum_{t=0}^T r_t\right]$（本文不使用折扣因子）。

**价值函数**定义为从状态 $\mathbf{o}_t$ 开始的期望累积奖励：

$$V^\pi(\mathbf{o}_t) = \mathbb{E}_{\tau_{t+1:T}}\left[\sum_{t'=t}^T r_{t'}\right]$$

**n-step 优势函数**衡量某个动作相对于当前策略平均水平的好坏：

$$A^\pi(\mathbf{o}_t, \mathbf{a}_t) = \mathbb{E}_{\rho_\pi(\tau)}\left[\sum_{t'=t}^{t+N-1} r_{t'} + V^\pi(\mathbf{o}_{t+N})\right] - V^\pi(\mathbf{o}_t)$$

### 2.2 正则化 RL 与优势条件化策略提取

在实际 RL 中，我们通常不只是最大化奖励，还要限制策略不要偏离参考策略 $\pi_{\text{ref}}$ 太远（正则化）：

$$\mathcal{J}(\pi, \pi_{\text{ref}}) = \mathbb{E}_{\tau \sim \rho_{\pi_\text{ref}}}\left[\sum_{t=0}^T \gamma^t r_t\right] - \beta \mathbb{E}_{\mathbf{o} \sim \rho_{\pi_\text{ref}}}[D(\pi(\cdot|\mathbf{o}) \| \pi_{\text{ref}}(\cdot|\mathbf{o}))]$$

当 $D$ 取 KL 散度时，最优策略有经典闭式解：

$$\hat{\pi}(\mathbf{a}|\mathbf{o}) \propto \pi_{\text{ref}}(\mathbf{a}|\mathbf{o}) \exp(A^{\pi_{\text{ref}}}(\mathbf{o}, \mathbf{a}) / \beta)$$

**优势条件化策略提取**是一个关键的替代方案。定义改进概率 $p(I | A^{\pi_{\text{ref}}}(\mathbf{o}, \mathbf{a}))$，用单调递增函数 $g$ 度量动作相对于 $\pi_{\text{ref}}$ 的改进可能性。若定义策略为：

$$\hat{\pi}(\mathbf{a}|\mathbf{o}) \propto \pi_{\text{ref}}(\mathbf{a}|\mathbf{o}) \cdot p(I | A^{\pi_{\text{ref}}}(\mathbf{o}, \mathbf{a}))^\beta$$

则可以保证 $\mathcal{J}(\hat{\pi}) \geq \mathcal{J}(\pi_{\text{ref}})$——新策略一定不比参考策略差。

这就是 RECAP 策略提取方法的理论基础。

---

## 三、核心方法：RECAP

RECAP 由三个可迭代执行的子流程组成，每一轮迭代使用不同数据但算法逻辑完全相同：

### 3.1 数据收集

在目标任务上部署 VLA，收集异构数据：

- **自主 rollout**：机器人独立执行任务，每个 episode 标注成功/失败结果
- **专家干预（可选）**：人类遥操作者在机器人即将失败时接管，提供纠正动作
- **人工演示**：标准的高质量演示数据

对于专家干预数据，优势指标 $I_t$ 被强制设为 True（假设专家总是提供好的纠正）。所有数据——包括成功和失败的——都加入数据集 $\mathcal{D}_\ell$。

### 3.2 分布式价值函数训练

RECAP 训练一个**多任务分布式价值函数** $V^{\pi_{\text{ref}}}$，而非传统的标量价值函数。

**为什么要"分布式"？**

传统价值函数输出单个标量，对于高方差的机器人任务容易不稳定。分布式价值函数将返回值 $R_t(\tau)$ 离散化为 $B = 201$ 个 bin，输出一个概率分布 $p_\phi(V|\mathbf{o}_t, \ell) \in \Delta_B$：

$$\min_\phi \mathbb{E}_{\tau \in \mathcal{D}} \left[\sum_{\mathbf{o}_t \in \tau} H(R_t^B(\tau), p_\phi(V|\mathbf{o}_t, \ell))\right]$$

其中 $R_t^B$ 是将经验回报 $R_t(\tau) = \sum_{t'=t}^T r_{t'}$ 离散化到 201 个 bin 后的分布，$H$ 是交叉熵。

**连续价值和优势的提取：**

从离散分布恢复连续价值：$V^{\pi_{\text{ref}}}(\mathbf{o}_t, \ell) = \sum_{b \in [0,B]} p_\phi(V=b|\mathbf{o}_t) v(b)$，其中 $v(b)$ 是第 $b$ 个 bin 的中心值。

**奖励定义——极简设计：**

$$r_t = \begin{cases} 0 & \text{if } t = T \text{ 且成功} \\ -C_{\text{fail}} & \text{if } t = T \text{ 且失败} \\ -1 & \text{otherwise} \end{cases}$$

含义：每走一步扣 1 分（鼓励快速完成），成功时不扣分，失败时扣大额惩罚 $C_{\text{fail}}$。价值函数实际预测的是（取负后的）剩余步数，归一化到 $(-1, 0)$。

**架构设计：** 价值函数使用与 VLA 相同的架构（SigLIP 400M + Gemma 3），但用更小的 VLM 骨干（670M 参数），初始化自预训练 VLM。与 VLA 一同训练时，增加极少的推理成本。

### 3.3 优势条件化策略训练

这是 RECAP 最核心的创新。

**第一步：计算优势并二值化**

对数据集中每个状态-动作对，用价值函数计算 n-step 优势 $A^{\pi_{\text{ref}}}(\mathbf{o}_t, \mathbf{a}_t, \ell)$。然后通过任务相关的阈值 $\epsilon_\ell$ 二值化为改进指标：

$$I_t = \mathbb{1}\left(A^{\pi_{\text{ref}}}(\mathbf{o}_t, \mathbf{a}_t, \ell) > \epsilon_\ell\right)$$

$\epsilon_\ell$ 的设定策略：预训练阶段选择使约 30% 的演示数据具有正优势的值；微调阶段选择使约 40% 的 rollout 数据具有正优势的值。

**第二步：条件化训练——类似 classifier-free guidance**

策略模型接收优势指标 $I_t$ 作为额外文本输入——当 $I_t = \text{True}$ 时输入 "Advantage: positive"，否则输入 "Advantage: negative"。优势指标插入在语言 token $\hat{\ell}$ 之后、动作 token 之前。

训练目标是最小化条件化负对数似然：

$$\min_\theta \mathbb{E}_{\mathcal{D}_{\pi_{\text{ref}}}} \left[-\log \pi_\theta(\mathbf{a}_t | \mathbf{o}_t, \ell) - \alpha \log \pi_\theta(\mathbf{a}_t | I_t, \mathbf{o}_t, \ell)\right]$$

$$\text{where } I_t = \mathbb{1}\left(A^{\pi_{\text{ref}}}(\mathbf{o}_t, \mathbf{a}_t, \ell) > \epsilon_\ell\right)$$

这个目标有两项：

- **第一项** $-\log \pi_\theta(\mathbf{a}_t | \mathbf{o}_t, \ell)$：标准模仿学习损失（无条件），在所有数据上学习基本行为
- **第二项** $-\alpha \log \pi_\theta(\mathbf{a}_t | I_t, \mathbf{o}_t, \ell)$：优势条件化损失，学习"好动作长什么样 vs 差动作长什么样"

$\alpha$ 是权衡超参数。训练时随机以 30% 的概率丢弃优势条件（dropout），使模型可以同时作为条件/无条件策略使用，为推理时的 CFG 做准备。

**与 classifier-free guidance 的深层联系：**

这种设计与扩散模型中的 classifier-free guidance（CFG）有深刻的对应关系。训练后，我们可以通过设置 $\beta > 1$ 在推理时进一步锐化策略：

$$\hat{\pi}(\mathbf{a}_{t:t+H}|\mathbf{o}_t, \ell) \propto \pi_{\text{ref}}(\mathbf{a}_{t:t+H}|\mathbf{o}_t, \ell) \left(\frac{\pi_{\text{ref}}(\mathbf{a}_{t:t+H}|I_t, \mathbf{o}_t, \ell)}{\pi_{\text{ref}}(\mathbf{a}_{t:t+H}|\mathbf{o}_t, \ell)}\right)^\beta$$

由于模型同时学了条件分布和无条件分布的梯度，可以直接在 flow matching 推理中沿着这个梯度方向走，无需额外训练。

**关键优势——在所有数据上训练：**

与 AWR 等方法不同，RECAP **不丢弃任何数据**。低优势数据同样参与训练，策略通过优势条件学习到"在这种情况下不要这样做"。这在数据宝贵的真实机器人场景中至关重要。

### 3.4 完整流程（Algorithm 1）

```
输入：多任务演示数据集 D_demo

1. 在 D_demo 上训练价值函数 V_pre（Eq. 1）
2. 在 D_demo 上训练策略 π_pre（Eq. 3），使用 V_pre
3. 对每个目标任务 ℓ：
   a. 用演示初始化任务数据集 D_ℓ
   b. 在 D_ℓ 上训练初始价值函数和策略
   c. for k = 1 to K:
      - 用 π^{k-1}_ℓ 收集自主 rollout（可选专家干预），加入 D_ℓ
      - 在 D_ℓ 上更新价值函数 V^k_ℓ
      - 在 D_ℓ 上更新策略 π^k_ℓ
```

每次迭代中价值函数和策略都从**预训练 checkpoint** 开始微调（而非上一轮的 checkpoint），避免迭代漂移。

---

## 四、模型架构：从 π₀.₆ 到 π₀.₆*

### 4.1 基础模型 π₀.₆

π₀.₆ 基于 π₀.₅ 演进，主要改进：

- **更大骨干**：VLM 使用 SigLIP 400M + Gemma 3（4B 模型）
- **更大 Action Expert**：860M 参数的独立动作专家，用 flow matching 生成连续动作
- **更多训练数据**：增加了多平台、多机器人的异构数据
- **Knowledge Insulation（KI）训练**：端到端训练整个模型，但对 action expert 使用 stop gradient，防止 flow matching 梯度干扰 VLM 部分

模型输出包含：
- **离散化动作** $\mathbf{a}_{t:t+H}^\ell$：通过 FAST tokenizer 量化的动作 token，用于高层策略推理
- **连续动作** $\mathbf{a}_{t:t+H}$：由 action expert 生成的关节角和夹爪命令（50 Hz）
- **子任务预测** $\hat{\ell}$：下一步子任务的文本描述（如 "pick up the coffee cup"），为动作生成提供高层引导

### 4.2 从 π₀.₆ 到 π₀.₆*

π₀.₆* 在 π₀.₆ 基础上增加了**二值化优势指标 $I_t$ 的条件化输入能力**，使其适合用 RECAP 进行 RL 训练。

训练损失结合了离散动作的自回归似然和连续动作的 flow matching 目标：

$$\log \pi_\theta(\mathbf{a}_{t:t+H}, \mathbf{a}_{t:t+H}^\ell | I_t, \mathbf{o}_t, \ell, \hat{\ell}) \geq \mathbb{E}_{\eta, \omega}\left[\log p_\theta(\mathbf{a}_{t:t+H}^\ell | I_t, \mathbf{o}_t, \ell, \hat{\ell}) - \alpha_\eta \left\|\omega - \mathbf{a}_{t:t+H} - f_\theta(\mathbf{a}_{t:t+H}^{\eta,\omega}, I_t, \mathbf{o}_t, \ell, \hat{\ell})\right\|^2\right]$$

其中 $\mathbf{a}_{t:t+H}^{\eta,\omega} = \eta \mathbf{a}_{t:t+H} + (1-\eta)\omega$，$\omega \sim \mathcal{N}(0, \mathbf{I})$ 是噪声，$\eta \in [0,1]$ 是 flow matching 时间索引。

### 4.3 硬件平台

迭代改进实验使用双臂 6-DoF 机械臂系统：
- 2 个 6-DoF 手臂 + 平行夹爪，50 Hz 关节位置控制
- 3 个摄像头：底部基座摄像头 + 左右手腕摄像头
- 平台可灵活部署在桌面、咖啡机旁等场景

---

## 五、实验设置与结果

### 5.1 评估任务

三大类长时域复杂任务：

**叠衣服（T 恤和短裤）：** 从篮子中取出衣物 → 展开 → 折叠 → 摆放整齐。限时 200 秒，需在桌面右上角整齐码放。

**叠衣服（多样化物品）：** 覆盖 11 种物品（毛巾、衬衫、毛衣、牛仔裤、T恤、短裤、polo衫、裙子、长袖衬衫、袜子、内衣），限时 500 秒。评估取最难物品（纽扣衬衫）。

**制作浓缩咖啡（双杯）：** 拿手柄 → 放研磨器 → 装粉压实 → 锁手柄 → 放杯子 → 萃取 → 端咖啡。需在 200 秒内无严重失误完成。

**组装箱子：** 从扁平纸板折叠成箱子 → 贴标签 → 放入指定位置码放。限时 600 秒，在真实工厂部署。

### 5.2 基线方法

| 方法 | 描述 |
| --- | --- |
| 预训练 $\pi_{0.5}$ | 不使用 RL，直接部署预训练模型 |
| 预训练 $\pi_{0.6}$ | 不含优势指标 $I_t$，监督学习预训练 |
| RL 预训练 $\pi_{0.6}^*$ | 含 $I_t$ 的离线 RL 预训练模型 |
| $\pi_{0.6}^*$ offline RL + SFT | 在 RL 预训练基础上用演示数据 SFT |
| AWR | 优势加权回归（替代策略提取方法） |
| PPO | 基于 SPO 约束的近端策略优化变体 |

### 5.3 主要结果

#### 吞吐量（成功任务数/小时）

| 任务 | $\pi_{0.5}$ 预训练 | $\pi_{0.6}$ 预训练 | $\pi_{0.6}^*$ offline RL | $\pi_{0.6}^*$ offline RL + SFT | $\pi_{0.6}^*$ Ours |
| --- | --- | --- | --- | --- | --- |
| 叠衣服（简单） | ~15 | ~18 | ~25 | ~30 | **~55** |
| 叠衣服（多样） | ~2 | ~3 | ~4 | ~5 | **~9** |
| 制作浓缩咖啡 | ~5 | ~8 | ~12 | ~15 | **~25** |
| 组装箱子 | ~3 | ~4 | ~6 | ~8 | **~12** |

**关键发现：** 在多样化叠衣服和浓缩咖啡这两个最难的任务上，RECAP 将吞吐量提升了 **2 倍以上**。

#### 成功率

| 任务 | $\pi_{0.5}$ 预训练 | $\pi_{0.6}$ 预训练 | $\pi_{0.6}^*$ offline RL + SFT | $\pi_{0.6}^*$ Ours |
| --- | --- | --- | --- | --- |
| 叠衣服（简单） | ~60% | ~75% | ~85% | **~95%** |
| 叠衣服（多样） | ~30% | ~40% | ~60% | **~75%** |
| 制作浓缩咖啡 | ~30% | ~45% | ~65% | **~90%** |
| 组装箱子（各子任务） | 较低 | 中等 | 较高 | **最高且最一致** |

所有任务上 $\pi_{0.6}^*$ 均显著优于基线。除多样叠衣服外，最终模型成功率均在 **90%+** 范围。

### 5.4 多轮迭代改进

在叠衣服（T恤）和组装箱子上进行两轮 RECAP 迭代（$i=1, i=2$）：

**叠衣服：** 吞吐量整体提升约 50%。第一轮迭代后成功率已超 90%，第二轮主要提升速度。

**组装箱子：** 需要更多数据。第一轮吞吐量先下降后在第二轮显著提升（2× 改善），最终各子任务成功率约 90%。

### 5.5 策略提取方法对比

在叠衣服（T恤）任务上对比 RECAP 与 AWR、PPO：

| 方法 | 吞吐量 | 成功率 |
| --- | --- | --- |
| $\pi_{0.6}^*$ offline RL + SFT | 基线 | 基线 |
| AWR | 合理但较低 | 合理 |
| PPO | 不稳定 | 较差 |
| **RECAP（Ours）** | **最高** | **最高** |

AWR 和 PPO 均无法超越 offline RL + SFT 基线。PPO 因需要极小的 trust region（$\eta = 0.01$）来稳定训练，性能受限。AWR 能达到合理成功率但策略更慢，吞吐量较低。

### 5.6 失败模式消除

在严格标准的叠衣服任务上（要求衣领居中朝上），基线 offline RL + SFT 策略经常折错方向。应用两轮 RECAP（600 条纯自主轨迹，无专家干预），成功率达到 **97%** 且速度很快。

这证明 RECAP 可以仅通过 RL **有效消除特定的失败模式**，即使完全没有干预数据或额外演示。

---

## 六、局限性与未来方向

### 6.1 非完全自主

当前系统依赖人工进行奖励标注、专家干预和 episode 重置。虽然 VLA 本身可以通过高层策略推理来自动重置场景，但完全自主的数据收集循环仍是未来方向。

### 6.2 探索策略简单

当前的探索完全依赖策略本身的随机性和人工干预，是一种"贪心"策略。更复杂的探索方法（如好奇心驱动、高层规划引导探索）有望进一步提升效果。

### 6.3 离线迭代 vs 在线学习

RECAP 采用"收集一批数据 → 重新训练 → 再收集"的离线迭代模式，而非实时更新的在线 RL。扩展到全并发在线框架是有前景的未来方向。

### 6.4 价值函数的局限

使用简单的 on-policy Monte Carlo 估计器训练价值函数，虽然简单可靠，但不如 off-policy Q-function 估计器高效。未来可探索更高效的离线价值估计方法。

---

## 七、个人思考

### 7.1 与 RISE 的互补视角

RECAP 和 RISE 代表了 VLA + RL 的两条路线：

- **RECAP（本文）**：在**真实世界**收集数据（包括自主 rollout + 专家干预），用离线 RL 训练。优势条件化避免了在线策略梯度的不稳定性。
- **RISE**：训练**世界模型**在想象空间做 RL，完全不需要真实交互。组合式世界模型（动力学 + 价值）提供优势信号。

两者的共同点是都使用了**优势条件化**作为策略提取机制，但数据来源完全不同。RECAP 的数据更真实但获取成本高；RISE 的数据无限但受世界模型精度限制。

### 7.2 优势条件化 vs CFG 的洞察

论文揭示了一个优美的理论联系：优势条件化策略提取与 diffusion/flow matching 模型中的 classifier-free guidance 在数学上是等价的。条件模型学的是 $\nabla_\mathbf{a} \log \pi(\mathbf{a}|I_t, \mathbf{o}_t)$，无条件模型学的是 $\nabla_\mathbf{a} \log \pi(\mathbf{a}|\mathbf{o}_t)$，两者之差就是"改进梯度"——指向更好动作的方向。设 $\beta > 1$ 就是沿这个方向多走几步。

### 7.3 "不丢弃差数据"的价值

RECAP 的一个重要设计决策是在**所有数据**上训练策略（好的和差的都用），通过优势条件区分好坏。相比 AWR 等方法丢弃或大幅降权低质量数据，这种方式：
- 数据利用率 100%（在真实机器人场景中极其宝贵）
- 策略同时学到"该做什么"和"不该做什么"
- 避免了 importance weighting 可能导致的数值不稳定

### 7.4 实际部署的工程启示

π₀.₆* 展现了令人印象深刻的实际部署能力——连续 13 小时制作浓缩咖啡、在陌生家庭连续 2 小时叠衣服、在工厂装配真实包装箱。这些不是实验室演示，而是接近生产级别的可靠性。RECAP 方法使得**单一训练框架**就能应对从精密力控到可变形物体操作的多样化场景。

---

## 参考

- **π₀ (2024)**：Flow Matching VLA 基础模型，π₀.₆* 的前身架构
- **π₀.₅ (2025)**：异构协同训练 VLA，π₀.₆ 的直接前代
- **RISE (2026)**：组合式世界模型 + 想象空间 RL，与 RECAP 形成互补的 VLA+RL 路线
- **Diffusion Guidance (Frans et al., 2025)**：可控策略改进算子，RECAP 优势条件化方法的理论基础（CFGRL）
- **AWR (Peng et al., 2019)**：优势加权回归，本文的主要对比策略提取方法
- **VLA-RL (2025)**：在线 PPO 微调自回归 VLA，代表另一条 RL 微调路线
