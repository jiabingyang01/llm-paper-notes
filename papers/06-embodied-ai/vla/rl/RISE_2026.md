# RISE：基于组合式世界模型的机器人策略自改进框架——原理详解

> 论文：*RISE: Self-Improving Robot Policy with Compositional World Model*
> 机构：香港中文大学、Kinetix AI、香港大学、清华大学、Horizon Robotics 等
> 发布时间：2026年2月

---

## 一、论文要解决什么问题？

### 1.1 VLA 模型的核心困境

当前主流的机器人控制方案是 Vision-Language-Action（VLA）模型。它们依靠**模仿学习（Imitation Learning, IL）**训练——看专家怎么做，然后照着学。这种方式存在一个根本性的缺陷，叫做 **exposure bias（暴露偏差）**：

- 训练时，模型看到的全部是专家的正确操作
- 部署时，一旦执行出了一点小偏差，模型就进入了训练时从未见过的状态
- 模型缺乏纠错能力，偏差一步步放大，最终任务失败

这个问题在需要**精确抓取运动物体**或**双臂协调**的复杂操作任务中尤为致命。

### 1.2 强化学习的潜力与障碍

**强化学习（RL）** 可以解决上述问题，因为 RL 让机器人从自身的成功和失败中学习，天然具有纠错能力。在仿真环境（如 LIBERO）中，RL 已经很成功了，因为：

- 可以大规模并行交互
- 状态和奖励完全可控
- 环境可以自动重置

但在**真实物理世界**中，情况完全不同：

- 机器人执行是**串行**的，每次只能做一个动作
- 执行过程**缓慢**，每个 episode 需要几十秒到几分钟
- 出错后需要**人工监控和重置**，人力成本极高
- 存在**安全风险**，错误的动作可能损坏硬件或环境

这些限制使得前人的真实世界 RL 方法只能依赖离线数据（off-policy），与当前策略的分布差距很大，导致策略改进效果有限。

### 1.3 RISE 的核心思路

RISE 提出的解决方案直接而优雅：**既然真实世界做 RL 太贵，就在"想象空间"里做 RL**。

具体来说，RISE 训练一个**组合式世界模型（Compositional World Model）**，它能够：

1. **想象未来**：给定当前画面和机器人想做的动作，预测出执行这些动作后会看到什么
2. **评判好坏**：对想象出的状态打分，判断是靠近成功还是走向失败

然后在这个想象空间中进行 on-policy 的强化学习，让策略不断自我改进，完全不需要与真实物理环境交互。

---

## 二、预备知识

### 2.1 世界模型的数学形式化

RISE 的世界模型由两个独立模块组成：

**符号约定：**

- $o_t = [m_t^1, \dots, m_t^n]$：时间步 $t$ 的多视角观测（$n$ 个摄像头的画面）
- $O_t = \{o_{t-N}, \dots, o_{t-1}, o_t\}$：长度为 $N$ 的历史观测窗口
- $\ell$：语言指令，描述当前任务（如"把蓝色积木放到蓝色箱子里"）
- $\pi$：策略网络
- $H$：动作序列的长度（action chunk size）

**动力学模型 $\mathcal{D}$** 的工作：

策略根据当前观测提出一整串动作（不是单个动作，而是一个 action chunk）：

$$\mathbf{a}_t = [a_t, a_{t+1}, \dots, a_{t+H-1}] \sim \pi(\cdot | o_t, \ell)$$

动力学模型接收历史观测和这个动作序列，**预测未来 $H$ 步的视觉画面**：

$$\hat{o}_{t+1}, \dots, \hat{o}_{t+H} = \mathcal{D}(O_t, \mathbf{a}_t)$$

本质上，这就是一个**条件视频生成模型**：给定当前画面 + 动作序列 → 生成一段"未来视频"。

**价值模型 $\mathcal{V}$** 的工作：

给定任意一帧观测和任务指令，输出一个标量分数，表示这个状态距离任务完成的进度：

$$\mathcal{V}(\hat{o}_t, \ell) \rightarrow \text{标量值}$$

### 2.2 优势函数——两个模块的协作方式

这是 RISE 最关键的公式。策略提出一个动作计划后，动力学模型想象出未来 $H$ 帧画面，价值模型给每一帧打分。**优势函数（Advantage）** 衡量的是：执行这个动作计划后，状态是否比现在更好？

$$A(o_t, \mathbf{a}_t, \ell) = \left(\frac{1}{H}\sum_{k=1}^{H}\mathcal{V}(\hat{o}_{t+k}, \ell)\right) - \mathcal{V}(o_t, \ell)$$

逐项解读：

- $\frac{1}{H}\sum_{k=1}^{H}\mathcal{V}(\hat{o}_{t+k}, \ell)$：未来 $H$ 帧想象画面的**平均价值**
- $\mathcal{V}(o_t, \ell)$：当前帧的价值
- 两者之差就是优势

直觉理解：

- $A > 0$：这个动作让情况变好了（高优势）→ 策略应该多做这类动作
- $A < 0$：这个动作让情况变糟了（低优势）→ 策略应该避免这类动作
- $A \approx 0$：这个动作没什么变化

**举个例子：** 传送带上有个蓝色积木，机器人需要抓住它放到蓝色箱子里。

- 策略A提出"伸手去抓" → 动力学模型想象出"成功抓住积木"的画面 → 价值模型给高分 → 优势为正
- 策略B提出一个偏了的动作 → 想象出"手落空，积木滑走"的画面 → 价值模型给低分 → 优势为负

**为什么不用逐步奖励 $r$？**

值得注意的是，论文在 Section II-B 虽然写了标准 RL 的期望回报公式 $J(\pi) = \mathbb{E}_{\tau \sim \rho_\pi}[\sum_{t=0}^{T} r(o_t, a_t)]$，但**并没有给 $r$ 一个显式定义**。RISE 实际训练策略时，完全不用逐步奖励，而是直接用上面的优势函数 $A$ 作为学习信号。这是 RISE 与传统 RL 的一个重要区别：它**跳过了"定义逐步奖励→累加回报→最大化回报"的传统路线**，直接在 action chunk 级别计算优势。

### 2.3 强化学习框架

RISE 将问题建模为标准 MDP（马尔可夫决策过程），元组为 $(O, \ell, A, H, r)$。

为了确保策略稳定改进（不会越改越差），RISE 借鉴了 $\pi_{0.6}^*$ 的概率推断框架。核心思路是构建一个**目标分布** $\hat{\pi}$，通过"改进概率"对参考策略加权：

$$\hat{\pi}(\mathbf{a}_t | o_t, \ell) \propto \pi_{\text{ref}}(\mathbf{a}_t | o_t, \ell) \cdot p(I \mid A^{\pi_{\text{ref}}}(o_t, \mathbf{a}_t, \ell))^\beta$$

其中 $I$ 表示"改进"事件，$\beta$ 控制改进信号的强度。

应用贝叶斯公式展开改进似然：

$$p(I \mid \mathbf{a}_t, o_t, \ell) \propto \frac{\pi_{\text{ref}}(\mathbf{a}_t \mid I, o_t, \ell) \cdot p(I | o_t, \ell)}{\pi_{\text{ref}}(\mathbf{a}_t \mid o_t, \ell)}$$

将此式代入目标分布，并设 $\beta = 1$，无条件先验 $\pi_{\text{ref}}$ 被消去，得到简化目标：

$$\hat{\pi}(\mathbf{a}_t | o_t, \ell) = \pi_{\text{ref}}(\mathbf{a}_t \mid I, o_t, \ell)$$

**用大白话说**：目标策略就是"在已知会改进的条件下"的参考策略。实际实现时，通过**将优势离散化为多个 bin**，让策略以优势等级为条件来生成动作，引导策略朝高回报方向移动。

---

## 三、方法论详解

RISE 的整体框架分三个阶段：

1. **训练组合式世界模型**（Section III-A）
2. **策略预热**（Section III-B）
3. **自改进循环**（Section III-C）

### 3.1 组合式世界模型

#### 3.1.1 为什么要"组合式"？

动力学预测和价值估计是两个截然不同的问题：

- 动力学模型需要**像素级的视频生成能力**——要生成逼真的多视角未来画面
- 价值模型需要**机器人任务的语义理解能力**——要判断某个状态距离成功有多远

如果把两个目标揉进同一个模型，互相干扰，效果不好。拆开后，各自用最合适的架构和训练目标独立优化。

**一个关键特性：世界模型仅在训练阶段使用，在真实世界推理时零额外开销。**

#### 3.1.2 可控动力学模型 $\mathcal{D}$

**基础架构选择：** 基于预训练的 Genie Envisioner（GE-base），继承了 LTX-Video 的架构。

选择它的核心原因是**速度**：

- Cosmos（另一个高级世界模型）：生成 25 个多视角观测需要超过 10 分钟
- Genie Envisioner：完成同样任务只需不到 2 秒
- 速度差距约 **300 倍**

RL 训练需要海量的 rollout（想象交互），如果每次想象都要等 10 分钟，整个系统就无法运转。

**动作可控性增强：** 原始 GE-base 是用文本提示控制生成的，无法接受精细的机器人关节动作。RISE 做了以下改造：

- 在大规模动作标注数据集（Agibot World 和 Galaxea）上进一步训练
- 加入一个轻量级的**动作编码器**，让模型能理解 14 维的关节动作信号
- 对上下文帧施加更强的噪声，提高对运动模糊和视觉伪影的鲁棒性

**Task-Centric Batching 策略：** 这是一个重要的训练技巧。在异构动作数据上微调世界模型时，如果同一 batch 混入了很多不同任务和视觉场景（比如"桌面抓取"和"厨房操作"同时出现），模型会被场景差异搞混，很难学好"动作→结果"的因果关系。

解决方案是：**每个 batch 只从少数几个任务中采样，但保证同一任务下覆盖尽可能多不同动作的样本。**

直觉：优先让模型学"同一个场景下，不同动作导致不同结果"，而不是"不同场景长什么样"。这样动作可控性显著提升。

**训练配置：**

- 预训练：16 块 NVIDIA H100 GPU，全局 batch size 512，约 7 天，120k 步
- 任务微调：8 块 H100，batch size 64，约 3 天，50k 步
- 输入：4 帧历史 + 3 个视角（顶部俯视 + 左右手腕）
- 输出：预测未来 25 帧

#### 3.1.3 进度价值模型 $\mathcal{V}$

**初始化选择：** 基于预训练 VLA 策略 $\pi_{0.5}$，带来两个优势：

- $\pi_{0.5}$ 在大量机器人数据上训练过，天然具有以机器人为中心的理解能力
- 原生支持多视角输入（一般的 VLM 只处理单视角）

**训练目标——双重损失设计：**

这是一个精巧的设计，用两个互补的损失联合训练。

**损失一：进度估计 $\mathcal{L}_{\text{prog}}$**

最朴素的想法：在一个成功的 episode 中，越靠后的帧离完成越近。让价值模型预测"当前帧处于整个 episode 的哪个时间位置"：

$$\mathcal{L}_{\text{prog}} = \mathbb{E}_{(o_t, \ell) \sim \mathcal{D}_{\text{exp}}} \left[(\mathcal{V}(o_t, \ell) - t/T)^2\right]$$

其中 $t$ 是当前时间步，$T$ 是 episode 总长度。

- **优点**：每一帧都有监督信号，非常密集，训练稳定
- **缺点**：太"平滑"了，对失败不敏感。比如在第 70% 的进度时机器人突然失手掉了东西，这个损失仍然给出约 0.7 的分数，无法反映"出大问题了"

**损失二：时序差分学习 $\mathcal{L}_{\text{TD}}$**

经典的 TD learning，利用成功和失败的 rollout 数据：

$$\mathcal{L}_{\text{TD}} = \mathbb{E}_{(o_t, \ell, o_{t+1}) \sim \mathcal{D}} \left[(\mathcal{V}(o_t, \ell) - y_t)^2\right]$$

$$y_t = r_t + \gamma \mathcal{V}(o_{t+1}, \ell)$$

其中：

- $\gamma = 0.995$ 是时序折扣因子
- $r_t$ 的定义：
  - 中间步：$r_t = 0$
  - 成功 episode 最后一步：$r_t = +1$
  - 失败 episode 最后一步：$r_t = -1$

直觉：TD 学习从终端的成功/失败信号开始，通过 bootstrapping 逐步向前传播。成功 episode 中靠近结尾的帧会获得高价值，而失败 episode 中接近失败的帧价值会急剧下降。

- **优点**：对"快成功了但最后一步失手"的情况非常敏感
- **缺点**：单独使用时数值不够稳定

**联合损失：**

$$\mathcal{L}_{\mathcal{V}} = \mathcal{L}_{\text{prog}} + \mathcal{L}_{\text{TD}}$$

训练策略：先用进度估计损失单独训练 10k 步（打好单调递增的基础结构），再加入 TD 损失联合训练 40k 步（加入成败敏感性）。

**训练配置：**

- 8 块 GPU，batch size 64
- 总训练 50k 步，约 1 天完成
- 学习率 $2.5 \times 10^{-5}$

### 3.2 策略预热（Policy Warm-up）

在进入想象空间自改进之前，先用真实世界的离线数据对策略做预热。目的有两个：

1. 将策略锚定到**物理上合理的行为分布**上
2. 赋予策略**优势条件化**的能力

**数据构成（每个任务）：**

- 专家演示（人类遥操作）
- 策略 rollout（机器人自己跑的成功和失败数据）
- 人工纠正数据（DAgger 数据，人类在机器人快失败时接管）

**训练方式：**

策略不仅接收观测 $o_t$ 和任务指令 $\ell$，还接收一个**优势等级**作为条件输入。训练时：

- **专家演示和人工纠正数据**：直接赋予最高优势 $\mathbb{1}$（代表"最好的行为"）
- **策略 rollout 数据**：用学到的价值模型通过公式计算真实优势

一个重要发现：论文实验表明，**只对 rollout 数据标注优势**（而不是对所有数据都标注），效果更好。这可能是因为专家数据质量一致地高，不需要再用优势去区分。

这样预热后，策略就学会了"什么样的动作对应高优势（好的），什么样的对应低优势（差的）"，为后续从想象中的试错学习做好了准备。

### 3.3 自改进循环（Self-Improving Loop）

这是 RISE 的精髓。整个循环交替执行**Rollout 阶段**和 **Training 阶段**。

#### 3.3.1 Rollout 阶段（在想象中探索）

步骤如下：

**第一步：采样初始状态。** 从离线数据集中随机选取一个真实的初始观测 $o_t$。

**第二步：策略生成动作。** 给策略输入最高优势 $\mathbb{1}$（相当于告诉策略"请给出你认为最好的动作"）：

$$\hat{a}_t = \pi_{\text{rollout}}(\mathbb{1}, o_t, \ell)$$

**第三步：动力学模型想象未来。** 将 $o_t$ 和 $\hat{a}_t$ 输入动力学模型，生成未来 $H$ 步的想象画面。

**第四步：价值模型评估优势。** 价值模型对每一帧想象画面打分，计算真实的**评估优势**：

$$A^{\pi_{\text{rollout}}}(o_t, \hat{a}_t, \ell) = \left(\frac{1}{H}\sum_{k=1}^{H}\mathcal{V}(\hat{o}_{t+k}, \ell)\right) - \mathcal{V}(o_t, \ell)$$

注意这里有一个巧妙的"差距"：

- **提示优势** = $\mathbb{1}$（策略以为自己在给出最优动作）
- **评估优势** = $A^{\pi_{\text{rollout}}}$（世界模型客观评估这个动作的实际效果）

如果策略很强，评估优势也接近最高。如果策略其实不行，评估优势会低。**这个差距就是学习的空间。**

**第五步：优势离散化。** 将连续的优势值离散化为 $N=10$ 个均匀 bin。

**第六步：状态复用。** 想象出的未来状态可以作为下一轮 rollout 的输入（最多串联两次，因为视频生成模型会有误差累积）。

**第七步：更新 rollout 策略。** Rollout 策略的参数通过 EMA（指数移动平均）从行为策略混合更新，衰减率 0.995。

所有的 $(o_t, \hat{a}_t, A)$ 数据存入缓冲区，供 Training 阶段使用。

**与先前方法的核心区别：** RISE 不需要模拟整个 episode 到终端来获取奖励，而是在每个 action chunk 级别直接计算优势。这避开了世界模型长程预测不准确的问题。

#### 3.3.2 Training 阶段（从想象经验中学习）

用 Rollout 阶段收集的数据 $\langle o, \hat{a}, A \rangle$ 训练策略：

$$\pi(A^{\pi_{\text{rollout}}}(o, \hat{a}_t, \ell),\ o_t,\ \ell) \rightarrow \hat{a}$$

含义是：给定观测、任务指令和评估出的优势等级，策略应该输出对应质量的动作。

- 高优势数据教策略："在这种情况下这样做是对的"
- 低优势数据教策略："在这种情况下这样做会失败"

为防止**灾难性遗忘**，同时将离线标注数据混入训练 batch。

训练目标使用通用的 **flow-matching** 准则优化。

#### 3.3.3 循环往复

Training 阶段完成后，策略变强了一点 → 用更新后的策略做新的 Rollout → 产生更高质量的想象数据 → 再训练 → 如此反复。整个自改进阶段约训练 10k 步。

### 3.4 推理时的工作方式

**世界模型在推理时完全不使用。** 部署时就是一个标准的 VLA：接收多视角画面 → 输出动作。策略被条件化到最高优势 bin $\mathbb{1}$（意思是"请给出你最好的表现"）。

实际部署采用**异步控制框架**：

- VLA 策略以较低频率推断 action chunk（$H=50$ 步）
- 机器人控制器以 30 Hz 频率执行关节命令
- 使用 **Temporal Ensembling** 策略平滑过渡，避免推理延迟导致的动作冻结

---

## 四、实验设置

### 4.1 硬件平台

使用双臂 7-DoF AgileX 机器人，配备绝对关节控制。每只手臂 6 个自由度 + 1 个夹持器自由度，共 14 维动作空间。摄像头配置：

- 1 个顶部俯视摄像头（高度约 0.75m）
- 2 个手腕摄像头（左右各一个）
- 控制频率 30 Hz

### 4.2 评估任务

三个精心设计的高难度任务：

**任务一：动态砖块分拣（Dynamic Brick Sorting）**

- 从运转中的传送带上精确抓取不同颜色的砖块
- 放入对应颜色的指定箱子中
- 挑战：动态目标追踪 + 精确抓取 + 颜色识别

**任务二：背包装物（Backpack Packing）**

- 打开背包 → 放入衣物 → 提起背包 → 拉拉链
- 挑战：柔性可变形物体操作，表面顺应性导致的不确定性

**任务三：盒子封装（Box Closing）**

- 放入杯子 → 折叠侧翼 → 折叠后翼 → 插入锁扣
- 挑战：精确的双臂协调，极小的几何容差

### 4.3 评估指标

每个任务满分 10 分，分解为多个子目标（里程碑式评分）：

**动态砖块分拣：**

| 子目标 | 得分 |
| --- | --- |
| 成功抓取砖块 | 每次 1.0 分 |
| 放入正确颜色箱 | 每次 1.5 分 |
| 清空工作区 | 满分 10.0 |

**背包装物：**

| 子目标 | 得分 |
| --- | --- |
| 打开背包并放入衣物 | 2.5 分 |
| 提起以整理内容 | 5.0 分 |
| 拉链拉到一半 | 7.5 分 |
| 拉链完全拉上 | 10.0 分 |

**盒子封装：**

| 子目标 | 得分 |
| --- | --- |
| 放入杯子 | 2.5 分 |
| 折叠侧翼 | 5.0 分 |
| 折叠后翼 | 7.5 分 |
| 插入锁扣 | 10.0 分 |

每个评估结果基于 **20 次自主试验**的平均值。

### 4.4 任务数据规模

| 任务 | 专家演示 | 策略 Rollout | DAgger 数据 |
| --- | --- | --- | --- |
| 动态砖块分拣 | 3063 条 | 610 条 | — |
| 背包装物 | 2478 条 | 507 条 | — |
| 盒子封装 | 2286 条 | 524 条 | 540 条 |

---

## 五、实验结果

### 5.1 基线方法

所有方法都基于 $\pi_{0.5}$（Physical Intelligence 发布的 VLA 模型）初始化，使用相近的计算预算：

- **$\pi_{0.5}$**：仅在任务演示上做模仿学习微调
- **$\pi_{0.5}$ + DAgger**：在策略 rollout 中加入人工纠正数据
- **$\pi_{0.5}$ + PPO**：用标准 PPO 算法做在线 RL 微调
- **$\pi_{0.5}$ + DSRL**：冻结策略，只优化扩散模型的潜在噪声分布
- **RECAP**：基于优势条件化的离线 RL 方法

### 5.2 主要结果

| 方法 | 砖块分拣成功率 | 砖块分拣得分 | 背包装物成功率 | 背包装物得分 | 盒子封装成功率 | 盒子封装得分 |
| --- | --- | --- | --- | --- | --- | --- |
| $\pi_{0.5}$ | 35.00% | 8.28 | 30.00% | 4.25 | 35.00% | 7.50 |
| $\pi_{0.5}$ + DAgger | 15.00% | 6.10 | 50.00% | 7.00 | 40.00% | 7.50 |
| $\pi_{0.5}$ + PPO | 10.00% | 7.68 | 35.00% | 5.88 | 10.00% | 4.75 |
| $\pi_{0.5}$ + DSRL | 10.00% | 6.65 | 10.00% | 3.50 | 10.00% | 7.63 |
| RECAP | 50.00% | 9.00 | 40.00% | 6.13 | 60.00% | 8.13 |
| **RISE（Ours）** | **85.00%** | **9.78** | **85.00%** | **9.50** | **95.00%** | **9.88** |

**关键观察：**

1. **在线 RL 适应严重不稳定**：PPO 和 DSRL 在真实世界中表现极差，砖块分拣从 35% 降到了 10%，说明直接在物理世界做 on-policy RL 对 VLA 来说是灾难性的。

2. **DAgger 效果有限**：虽然引入了人工纠正，但在砖块分拣这种动态任务上反而更差（15%），可能因为人工纠正的频率和质量难以保证。

3. **RECAP 验证了优势条件化的价值**：50%/40%/60% 的成功率已经优于纯模仿学习和在线 RL 方法，但仍远不及 RISE。

4. **RISE 全面碾压**：85%/85%/95% 的成功率，比 RECAP 分别高出 35%/45%/35%，比纯模仿学习高出 50%/55%/60%。

### 5.3 消融实验

#### 5.3.1 离线数据混合比例

| 离线数据比例 | 抓放成功率 | 分拣准确率 | 完整成功率 | 得分 |
| --- | --- | --- | --- | --- |
| 0.1 | 15.00% | 83.33% | 5.00% | 1.35 |
| 0.3 | 78.75% | 80.95% | 25.00% | 7.03 |
| **0.6** | **90.00%** | **87.50%** | **50.00%** | **8.32** |
| 0.9 | 90.00% | 80.56% | 30.00% | 7.90 |

**结论**：离线数据比例存在一个甜蜜点（约 0.6）。

- 比例太低（0.1）：灾难性遗忘，成功率暴跌至 5%
- 比例太高（0.9）：过度正则化，策略被束缚在离线分布上，无法探索更优策略

#### 5.3.2 在线动作与在线状态的贡献

| 在线动作 | 在线状态 | 抓放成功率 | 分拣准确率 | 完整成功率 | 得分 |
| --- | --- | --- | --- | --- | --- |
| ✗ | ✗ | 80.00% | 76.56% | 35.00% | 6.98 |
| ✓ | ✗ | 96.25% | 84.42% | 40.00% | 8.73 |
| **✓** | **✓** | **98.75%** | **92.41%** | **70.00%** | **9.43** |

**结论**：

- 仅加入在线动作：成功率从 35% 提升到 40%。这是因为在线 rollout 扩展了动作探索空间，让策略能区分高优势和低优势动作。
- 同时加入在线状态：成功率进一步提升到 70%。世界模型生成的在线状态提供了**几乎无限的训练分布**，突破了固定离线数据集的限制。

#### 5.3.3 各模块的重要性

| 模块 | 变体 | 抓放成功率 | 分拣准确率 | 完整成功率 | 得分 |
| --- | --- | --- | --- | --- | --- |
| 动力学 | 无预训练 | 97.50% | 60.26% | 15.00% | 7.43 |
| 动力学 | 无 Task-Centric | 93.75% | 89.33% | 40.00% | 8.78 |
| 价值 | 无进度估计损失 | 95.00% | 86.84% | 50.00% | 8.78 |
| 价值 | 无 TD 学习损失 | 98.75% | 72.15% | 35.00% | 8.38 |
| **RISE** | **完整设计** | **98.75%** | **92.41%** | **70.00%** | **9.43** |

**关键发现**：

- **去掉视觉预训练**：分拣准确率暴跌 32%，完整成功率降至 15%。说明预训练提供的视觉先验至关重要。
- **去掉 Task-Centric Batching**：完整成功率降 30%，验证了这种批处理策略对动作可控性的重要性。
- **去掉进度估计损失**：完整成功率降 20%，密集的进度信号对价值模型的基础结构很重要。
- **去掉 TD 学习损失**：完整成功率降 35%，分拣准确率降 20%。TD 学习提供的成败敏感性不可或缺。

#### 5.3.4 动力学模型质量对比

| 方法 | PSNR ↑ | LPIPS ↓ | SSIM ↑ | FVD ↓ | EPE ↓ |
| --- | --- | --- | --- | --- | --- |
| Cosmos | 21.17 | 0.14 | 0.79 | 97.90 | 1.21 |
| GE | 21.16 | 0.11 | 0.79 | 85.72 | 1.05 |
| RISE（无 Task-Centric） | 22.67 | 0.08 | 0.80 | 61.22 | 0.68 |
| **RISE（完整）** | **23.90** | **0.07** | **0.82** | **66.84** | **0.54** |

其中 EPE（End-Point Error，光流端点误差）是衡量**动作可控性**的关键指标——越低说明生成的运动越符合输入动作。RISE 的 EPE 从基线的 1.21/1.05 降到了 0.54，证明 Task-Centric 预训练显著增强了运动感知能力。

#### 5.3.5 扩展训练能否弥补差距？

为了排除"RISE 只是训练更久"的可能，论文给 RECAP 和 DSRL 额外增加了 50k 步训练：

- RECAP 饱和在 30%-50% 的成功率
- DSRL 饱和在 5%-10%
- RISE 仅用额外 9k 步就从 50% 提升到 85%

结论：差距来自方法本质的不同，而非训练时长。

#### 5.3.6 优势 bin 的验证

用不同优势等级条件化策略执行任务：

| 优势等级 | 抓放成功率 | 分拣准确率 | 完整成功率 |
| --- | --- | --- | --- |
| Bin 10（最高） | 100.00% | 95.25% | 85.00% |
| Bin 5（中等） | 93.75% | 90.79% | 60.00% |
| Bin 1（最低） | 95.00% | 84.00% | 40.00% |

从高 bin 到低 bin，性能明显下降，特别是分拣和完整成功率。这验证了：

1. 学到的优势信号是有意义的
2. 策略确实通过优势条件化捕获了不同质量的行为模式

---

## 六、用类比总结 RISE 的核心原理

想象你在学打网球：

**纯模仿学习（传统 VLA）：** 你只看教练的录像，然后上场模仿。录像里只有正确的击球，从没有"失误后怎么补救"的画面。一旦你的动作偏了，就完全不知道该怎么办。

**真实世界 RL（PPO 等）：** 你在真实球场上不停地打，从每次击球的成功和失败中学习。但每次打坏一个球都要有人帮你捡，场地费很贵，你一天只能练几十个球。

**RISE 的做法：** 你在脑子里（世界模型）构建了一个"虚拟球场"。你可以在脑海中尝试各种挥拍方式（动力学模型想象结果），你的"内心教练"（价值模型）会告诉你"这样挥效果好，优势为正""那样挥会打飞，优势为负"。你在脑子里练了成千上万次（自改进循环）之后再上真实球场，表现就好多了。而且上场时你的脑子不需要再做额外计算——之前的想象训练已经内化到你的肌肉记忆（策略网络权重）里了。

---

## 七、局限性与未来方向

### 7.1 想象与现实的差距

世界模型在罕见或训练数据中欠代表的场景中，仍可能产生物理上不合理的预测。未来需要不确定性感知的想象机制，以及显式编码几何约束的方法。

### 7.2 模拟-真实数据的最优比例

实验表明真实数据仍不可或缺（最优离线比例约 0.6），但如何确定这个比例仍是一个开放问题。

### 7.3 从物理成本到计算成本

RISE 将瓶颈从物理交互转移到了计算。预训练动力学模型需要 16 块 H100 跑 7 天，这对资源有限的团队来说仍然是很高的成本。提升世界模型的训练和推理效率是重要的未来方向。
