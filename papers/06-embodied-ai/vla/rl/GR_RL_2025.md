# GR-RL：从通才到专家——用强化学习让 VLA 掌握长时域灵巧精密操作

> 论文：*GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation*
> 机构：ByteDance Seed
> 发布时间：2025年12月
> 🔗 [arXiv](https://arxiv.org/abs/2512.01801) | [PDF](https://arxiv.org/pdf/2512.01801) | [项目主页](https://seed.bytedance.com/gr_rl)

---

## 一句话总结

GR-RL 提出一个多阶段训练流水线（离线数据过滤 → 形态对称增强 → 在线隐空间 RL），将通才 VLA（GR-3）特化为能自主穿鞋带的专家策略，在需要毫米级精度、长时域推理和柔性物体交互的穿鞋带任务上达到 83.3% 成功率，是已知首个能完成多孔穿鞋带的学习型策略。

---

## 一、问题与动机

### 1.1 VLA 的"通才困境"

大规模 VLA（Vision-Language-Action）模型（如 π₀、GR-3、RT-2）在泛化能力上取得了巨大进展——给一个语言指令和视觉观测，就能执行多种操作任务。但**通用不等于可靠**，当前 VLA 在两个关键维度上仍然力不从心：

1. **灵巧精密操控**：毫米级的精度控制，尤其是对柔性物体（如鞋带、布料）
2. **长时域鲁棒性**：任务步数越长，误差越容易累积，与精密操控耦合后问题更加严重

### 1.2 穿鞋带：终极测试场

穿鞋带任务几乎同时包含了灵巧操作的所有难点：

| 挑战 | 穿鞋带中的体现 |
| --- | --- |
| **柔性物体操控** | 鞋带和鞋本身都是可变形的 |
| **毫米级精度** | 鞋带必须精确穿过鞋孔 |
| **长时域推理** | 抓取→穿孔→换手→拉紧，多个阶段串联 |
| **异常恢复** | 鞋带滑落、穿偏后需要自主重试 |

经典方法靠运动规划+预定义动作原语来解决穿鞋带问题，但泛化能力受限，无法应对新配置和失败恢复。单纯的行为克隆也会因为示教数据的噪声而学到次优策略。

### 1.3 人类示教数据的两大瓶颈

GR-RL 识别出当前 VLA 训练流程的两个核心瓶颈：

**瓶颈一：人类示教是次优的**。在极端精密和灵巧的操作场景下，人类遥操作员也会犹豫、犯错、引入噪声。直接模仿所有数据，等于把"犹豫"和"失误"也学进策略里。

**瓶颈二：训练-推理不匹配**。离线训练时，VLA 学的是从滑动窗口中提取的固定长度动作块（action chunk）；但部署时，为了平滑执行，通常会用时间集成（temporal ensembling）、后退控制（receding horizon control）等后处理。训练看到的是"原始动作"，执行的却是"优化后的动作"——两者之间存在系统性偏差。

### 1.4 GR-RL 的核心思路

GR-RL 不追求从头训练一个新模型，而是从 GR-3（一个 5B 参数的通才 VLA）出发，通过**三阶段流水线**把它"特化"为精密操作专家：

1. **离线过滤**：学一个任务进度评估器，自动识别并剔除次优数据
2. **对称增强**：利用双臂操作的形态对称性做数据扩增
3. **在线 RL**：在隐空间中做结构化探索，修复训练-推理不匹配

---

## 二、预备知识

### 2.1 GR-3 与 Mixture-of-Transformer 架构

GR-RL 的基座模型 GR-3 采用 **Mixture-of-Transformer（MoT）** 架构，由两个核心组件组成：

- **视觉-语言模型（VLM）**：Qwen2.5-VL-3B-Instruct 作为骨干网络，接收多视角 RGB 图像、语言指令和本体感受状态
- **动作扩散 Transformer（Action DiT）**：通过 Flow Matching 目标训练的扩散 Transformer，从 VLM 后半层的 KV 缓存中获取视觉-语言条件，生成动作块

策略 $\pi_\theta$ 控制一个具有移动底盘的双臂机器人，生成 $k$ 步动作块：

$$\mathbf{a}_t = \pi_\theta(l, \mathbf{o}_t, \mathbf{s}_t) = a_{t:t+k}$$

其中 $l$ 是语言指令，$\mathbf{o}_t$ 是视觉观测，$\mathbf{s}_t$ 是本体感受状态。

### 2.2 Flow Matching 回顾

Flow Matching 是一种通过训练连续归一化流来学习生成模型的方法。给定数据分布中的样本 $x_1$，从噪声 $x_0 \sim \mathcal{N}(0, I)$ 沿直线路径插值：

$$x_t = (1 - t)x_0 + tx_1$$

训练目标是学一个速度场 $v_\theta(t, x_t)$ 来匹配这条直线路径：

$$\mathcal{L}_{\text{FM}} = \mathbb{E}_{x_0, x_1, t}\left[\|v_\theta(t, x_t) - (x_1 - x_0)\|^2\right]$$

推理时通过多步 Euler 积分从噪声 $\epsilon \sim \mathcal{N}(0, I)$ 生成动作。

### 2.3 分布式强化学习

传统 RL 中的 Critic 直接回归一个标量 Q 值。**分布式 RL**（Distributional RL）则将 Q 值建模为一个**离散分布**，用一组固定支撑点上的概率来表示值函数：

$$Q(s, a) \approx \sum_{i=1}^{N} p_i \cdot z_i$$

其中 $\{z_i\}_{i=1}^N$ 是等间距的支撑点（如 [0, 1] 区间上的 N 个点），$\{p_i\}$ 是对应的概率。训练目标是最小化目标分布与预测分布之间的交叉熵。

**为什么分布式比回归式更好？**

- **天然有界**：设上界为 1、下界为 0，预测值自动约束在 [0, 1] 区间，避免过估计
- **捕捉不确定性**：用分布而非点估计来表示价值，更好地反映真实世界轨迹中的随机性
- **稀疏奖励友好**：在长时域 + 二值奖励的设定下，回归式 Critic 严重过估计，而分布式 Critic 收敛到合理尺度

### 2.4 TD3+BC

TD3+BC 是一种极简的离线 RL 算法，在标准 TD3 的基础上加入行为克隆正则化：

$$\mathcal{L}(\theta) = -\lambda Q(s, \pi_\theta(s)) + \|\pi_\theta(s) - a\|^2$$

其中 $\lambda$ 控制 Q 最大化与模仿示教数据之间的平衡。GR-RL 用 TD3+BC 来训练离线 Critic。

---

## 三、核心方法

### 3.1 阶段一：基于学习任务进度的数据过滤

#### 3.1.1 动机：不是所有示教数据都值得学

对于穿鞋带这样的高精度操作，即使是经验丰富的遥操作员收集的轨迹也包含大量次优片段：错误尝试、犹豫、无效动作。直接模仿所有数据，等于让策略学到了多模态的噪声动作分布。但人工标注哪些片段是次优的，又太主观且不可扩展。

#### 3.1.2 核心想法：用 Q 值作为任务进度

GR-RL 提出用**离线 RL 训练的 Critic 值**作为自动化的任务进度评估器。直觉很简单：一个好的值函数应该能反映"当前状态离成功还有多远"——值高说明接近成功，值低说明离成功还很远或者刚犯了错。

**奖励设计**：采用稀疏奖励，仅在轨迹末尾给出成功/失败信号：

$$r(\mathbf{o}_t, l, \mathbf{s}_t, \mathbf{a}_t) = \begin{cases} \gamma^{T-t}\mathbb{I}(\tau), & t > T - k \\ 0, & t \leq T - k \end{cases}$$

其中 $\mathbb{I}(\tau)$ 是指示函数（成功轨迹为 1，失败为 0），$T$ 是轨迹长度，$\gamma$ 是折扣因子。

**失败数据增强**：由于大多数收集的轨迹是成功的，GR-RL 标注每条轨迹中的**重试关键帧** $m_i$（遥操作员明显犯错的位置），将一条成功轨迹 $\tau_{0:T}$ 扩增为 $M$ 条失败轨迹 $\tau_{0:m_i}$（借鉴 Hindsight Experience Replay 的思想）。这为 Critic 提供了充分的正/负样本对比。

#### 3.1.3 任务进度计算

训练好 Critic $Q_\phi$ 后，对数据集中每个转换计算进度值：

$$\rho_t := \text{mean}(Q_\phi(\mathbf{o}_t, l, \mathbf{s}_t, \mathbf{a}_t))$$

其中 $\text{mean}$ 取分布式 Critic 的类别分布均值（回忆：分布式 Critic 输出的是一组概率，均值就是期望 Q 值）。

#### 3.1.4 过滤规则

定义一个转换在时刻 $t$ 为**次优的**，如果在其后续 $k$ 步窗口 $\rho_{t:t+k}$ 内存在超过阈值 $\delta$ 的值下降：

$$\text{suboptimal}(t) \iff \exists\, j \in [t, t+k],\; \rho_t - \rho_j > \delta$$

直觉：如果 Critic 预测的进度突然下跌，说明这个时刻做了一个"倒退"的动作——应该被过滤掉。过滤后的数据集用于标准行为克隆训练 $\pi_\theta$。

#### 3.1.5 分布式 vs 回归式 Critic 的对比

论文做了关键的消融对比：

| Critic 类型 | 对微妙失败的敏感度 | 长期效应捕捉 | 值范围 |
| --- | --- | --- | --- |
| 回归式（直接回归 $t/T$） | 低：过度平滑，对毫米级失败不敏感 | 差：无法识别"放下鞋带调整抓取"的长期收益 | 无界 |
| 非分布式 TD Critic | 中等 | 中等：严重过估计 | 无界，可能为负 |
| **分布式 Critic（GR-RL）** | **高：能检测鞋带滑出鞋孔的瞬间** | **好：能捕捉主动调整抓取位姿的长期价值** | **[0, 1]，天然有界** |

### 3.2 阶段二：形态对称增强

这是一个简单但极其有效的数据增强方法，利用双臂操作的**形态学对称性**。

**增强流程**：

1. **图像翻转**：将所有图像水平翻转
2. **相机交换**：左手腕相机图像与右手腕相机图像互换
3. **动作和状态镜像**：本体感受状态 $\mathbf{s}_t$ 和动作 $\mathbf{a}_t$ 在世界坐标系下做镜像变换，再转换回各自的腕部局部坐标系
4. **语言指令翻转**：将空间描述中的"左"和"右"互换，例如"左边的鞋孔"→"右边的鞋孔"

**为什么有效？** 双臂机器人的左右手在物理上是对称的，"左手做的事情"和"右手做的事情"在镜像下是等价的。这相当于用零成本把训练数据量翻倍，同时提升了策略对左/右操作的均衡能力。

### 3.3 阶段三：在线隐空间 RL

#### 3.3.1 动机：弥合训练-推理鸿沟

离线训练好的策略在部署时需要经过后处理（时间集成、后退控制等）才能平滑执行，这导致训练时看到的动作和实际执行的动作之间存在系统性偏差。唯一能真正解决这个不匹配的方法是：**让模型在部署条件下闭环交互，从实际执行效果中学习**——即在线 RL。

#### 3.3.2 核心挑战：精密任务的探索困难

在需要毫米级精度的任务中，传统的探索方式（给动作加高斯噪声）几乎不可能成功——噪声稍大就穿不进鞋孔。GR-RL 的解法是**在隐空间中做结构化探索**，具体方法借鉴了 Latent Space RL 的思路。

#### 3.3.3 噪声预测器

GR-RL 在共享的 VLM 骨干之后添加一个**噪声预测器** $\pi_{\theta'}$（51.5M 参数），用来预测 Action DiT 的初始噪声 $\epsilon_t$：

$$\epsilon_t \sim \pi_{\theta'}(\mathbf{o}_t, l, \mathbf{s}_t)$$

这个预测噪声替代了标准的 $\mathcal{N}(0, I)$ 采样，送入 Flow Matching 的去噪过程生成最终动作。直觉是：**与其在动作空间中盲目探索，不如在去噪的起点上做有方向的偏移**——从而引导 Flow 模型生成更高回报的动作。

#### 3.3.4 噪声预测器的训练目标

$$\mathcal{L}(\pi_{\theta'}) = \mathbb{E}_{(\mathbf{o}_t, l, \mathbf{s}_t) \sim \mathcal{D}}\left[-Q_{\phi'}(\mathbf{o}_t, l, \mathbf{s}_t, \epsilon_t) + c \cdot \max\left(\frac{1}{2}\|\epsilon_t\|^2 - \beta, 0\right)\right], \quad \epsilon_t \sim \pi_{\theta'}(\mathbf{o}_t, l, \mathbf{s}_t)$$

两项的含义：

- **第一项**：最大化噪声空间 Q 值——生成能导致高回报的噪声
- **第二项**：正则化约束——当噪声偏离标准高斯分布太远（$\|\epsilon_t\|^2 > 2\beta$）时给惩罚，防止生成离线训练分布之外的离谱动作

#### 3.3.5 噪声空间 Q 函数的蒸馏

为了避免在策略优化时反向传播穿过整个 Flow 模型（计算代价太高），GR-RL 蒸馏一个独立的噪声空间 Q 函数 $Q_{\phi'}$：

$$\mathcal{L}(Q_{\phi'}) = \text{cross\_entropy}\left(Q_{\phi'}(\mathbf{o}_t, l, \mathbf{s}_t, \epsilon_t),\; Q_\phi(\mathbf{o}_t, l, \mathbf{s}_t, \pi_\theta(\mathbf{o}_t, l, \mathbf{s}_t | \epsilon_t))\right)$$

$$\epsilon_t \sim \begin{cases} \mathcal{N}(0, I) & \text{w.p. } 0.5 \\ \pi_{\theta'}(\mathbf{o}_t, l, \mathbf{s}_t) & \text{otherwise} \end{cases}$$

关键设计：训练 $Q_{\phi'}$ 时，噪声以 50% 概率从标准高斯采样，50% 从噪声预测器采样。这是 GR-RL 相对于原始方法的改进——**确保 Q 函数在整个噪声空间上都有良好覆盖**，而不仅仅是噪声预测器探索过的区域。

#### 3.3.6 缓冲区管理

为了样本高效地完成离线到在线的适配：

- **Off-policy 缓冲区**：预热阶段用离线策略的 rollout 填充（不使用遥操作数据，避免动力学不匹配）
- **On-policy 缓冲区**：仅保留最近两个 checkpoint 生成的轨迹，过期数据推入 off-policy 缓冲区
- **采样**：从两个缓冲区均匀采样

训练节奏：每收集 12 个新 episode，做 50 步优化更新（仅更新噪声预测器 $\pi_{\theta'}$ 和两个 Critic 头 $Q_\phi, Q_{\phi'}$，VLM 骨干冻结）。

---

## 四、机器人平台

GR-RL 在 **ByteMini-v2** 上验证，这是一款轮式移动双臂操作机器人：

| 组件 | 规格 |
| --- | --- |
| 双臂 | 7 自由度，球形腕关节设计 |
| 夹爪 | 1 自由度 |
| 移动底盘 | 3 自由度全向移动平台 |
| 头部 | 2 自由度 |
| 升降 | 1 自由度 |
| 传感器 | 头部 RGB 相机 + RGB-D 相机、手部 RGB-D 相机、3D 激光雷达 |
| 计算 | Dell NUC T3280 + A2000 GPU |

相比前代 ByteMini-v1，v2 的关键升级包括：肘部关节扭矩从 17Nm 提升到 35Nm（负载能力从 1.4kg 增至 3.15kg），底盘面积缩小（500×720mm → 450×650mm），以及更好的外观封装和可用性。

---

## 五、实验结果

### 5.1 实验设置

- **任务**：穿鞋带——将鞋带穿过正确的鞋孔并拉紧放在桌上
- **观测**：三视角 RGB 图像 + 本体感受状态 + 语言指令
- **推理优化**：对预测的动作块施加 jerk 和时间连续性约束
- **RL 奖励**：二值稀疏奖励（鞋带穿过正确鞋孔且完全放在桌上才得 1 分）

### 5.2 多阶段训练的逐步提升

| 方法 | 成功率 | 相比上一阶段提升 |
| --- | --- | --- |
| GR-3（全量数据行为克隆） | 45.7% | — |
| Filtered BC（数据过滤后行为克隆） | 61.6% | +15.9% |
| Filtered BC + Aug（加对称增强） | 72.7% | +11.1% |
| **GR-RL（加在线 RL）** | **83.3%** | **+10.6%** |

**每个阶段都带来显著提升**，从 45.7% 提升到 83.3%，总计提升 37.6 个百分点。

### 5.3 各阶段详细成功率分析

论文将穿鞋带任务分解为四个关键阶段，分析不同模型在各阶段的成功率：

| 阶段 | GR-3 | Filtered BC | Filtered BC+Aug | GR-RL |
| --- | --- | --- | --- | --- |
| 抓起鞋带 | 90.6% | 92.8% | 96.2% | 97.9% |
| 穿入鞋孔 | 58.7% | 71.0% | 76.5% | 89.6% |
| 换手成功 | 46.4% | 63.8% | 74.2% | 83.3% |
| 拉出鞋带 | 45.7% | 61.6% | 72.7% | 83.3% |

**关键发现**：

- **数据过滤和在线 RL 主要减少"穿入鞋孔"阶段的失败**——这是精度要求最高的阶段
- **数据增强在所有阶段均有提升**，虽然幅度较小但全面
- **GR-RL 在穿孔阶段的成功率从 58.7% 飙升到 89.6%**，验证了在线 RL 对精密操控的价值

### 5.4 在线训练动态

在线 RL 的训练曲线显示了典型的**离线到在线适配模式**：

1. **初始下降**：前几轮迭代成功率下降，因为离线到在线的分布偏移
2. **快速恢复**：成功率迅速恢复并超越离线性能
3. **持续攀升**：成功率持续增长，超过 90%
4. **最终选择**：取 500 步在线训练时的 checkpoint 做最终评估（83.3%）

### 5.5 GR-RL 展现的鲁棒行为

论文展示了 GR-RL 学到的多种鲁棒行为，这些行为**不是人工编程的**，而是通过 RL 探索自主发现的：

- **跨颜色泛化**：能处理不同颜色的鞋子
- **掉落重抓**：鞋带意外掉落后自动重新抓取
- **穿偏重试**：鞋带没有精确穿过鞋孔时自动重新尝试
- **主动调整抓取位姿**：抓取点离鞋带尖端太远时，主动放下鞋带、在鞋面上滑动后重新抓取
- **调整鞋子朝向**：鞋子歪了先把它摆正再穿
- **调整场景布局**：鞋子太远时先拉近，再调整鞋带位置
- **处理交叉鞋带**：两根鞋带交叉时能识别正确的一根并把它拽出来

---

## 六、局限性与未来方向

### 6.1 行为漂移问题

在稀疏且有噪声的奖励下，在线 RL 阶段的策略行为可能不稳定。这可能源于轻量噪声预测器的容量不足，或大隐空间中的信用分配困难。

### 6.2 专家知识未回馈通才

当前流水线是**单向的**（通才 → 专家），没有将专家学到的改进蒸馏回基座 VLA。如何让特化知识反向增强通才能力是一个有价值的方向。

### 6.3 奖励依赖人工标注

虽然进度评估器是自动学习的，但失败轨迹的生成依赖于人工标注"重试关键帧"。完全自动化的奖励发现仍是开放问题。

---

## 七、个人思考

### 7.1 "过滤 > 增加"的数据哲学

GR-RL 最令人印象深刻的结果之一是：**仅靠过滤掉次优数据就将成功率从 45.7% 提升到 61.6%**——这比收集更多数据可能更高效。在精密操作领域，数据质量的重要性远超数据数量。这与 LLM 领域"高质量数据 > 大量数据"的趋势一致。

### 7.2 分布式 Critic 作为通用进度评估器

将分布式 Critic 的 Q 值重新解读为"任务进度"是一个非常优雅的洞察。由于分布式 Critic 天然有界（[0, 1]），其输出直接对应"离成功还有多远"的语义。这个想法不仅可以用于数据过滤，还可以用于：任务分解、课程学习、异常检测等。有趣的是，π*₀.₆ 也独立地采用了类似的分布式 Critic 来学习任务进度。

### 7.3 隐空间 RL vs 动作空间 RL

GR-RL 选择在噪声空间而非动作空间做 RL，这是一个务实的工程选择——避免了反向传播穿过整个 Flow 模型的计算代价。但 51.5M 的噪声预测器容量有限，论文也承认这可能是行为漂移的原因之一。SAC Flow 提出的速度重参数化方法（GRU/Transformer 风格）提供了另一种思路：直接在动作空间做 off-policy RL，通过序列模型的梯度稳定性设计来解决反向传播问题。两种路线各有取舍：

| 维度 | GR-RL（隐空间 RL） | SAC Flow（动作空间 RL） |
| --- | --- | --- |
| 计算代价 | 低（不穿过 Flow 模型） | 高（穿过 K 步 Euler 积分） |
| 表达能力 | 受限于噪声预测器容量 | 直接优化最终动作 |
| 探索方式 | 在噪声空间偏移 | 在动作空间注入高斯噪声 |
| 适用场景 | 真实世界（样本效率优先） | 仿真（可大量交互） |

### 7.4 形态对称增强的普适性

左右手镜像增强看似简单，却带来了 11.1% 的成功率提升。这提示我们：在任何具有物理对称性的系统中（双臂、双足、对称工件），都应该首先考虑对称增强。它是"免费午餐"——零数据收集成本，零模型修改，纯粹的数据层面操作。

### 7.5 与 π*₀.₆ 的比较

GR-RL 和 Physical Intelligence 的 π*₀.₆ 几乎同时期发布，都在解决"如何用 RL 改进 VLA"的问题。两者有很多共同点（分布式 Critic、真实世界 off-policy RL），但策略改进的方式不同：

- **π*₀.₆**：用优势条件化去噪（advantage-conditioned denoising），在去噪过程中把优势信息注入
- **GR-RL**：用过滤行为克隆 + 噪声预测器引导

GR-RL 的数据过滤阶段给出了更强的离线基座策略，从而缩小了在线探索的搜索空间。这种"先做减法（过滤）、再做加法（RL）"的策略可能比直接做 RL 更稳健。

---

## 参考

- [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)：GR-RL 的基座 VLA 模型
- [π₀: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/abs/2410.24164)：Flow Matching VLA 的代表性工作
- [π*₀.₆: A VLA that Learns from Experience](https://arxiv.org/abs/2511.14759)：同期的真实世界 RL 改进 VLA 工作
- [TD3+BC: A Minimalist Approach to Offline RL](https://arxiv.org/abs/2106.06860)：GR-RL 用于训练离线 Critic 的算法
- [Steering Your Diffusion Policy with Latent Space RL](https://arxiv.org/abs/2506.15799)：GR-RL 在线 RL 阶段的方法基础
- [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495)：失败轨迹增强的灵感来源
- [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://arxiv.org/abs/2303.04137)：扩散策略的代表性工作
- [SERL: Sample-Efficient Robotic RL](https://arxiv.org/abs/2401.16013)：真实世界 off-policy RL 的工程实践
