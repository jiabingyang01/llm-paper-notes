# WoVR：用世界模型作为可靠模拟器进行 VLA 策略的 RL 后训练——原理详解

> 论文：*WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL*
> 机构：清华大学、中科院自动化所、中国科学院大学、中关村科学院、Infinigence AI
> 发布时间：2026年2月
> 🔗 [arXiv](https://arxiv.org/abs/2602.13977) | [PDF](https://arxiv.org/pdf/2602.13977) | [HuggingFace](https://huggingface.co/Collections/RLinf/wovr) | [代码](https://github.com/RLinf/RLinf)

---

## 一句话总结

WoVR 提出了一个**幻觉感知**的世界模型强化学习框架：通过稳定化的动作条件视频世界模型 + 关键帧初始化 Rollout（KIR）+ 策略-世界模型协同进化（PACE），在不依赖真实环境交互的情况下，将 LIBERO 平均成功率从 39.95% 提升到 69.2%（+29.3%），真实机器人成功率从 61.7% 提升到 91.7%（+30.0%）。

---

## 一、问题与动机

### 1.1 VLA 的 RL 后训练需求

VLA（Vision-Language-Action）模型通过模仿学习取得了显著进展，但其性能天花板受限于演示数据的质量和覆盖范围。RL 后训练已被证明可以显著提升 VLA 性能（参见 [VLA-RL](./VLA_RL_2025)、[RISE](./RISE_2026)），但面临一个根本性矛盾：

- **On-policy RL**（如 PPO、GRPO）需要大规模环境并行交互
- **真实机器人**交互昂贵、缓慢、需要人工监督
- **传统仿真器**难以精确还原真实世界的接触丰富动力学

### 1.2 用学习的世界模型替代真实环境

一个自然的想法是：训练一个**世界模型**来替代真实环境，在"想象空间"中做 RL。这与 [RISE](./RISE_2026) 的思路一致。但 WoVR 指出了一个被先前工作（如 World-Env、WMPO）忽视的关键问题：

> **世界模型不是忠实的模拟器——它会产生幻觉。**

### 1.3 幻觉问题：不仅仅是视觉质量问题

WoVR 对"幻觉"给出了精确定义：**在闭环交互中，想象结果与真实结果之间的系统性不匹配**。世界模型可能生成视觉上逼真但物理上错误的状态转移，甚至产生虚假的成功信号。

幻觉产生的两大机制：

| 机制 | 说明 |
| --- | --- |
| **自回归反馈放大** | 模型以自己生成的帧为条件，小误差逐步放大 |
| **分布偏移** | 策略进化后，动作分布偏离世界模型的训练分布，导致 OOD 预测失败 |

**关键洞察**：如果用含幻觉的轨迹做策略优化，RL 会被激励去**利用模型的系统性错误**而非真实任务进展。策略学会了"在想象中成功"而非"在现实中成功"。

### 1.4 WoVR 的核心立场

WoVR 认为，世界模型用于 RL **不是一个建模问题，而是一个可靠性问题**。它不试图构建完美的世界模型（这几乎不可能），而是在三个层面显式控制幻觉：

1. **模拟器层**：构建动作可控、rollout 稳定的视频世界模型
2. **交互层**：通过关键帧初始化 rollout 减少有效误差深度
3. **对齐层**：通过策略-模型协同进化缓解分布偏移

---

## 二、预备知识

### 2.1 VLA 的 RL 微调公式化

策略优化被建模为 MDP $\mathcal{M} = (\mathcal{O}, \mathcal{A}, P, R, \gamma)$。在每个时间步 $t$，VLA 策略根据视觉观测 $o_t$ 和语言指令 $l_t$ 输出动作块：

$$a_t \sim \pi_\theta(\cdot | o_t, l_t) \in \mathcal{A}$$

RL 目标是最大化期望折扣回报：

$$J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=1}^{T} \gamma^{t-1} r_t\right]$$

标准策略梯度：

$$\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t | o_t, l_t) A(o_t, a_t)\right]$$

其中 $A(o_t, a_t) = Q(o_t, a_t) - V(o_t)$ 是优势函数。

### 2.2 世界模型 MDP（WM-MDP）

WoVR 将原始 MDP 重构为 WM-MDP：

$$\mathcal{M}_{WM} = (\mathcal{O}, \mathcal{A}, \hat{P}_\phi, \hat{R}_\psi, \gamma)$$

其中：

- $\hat{P}_\phi(o_{t+1} | o_t, a_t)$：学习的世界模型（参数 $\phi$）近似转移动力学
- $\hat{R}_\psi(\tilde{o}_{t+1})$：学习的奖励函数（参数 $\psi$）

在 WM-MDP 中，轨迹通过与世界模型交互生成：

$$\tilde{o}_{t+1} \sim \hat{P}_\phi(\cdot | o_t, a_t), \quad \tilde{r}_t = \hat{R}_\psi(\tilde{o}_{t+1})$$

策略优化目标形式不变，但所有交互发生在想象中：

$$J_{WM}(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta, \hat{P}_\phi}\left[\sum_{t=1}^{T} \gamma^{t-1} \hat{r}_t\right]$$

### 2.3 与 RISE 的公式对比

RISE 和 WoVR 都在想象空间中做 RL，但公式化方式有显著区别：

| 维度 | RISE | WoVR |
| --- | --- | --- |
| **优势计算** | 直接用价值模型差值（chunk 级别） | 通过 GRPO 的组相对优势（轨迹级别） |
| **回报定义** | $A = \frac{1}{H}\sum_{k=1}^H \mathcal{V}(\hat{o}_{t+k}) - \mathcal{V}(o_t)$ | $R(\tau) = \sum_t \gamma^{t-1} \hat{r}_t$ |
| **RL 算法** | 优势条件化 + 概率推断 | GRPO（组相对策略优化） |
| **奖励信号** | 进度价值函数（密集） | 二值成功分类器（稀疏） |
| **Rollout 长度** | 单个 action chunk（~25 帧） | 完整 episode（~512 帧） |

---

## 三、方法论详解

WoVR 在三个层面构建可靠性保障。

### 3.1 稳定化的动作条件世界模型

#### 3.1.1 骨架与动作条件化

世界模型基于 **Wan2.2-TI2V-5B** 视频扩散模型（~5B 参数）。与传统的图像到视频生成不同，机器人仿真需要显式的动作条件化。WoVR 设计了**双通道动作注入**机制：

**通道一：AdaLN-Zero 调制。** 在每个 DiT block 中，动作嵌入与扩散时间步嵌入融合，通过 AdaLN-Zero 风格的调制直接调控去噪过程的特征：

$$\text{Scale, Shift} \leftarrow \text{MLP}(\text{TimestepEmb} + \text{ActionEmb})$$

**通道二：交叉注意力。** 保留原始 DiT 的交叉注意力算子，但将文本嵌入替换为动作嵌入，让动作在全局层面条件化网络。

两个通道提供了**局部调制**和**全局上下文**的互补控制，确保生成的状态转移因果地响应策略的动作。

#### 3.1.2 首帧锚定推理上下文

即使有强动作条件化，逐块（chunk-by-chunk）自回归生成仍然会累积误差，导致空间漂移和背景坍塌。WoVR 采用**首帧锚定**策略：

在每个自回归步，模型条件在：

$$\text{Context} = [o_0, \; o_{t'-c':t'}]$$

即**始终保留 episode 的第一帧**（$o_0$）与最近的记忆帧拼接。这利用了一个经验发现（论文 Fig. 4 验证）：在去噪过程中，大量自注意力头自然地关注序列的第一帧。首帧锚定将这种天然倾向转化为结构性约束。

**与 RISE 动力学模型的对比**：

| 设计选择 | RISE | WoVR |
| --- | --- | --- |
| **骨架** | Genie Envisioner (~1B) | Wan2.2-TI2V-5B (~5B) |
| **推理速度** | ~25 帧 / <2s | 23 FPS |
| **动作条件化** | 轻量动作编码器 | 双通道（AdaLN-Zero + 交叉注意力） |
| **长程稳定性** | 最多串联 2 个 chunk | 首帧锚定支持 512 帧 rollout |
| **训练数据** | 大规模动作数据集预训练 | 任务内 3000 条轨迹 |

#### 3.1.3 训练目标

世界模型用 **Rectified Flow** 目标训练。令 $x_1 = z_{t+1:t+H}$ 为目标未来潜码，$x_0 \sim \mathcal{N}(0, I)$ 为噪声，采样中间时间点 $t \in [0, 1]$，训练模型 $u(\cdot; \phi)$ 预测速度：

$$\mathcal{L} = \mathbb{E}_{x_0, x_1, c, t}\left[\|u(x_t, c, t; \phi) - v_t\|^2\right]$$

其中条件 $c$ 包含首帧锚定上下文和动作。

**噪声上下文增强**：为缩小训练-推理差距，在训练时对非参考上下文潜码 $z_{t-c:t}$ 注入扩散噪声。这防止模型过度依赖精确的上下文复制，提升消耗自生成帧时的鲁棒性。

#### 3.1.4 奖励模型

真实机器人操作中，密集奖励设计不切实际。WoVR 使用**二值成功分类器**作为奖励：

$$r_{t+1} = \mathbb{I}(R_\psi(\tilde{o}_{t+1}) \geq 0.5)$$

分类器用 BCE 损失在标注的成功/失败状态上训练。这比 RISE 的进度价值函数更简单，但也更稀疏。

**对比**：

| 奖励设计 | RISE | VLA-RL | WoVR |
| --- | --- | --- | --- |
| **类型** | 密集进度值 + TD 学习 | 稀疏环境奖励 + RPRM | 二值成功分类器 |
| **标注需求** | 成功/失败 episode | 自动里程碑分割 | 成功/失败状态标注 |
| **信号密度** | 每帧 | 每步 | 每帧（但二值） |

### 3.2 幻觉感知的想象策略优化

#### 3.2.1 关键帧初始化 Rollout（KIR）

这是 WoVR 最有创见的设计。核心问题是：从初始状态 $o_0$ 开始的长程 rollout 会在早期阶段就累积预测误差，到达任务关键状态时已经"幻觉"到无法恢复的程度。

**KIR 的做法**：不总是从 $o_0$ 开始，而是从**关键帧** $o_k$ 初始化一部分 rollout，这些关键帧来自任务关键的中间状态，特别是当前策略遇到的**失败状态**附近。

**直觉**：
- 决定性的接触和纠正发生在关键状态的局部邻域
- 从 $o_0$ 出发，世界模型要先预测一段长前缀才能到达这些状态，而前缀中的误差已经足以破坏后续预测
- 从 $o_k$ 出发，有效预测深度大幅缩短，世界模型在短程内的预测质量更可靠

**视觉化理解**（论文 Fig. 5）：

- **初始状态 Rollout**：$o_0 \rightarrow \cdots \rightarrow$ 误差累积 → 幻觉成功（但真实世界失败）
- **关键帧初始化 Rollout**：$o_k \rightarrow$ 短程预测 → 正确建模失败 → 策略学到有效改进

#### 3.2.2 带掩码的 GRPO 优化

WoVR 采用 **GRPO**（Group Relative Policy Optimization）更新策略。给定一组想象轨迹 $\{\tau^{(i)}\}_{i=1}^{G}$：

$$\tau^{(i)} = \{(o_t^{(i)}, a_t^{(i)}, \hat{r}_t^{(i)})\}_{t=1}^{T}, \quad o_{t+1}^{(i)} \sim \hat{P}_\phi(\cdot | o_t^{(i)}, a_t^{(i)})$$

计算组相对优势：

$$R(\tau^{(i)}) = \sum_{t=1}^{|\tau^{(i)}|} \gamma^{t-1} \hat{r}_t^{(i)}, \quad \hat{A}^{(i)} = R(\tau^{(i)}) - \frac{1}{G}\sum_{j=1}^{G} R(\tau^{(j)})$$

**关键设计——后成功步骤掩码**：因为幻觉常在想象中达到"成功"后最为严重（世界模型不知道成功后该生成什么），WoVR 定义 $T_i^{\text{valid}}$ 为到达首次成功的有效步数，并只在有效步上计算损失：

$$J_{\text{GRPO}}(\theta) = \mathbb{E}\left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{T_i^{\text{valid}}}\sum_{t=1}^{T_i^{\text{valid}}}\min\left(\rho_t^{(i)}(\theta)\hat{A}^{(i)}, \; \text{clip}(\rho_t^{(i)}(\theta), 1-\epsilon, 1+\epsilon)\hat{A}^{(i)}\right)\right]$$

其中 $\rho_t^{(i)}(\theta) = \frac{\pi_\theta(a_t^{(i)} | o_t^{(i)}, l)}{\pi_{\theta_{\text{old}}}(a_t^{(i)} | o_t^{(i)}, l)}$。

**轨迹长度归一化**的重要性：KIR 产生的短轨迹通过 $\frac{1}{T_i^{\text{valid}}}$ 归一化，其每步贡献被放大，使得梯度以**短的、任务关键的片段**为主导，而非长的、容易漂移的延续。

**对比 RISE 的优势计算方式**：

| 维度 | RISE | WoVR |
| --- | --- | --- |
| **优势粒度** | Action chunk 级别 | 轨迹级别（组相对） |
| **幻觉处理** | 隐式（短 rollout 缓解） | 显式（后成功掩码 + KIR） |
| **RL 框架** | 优势条件化回归 | GRPO clipped objective |
| **探索机制** | 采样不同优势 bin | 组内对比多条轨迹 |

### 3.3 PACE：策略-世界模型协同进化

策略优化完全在世界模型中进行，但策略的动作分布持续进化，逐渐偏离训练初始世界模型时的数据分布。WoVR 通过 **PACE** 解决这一问题：

**具体流程**：

1. 用基础策略的 rollout 训练初始世界模型 $\text{WM}_{\text{Base}}$
2. 在 $\text{WM}_{\text{Base}}$ 中完成第一阶段策略优化
3. 用进化后的策略收集额外 rollout
4. 微调世界模型得到 $\text{WM}_{\text{Evo}}$
5. 在 $\text{WM}_{\text{Evo}}$ 中继续策略优化

**设计哲学**：PACE 只执行一次（或极低频率）的世界模型更新，与经典 Model-Based RL（如 Dyna、MBPO）的高频模型更新截然不同。这带来两个优势：

1. 不需要在策略训练期间持续进行人工监督或环境重置
2. 通过与进化策略对齐，缓解复合模型误差

**与 RISE 的对比**：RISE 的世界模型在自改进循环中完全固定不变，依赖于动力学模型的预训练质量。WoVR 的 PACE 策略更灵活——当策略演化导致动作分布偏移时，主动更新世界模型以跟上策略变化。

---

## 四、实验结果

### 4.1 世界模型质量评估（Q1）

在 LIBERO 环境中，3000 条 512 帧轨迹训练，200 条评估。所有方法使用相同的逐块自回归生成协议（4 帧上下文 + 8 帧动作块）。

| 方法 | 骨架大小 | FPS ↑ | LPIPS ↓ | FID ↓ | FVD ↓ | FloLPIPS ↓ |
| --- | --- | --- | --- | --- | --- | --- |
| EVAC | — | 2.7 | 0.146 | 46.5 | 345.8 | 0.205 |
| Cosmos-Predict2 | — | 3.5 | 0.315 | 165.9 | 275.7 | 0.265 |
| OpenSora (WMPO) | ~1.3B | 7.0 | 0.105 | 38.5 | 89.4 | 0.156 |
| **WoVR** | **~5B** | **23.0** | **0.091** | **34.3** | **68.0** | **0.154** |

以上为 512 帧 rollout 的结果。

**关键发现**：

1. WoVR 在所有指标上全面领先，且优势随 rollout 长度增加而扩大（长程稳定性更好）
2. 尽管使用了更大的骨架（~5B vs ~1.3B），WoVR 的推理速度反而最快（23 FPS vs 7 FPS），因为：
   - 仅需 **5 步扩散采样**（vs OpenSora 的更多步）
   - 使用 **3D VAE** 做时空联合潜码编码（vs 2D VAE）

### 4.2 策略优化效果（Q2）

在 LIBERO 四个任务套件上评估，每套件 10 个任务，共分配 2500 条真实环境轨迹预算。

| 方法 | 轨迹用途 | Spatial | Object | Goal | Long | **平均** |
| --- | --- | --- | --- | --- | --- | --- |
| OpenVLA-OFT-base | 基线 | 61.5 | 36.3 | 48.2 | 13.7 | 39.9 |
| GRPO (Online) | 在线交互 | 66.6 | 45.1 | 52.1 | 14.5 | 44.6 |
| WMPO | 世界模型训练 | 67.8 | 48.0 | 54.6 | 13.7 | 46.2 |
| **WoVR** | **世界模型训练** | **81.5** | **82.0** | **77.5** | **35.8** | **69.2** |

**深度分析**：

1. **GRPO (Online)** 直接在真实模拟器中用 2500 条轨迹做 on-policy 优化，每次更新需要近千条额外轨迹，sample efficiency 很低。

2. **WMPO** 证明了世界模型可以提供超越在线交互的收益，但在 **LIBERO-Long**（长时域任务）上零提升——自回归生成后期的 rollout 不稳定破坏了策略优化。

3. **WoVR** 全面大幅领先：
   - Object 套件 +45.7%（最大提升）
   - Long 套件 +22.1%（WMPO 完全失败的地方 WoVR 仍有显著提升）
   - 平均 69.2% vs 46.2%（WMPO），提升近 50%

**与 VLA-RL 的对比视角**：

VLA-RL 在 LIBERO 上用 PPO + RPRM 取得平均 81.0% 的成功率，但它使用了真实模拟器交互 + 48 GPU 小时。WoVR 的 69.2% 看似更低，但需注意：

- WoVR 的基线策略更弱（OpenVLA-OFT one-trajectory SFT, 39.9%，vs VLA-RL 的 OpenVLA SFT, 76.5%）
- WoVR 提升幅度更大（+29.3% vs VLA-RL 的 +4.5%）
- WoVR 完全不依赖真实环境交互做策略优化

### 4.3 真实机器人迁移（Q3）

在 Franka Emika Panda 机器人上测试两个接触丰富的操作任务：

| 方法 | Pick Banana | Pick Bread | **平均** |
| --- | --- | --- | --- |
| OpenVLA-OFT-base | 46.7% (14/30) | 76.7% (23/30) | 61.7% |
| **WoVR** | **93.3% (28/30)** | **90.0% (27/30)** | **91.7%** |

+30.0% 的平均提升，证明了 WoVR 优化的策略能可靠地从想象迁移到真实世界。

**与 RISE 的真实世界结果对比**：

| 维度 | RISE | WoVR |
| --- | --- | --- |
| **机器人** | AgileX 双臂 7-DoF | Franka Panda 单臂 |
| **任务难度** | 高（动态分拣、变形物体） | 中（拾取放置） |
| **代表性成功率** | 85-95% | 90-93% |
| **数据需求** | ~2500 条专家演示 + rollout | 10 条演示 + 150 条 rollout |

### 4.4 消融实验

#### 世界模型机制消融（LIBERO-Spatial, 512 帧）

| 变体 | LPIPS ↓ | FID ↓ | FVD ↓ |
| --- | --- | --- | --- |
| **WoVR（完整）** | **0.091** | **36.7** | **73.5** |
| w/o 首帧锚定 | 0.133 | 73.9 | 123.5 |
| w. 单帧上下文 | 0.120 | 64.5 | 86.0 |
| w/o 噪声上下文 | 0.099 | 44.7 | 77.3 |

**关键发现**：

- **首帧锚定影响最大**：去掉后 FID 翻倍，FVD 增加 68%。这验证了长程 rollout 中全局场景一致性的关键性。
- **噪声上下文的作用随 rollout 长度增加而增大**：短程影响温和，但在 512 帧 rollout 上差距显著——缩小了训练-推理 gap。
- **多帧记忆优于单帧**：4 帧记忆 + 首帧锚定的组合是最优配置。

#### 策略优化机制消融（LIBERO-Spatial）

| 方法 | 成功率 |
| --- | --- |
| **WoVR（完整）** | **0.815** |
| w/o KIR | 0.782（-3.3%） |
| w/o PACE | 0.710（-10.5%） |

**PACE 的影响远大于 KIR**：去掉 PACE 后成功率下降 10.5%，说明分布偏移是世界模型 RL 中最致命的问题——固定的世界模型在策略进化后迅速失去可靠性。

---

## 五、用类比总结 WoVR 的核心原理

想象你是一个象棋教练，需要训练一个 AI 下棋。

**纯模仿学习（传统 VLA）**：你把大师棋谱全部给 AI 看，让它照着学。但 AI 从没见过对手走出意外招数后该怎么应对。

**在线 RL（GRPO Online）**：你让 AI 不断与真人对弈。但找高水平对手很贵，每盘棋要花很长时间，而且 AI 犯低级错误时可能打翻棋盘（物理损坏）。

**朴素世界模型 RL（WMPO）**：你在脑子里模拟棋局让 AI 练习。但你的棋力有限——当 AI 走出你不熟悉的招式时，你会下意识地"想象"出错误的棋盘状态（幻觉）。AI 学会了利用你的错误想象赢棋（而非真正提升棋力）。

**WoVR 的做法**：
1. **稳定的世界模型**（首帧锚定）：你始终参考真实棋盘的初始布局来校准想象，防止想着想着棋盘布局就"漂移"了
2. **关键帧初始化 Rollout（KIR）**：不从开局开始想象整盘棋，而是直接从中盘的关键局面开始——AI 在残局杀法这种关键阶段的训练，比从头想象整盘棋更有效
3. **PACE 协同进化**：当 AI 学会了新招式后，你也更新自己的想象能力（用 AI 的新棋路来校准），确保你的想象跟上 AI 的进步

---

## 六、局限性与未来方向

### 6.1 幻觉并未完全消除

WoVR 减少但未完全消除幻觉，特别在**超长时域**和**高度接触敏感**的场景中。论文诚实地承认这一点。

### 6.2 依赖学习的奖励模型

二值成功分类器作为奖励信号，本身也是一个学习的组件，可能在世界模型生成的 OOD 帧上产生错误判断。相比 RISE 的双重损失价值模型和 VLA-RL 的 RPRM，WoVR 的奖励建模相对简单。

### 6.3 PACE 的频率选择

PACE 目前仅执行一次世界模型更新。论文未探讨多次迭代是否能带来进一步提升，以及更新频率与计算成本之间的最优权衡。

### 6.4 任务覆盖与通用性

真实世界实验仅覆盖两个较简单的拾取放置任务。相比 RISE 测试的动态传送带分拣、变形物体操作、精密双臂协调等高难度任务，WoVR 的真实世界验证范围有限。

---

## 七、个人思考

### 7.1 三级可靠性框架的启发性

WoVR 最大的学术贡献可能不是具体的技术方案，而是**将世界模型 RL 的可靠性问题分解为三个明确的层级**：模拟器质量、交互协议、策略-模型对齐。这个框架为后续工作提供了清晰的优化方向——每个层级都可以独立改进。

### 7.2 KIR 与课程学习的联系

KIR 本质上是一种**状态空间的课程学习**：从容易预测的短程片段开始训练（关键帧附近），比从头开始的全程 rollout 更有效。这与 VLA-RL 的任务难度课程选择异曲同工——前者是观测空间的课程，后者是任务空间的课程。

### 7.3 PACE 与 DAgger 的思想类比

PACE 让世界模型与进化策略保持对齐，本质上是 **DAgger 思想在世界模型训练上的应用**：DAgger 用当前策略的 rollout 来扩充模仿学习数据集，PACE 用进化策略的 rollout 来微调世界模型。两者都解决分布偏移问题，但作用的对象不同。

### 7.4 世界模型 RL 的"两条路线"正在成型

从 RISE 和 WoVR 可以看到世界模型 RL 的两条路线：

| 路线 | 代表 | 世界模型用途 | Rollout 长度 |
| --- | --- | --- | --- |
| **短程想象** | RISE | chunk 级别优势评估 | ~25 帧 |
| **长程仿真** | WoVR | 完整 episode 模拟 | ~512 帧 |

RISE 通过限制 rollout 长度来回避幻觉，WoVR 通过多层防御机制来应对幻觉。两种策略各有优劣：RISE 更保守但更可靠，WoVR 更激进但需要更复杂的工程。

### 7.5 推理效率的意外惊喜

WoVR 用 ~5B 的世界模型达到 23 FPS，比 ~1.3B 的 OpenSora 的 7 FPS 快 3 倍多。这说明**模型大小与推理速度不必然成反比**——合理的架构选择（3D VAE + 少步扩散采样）可以让更大的模型更快。这对计算预算受限的研究者很有参考价值。

---

## 参考

- [RISE: Self-Improving Robot Policy with Compositional World Model](https://arxiv.org/abs/2602.11075)：同期工作，用组合式世界模型（动力学 + 价值）在想象空间做 RL，思路互补
- [WMPO: World Model-Based Policy Optimization for VLA](https://arxiv.org/abs/2511.09515)：直接对标的基线，将世界模型视为标准仿真器的替代，但未显式处理幻觉问题
- [VLA-RL: Scalable Online RL for Autoregressive VLA](https://arxiv.org/abs/2505.18719)：在真实模拟器中用 PPO + RPRM 做 VLA 后训练，与 WoVR 在 LIBERO 上有直接可比性
- [RLinf: Flexible and Efficient Large-Scale RL](https://arxiv.org/abs/2509.15965)：WoVR 构建于 RLinf 框架之上，提供分布式 rollout 和训练支持
- [OpenVLA-OFT: Fine-Tuning VLA Models](https://arxiv.org/abs/2502.19645)：WoVR 的基础 VLA 策略
- [Wan: Open and Advanced Large-Scale Video Generative Models](https://arxiv.org/abs/2503.20314)：WoVR 世界模型的骨架来源
- [DeepSeek-R1 / GRPO](https://arxiv.org/abs/2501.12948)：WoVR 采用的策略优化算法 GRPO 的来源
- [LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning](https://arxiv.org/abs/2306.03310)：WoVR 的主要评估基准
