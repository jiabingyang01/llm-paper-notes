# FPO++：Flow Matching 策略梯度用于机器人控制——原理详解

> 论文：*Flow Policy Gradients for Robot Control*
> 机构：Amazon FAR、UC Berkeley、Stanford、HKU、CMU
> 作者：Brent Yi、Hongsuk Choi、Himanshu Gaurav Singh、Xiaoyu Huang、Takara E. Truong、Carmelo Sferrazza、Yi Ma、Rocky Duan、Pieter Abbeel、Guanya Shi、Karen Liu、Angjoo Kanazawa
> 发布时间：2026年2月
> [arXiv](https://arxiv.org/abs/2602.02481) | [项目主页](https://hongsukchoi.github.io/fpo-control)

---

## 一句话总结

用 conditional flow matching 损失差值近似似然比，绕开 flow 策略的显式密度计算，再通过逐样本裁剪和非对称信任域两个改进，使 flow 策略梯度方法首次在四足运动、人形动捕跟踪和操作微调等真实机器人控制任务中稳定训练并成功 sim-to-real。

---

## 一、问题与动机

### 1.1 策略梯度需要似然——flow 策略没有

基于似然的策略梯度方法（PPO 等）是机器人连续控制的主流范式。PPO 的核心公式需要计算动作的似然比：

$$\rho_\theta = \frac{\pi_\theta(a_t | o_t)}{\pi_{\theta_{\text{old}}}(a_t | o_t)}$$

对高斯策略来说，似然是解析可得的。但高斯策略是对角协方差的单峰分布，**各动作维度独立采样**，无法捕捉多模态或维度间耦合的动作分布。

**Flow 策略**（基于 flow matching 训练）可以表达任意复杂的多模态分布，已在模仿学习（Diffusion Policy、$\pi_0$ 等）中大获成功。但计算 flow 策略的精确动作似然需要对流场的体积变化做散度积分（如 FFJORD），在 RL 的高频更新场景中**计算代价过高**。

### 1.2 已有方案的问题

围绕"flow/diffusion + RL"，已有方法主要走两条路：

| 路线 | 代表方法 | 问题 |
| --- | --- | --- |
| 噪声似然 | DPPO、ReinFlow | 将去噪步视为 MDP 决策点，膨胀信用分配时域；噪声似然 $\neq$ 边际化后的动作似然 |
| 反向传播穿过展开 | NCDPO、GenPO | 需要特殊网络架构或展开计算，梯度爆炸/消失风险 |

### 1.3 FPO 的思路：完全绕过似然

FPO（Flow Policy Optimization）由 McAllister et al. 2025 提出，核心洞察是：**不需要计算真实似然，只要用 CFM 损失差值近似对数似然差值即可构造 PPO 式的代理比率**。

$$\hat{\rho}_{\text{FPO}}(\theta) = \exp\left(\hat{L}_{\text{CFM},\theta_{\text{old}}}(a_t; o_t) - \hat{L}_{\text{CFM},\theta}(a_t; o_t)\right)$$

直觉：CFM 损失越低，策略越"喜欢"这个动作，所以损失差值可以近似对数似然差值。

但原始 FPO **只在简单合成环境中验证过**，在真实机器人任务（高维动作空间、关节限位、稀疏奖励）上训练不稳定。

### 1.4 FPO++ 的贡献

FPO++ 对 FPO 做了两个简单但关键的改进：

1. **逐样本裁剪**（Per-sample ratio）：将 Monte Carlo 估计的每个 $(\tau_i, \epsilon_i)$ 样本独立裁剪，而非对整个动作的平均损失做一次裁剪
2. **非对称信任域 ASPO**：正优势动作用 PPO clipping，负优势动作用 SPO（提供回拉梯度而非截断梯度）

这两个改进使 FPO++ 在四足运动、人形 sim-to-real、操作微调三大类任务中首次稳定成功。

---

## 二、预备知识

### 2.1 PPO 的裁剪目标

PPO 通过裁剪似然比来约束策略更新幅度：

$$\psi_{\text{PPO}}(\rho_\theta, \hat{A}_t) = \min\left(\rho_\theta \hat{A}_t, \; \text{clip}(\rho_\theta, 1 \pm \varepsilon^{\text{clip}}) \hat{A}_t\right)$$

$$\max_\theta \; \mathbb{E}_{\pi_{\theta_{\text{old}}}} \left[\psi_{\text{PPO}}(\rho_\theta, \hat{A}_t)\right]$$

关键依赖：$\rho_\theta$ 需要可微的动作似然。

### 2.2 Flow 策略与 Conditional Flow Matching

Flow 策略通过学习速度场 $\hat{v}_\theta$，将标准高斯噪声 $\epsilon \sim \mathcal{N}(0, I)$ 映射到动作 $a_t$。使用线性插值路径：

$$a_t^{\tau_i} = \tau_i a_t + (1 - \tau_i)\epsilon_i$$

对应的速度场目标是 $a_t - \epsilon_i$。CFM 训练损失为：

$$\hat{L}_{\text{CFM},\theta}(a_t; o_t) = \frac{1}{N_{\text{mc}}} \sum_{i=1}^{N_{\text{mc}}} \left\|\hat{v}_\theta(a_t^{\tau_i}, \tau_i; o_t) - (a_t - \epsilon_i)\right\|_2^2$$

推理时从 $\epsilon \sim \mathcal{N}(0, I)$ 出发，用 $K$ 步 Euler 积分生成动作。

### 2.3 FPO 的代理似然比

FPO 用 CFM 损失差值构造代理比率：

$$\hat{\rho}_{\text{FPO}}(\theta) = \exp\left(\hat{L}_{\text{CFM},\theta_{\text{old}}}(a_t; o_t) - \hat{L}_{\text{CFM},\theta}(a_t; o_t)\right)$$

然后直接代入 PPO 裁剪目标：

$$\max_\theta \; \mathbb{E}_{\pi_{\theta_{\text{old}}}} \left[\psi_{\text{PPO}}(\hat{\rho}_{\text{FPO}}(\theta), \hat{A}_t)\right]$$

用大白话说：如果新策略对某个动作的 CFM 损失比旧策略更低（速度场预测更准），代理比率 $> 1$，说明新策略更"喜欢"这个动作——正好对应似然增大。

---

## 三、核心方法：FPO++

### 3.1 改进一：逐样本裁剪（Per-sample Ratio）

**原始 FPO** 对每个动作先平均 $N_{\text{mc}}$ 个 Monte Carlo 样本的损失，再算一个比率：

$$\hat{\rho}_{\text{FPO}}(\theta) = \exp\left(\frac{1}{N_{\text{mc}}} \sum_{i=1}^{N_{\text{mc}}} \left(\ell_{\theta_{\text{old}}}^{(i,t)} - \ell_\theta^{(i,t)}\right)\right)$$

问题：裁剪发生在平均之后——要么全部样本被裁，要么全部不被裁，信任域粒度太粗。

**FPO++** 为每个 $(\tau_i, \epsilon_i)$ 样本独立计算比率：

$$\hat{\rho}_{\text{FPO++}}^{(i)}(\theta) = \exp\left(\ell_{\theta_{\text{old}}}^{(i,t)} - \ell_\theta^{(i,t)}\right)$$

每个样本独立裁剪，共享同一个优势估计 $\hat{A}_t$。

直觉：on-policy 数据上两种公式的梯度完全相同（所有比率都等于 1）。但在多步梯度更新时，逐样本裁剪提供了**更细粒度的信任域**——某些 $(\tau_i, \epsilon_i)$ 对可能更新过大被裁剪，而其他对仍提供有效梯度。这实质上增大了有效 batch size，降低了梯度方差。

### 3.2 改进二：非对称信任域 ASPO

标准 PPO 裁剪的问题：当比率超出信任域时梯度**直接归零**。对负优势动作（需要降低其概率），这意味着 CFM 损失增大超过阈值后就没有梯度信号了——策略可能在这些动作上发生剧烈变化。

FPO++ 引入非对称信任域 ASPO：

- **正优势动作**（$\hat{A}_t \geq 0$）：使用标准 PPO clipping（梯度推动降低 CFM 损失）
- **负优势动作**（$\hat{A}_t < 0$）：使用 SPO 目标（梯度推动增大 CFM 损失时提供回拉力）

SPO 目标：

$$\psi_{\text{SPO}}(\rho_\theta, \hat{A}_t) = \rho_\theta \hat{A}_t - \frac{|\hat{A}_t|}{2\varepsilon^{\text{clip}}} (\rho_\theta - 1)^2$$

PPO clipping 在比率超限后梯度为零（"放任不管"），而 SPO 的二次惩罚项在比率偏离 1 时产生**回拉梯度**，将比率拉回信任域。

ASPO 的组合：

$$\psi_{\text{ASPO}}(\rho_\theta, \hat{A}_t) = \begin{cases} \psi_{\text{PPO}}(\rho_\theta, \hat{A}_t), & \hat{A}_t \geq 0 \\ \psi_{\text{SPO}}(\rho_\theta, \hat{A}_t), & \hat{A}_t < 0 \end{cases}$$

为什么非对称？将 CFM 损失理解为变分下界：
- 对负优势动作，SPO 约束防止**动作似然的激进降低**（保护熵）
- 同时防止**去噪后验与学习后验之间 KL 散度的激进增大**（稳定变分间隙）

### 3.3 FPO++ 完整目标

$$\max_\theta \; \mathbb{E}_{\pi_{\theta_{\text{old}}}} \left[\sum_{i=1}^{N_{\text{mc}}} \psi_{\text{ASPO}}\left(\hat{\rho}_{\text{FPO++}}^{(i)}(\theta), \hat{A}_t\right)\right]$$

### 3.4 Zero-Sampling 推理策略

训练时：从 $\epsilon \sim \mathcal{N}(0, I)$ 采样，通过 Euler 积分生成动作（随机探索）。

推理时：从 $\epsilon = \vec{0}$ 出发做 Euler 积分，称为 **zero-sampling**。

效果极为显著：
- G1 运动策略的 train return < 20，但 eval return > 32
- 操作任务的 base policy 成功率从 ~10% 跳到 >70%
- 允许大幅减少积分步数（50 步 → 5 步），降低部署延迟

---

## 四、实验设置

### 4.1 三大任务类别

| 类别 | 任务 | 机器人 | 训练模式 |
| --- | --- | --- | --- |
| 四足运动 | IsaacLab 速度跟踪 | Go2、Spot、H1、G1 | From scratch |
| 人形 sim-to-real | 运动跟踪 + 行走 | Booster T1、Unitree G1 | From scratch + 部署 |
| 操作微调 | Can、Square、Box Cleanup、Tray Lift、Threading | 单臂 / 双臂 | BC 预训练 + RL 微调 |

### 4.2 基线方法

| 方法 | 类型 | 核心区别 |
| --- | --- | --- |
| **Vanilla FPO** | Flow + on-policy | 原始 FPO 算法 |
| **Gaussian PPO** | 高斯 + on-policy | 标准 PPO，对角高斯策略 |
| **DPPO (fixed/learned noise)** | Diffusion + on-policy | 将去噪步建模为 MDP，计算噪声似然 |
| **ReinFlow** | Flow + on-policy | 注入噪声实现似然计算，用 PPO 微调 |

### 4.3 关键超参数

| 参数 | 运动任务 | 动捕跟踪 | 操作微调 |
| --- | --- | --- | --- |
| Flow 积分步数（训练） | 64 | 50 | 10 |
| Monte Carlo 样本数 | 16 | 16 | 8 |
| 并行环境数 | 4096 | 4096 | - |
| 裁剪参数 $\varepsilon^{\text{clip}}$ | 0.05 | 0.01 | - |
| 学习率 | $1\times10^{-4}$ | $3\times10^{-4}$ | Actor $1\times10^{-5}$ / Critic $1\times10^{-4}$ |
| 使用 ASPO | 是 | 是 | 否 |

---

## 五、实验结果

### 5.1 运动控制（From Scratch）

FPO++ 在 4 个 IsaacLab 运动环境中稳定训练并保持高回报，而 vanilla FPO 即使遍历 27 种超参组合仍频繁出现局部极小和灾难性失败。

关键发现：
- FPO++ 在所有机器人构型上训练稳定，达到并维持高回报
- 原始 FPO 在高维动作空间 + 关节限位 + 粗糙奖励函数的组合下极不稳定
- 即使加上梯度裁剪和观测归一化，FPO 仍无法稳定

**vs. Gaussian PPO**：FPO++ 在几乎所有并行环境数配置下收敛到更高回报，方差更小，表现出更好的样本效率。相同奖励函数下，FPO++ 在 Spot 上学到了更自然的"小跑"步态，而 Gaussian PPO 倾向于学"弹跳"步态——因为高斯策略的各维度独立采样难以探索到维度间耦合的对称行为。

### 5.2 人形 Sim-to-Real

FPO++ 在 Booster T1 和 Unitree G1 上实现了成功的 sim-to-real 迁移：
- T1：稳定的速度跟踪行走
- G1：跟踪 LAFAN 数据集中 6 段动捕序列（舞蹈、行走、跑步、格斗、跳跃），每段约 2.5 分钟

部署时使用 zero-sampling + 5 步积分（训练时 50 步），大幅降低延迟。

这是首次实现：(i) 不经过专家蒸馏的 flow 策略的人形 sim-to-real；(ii) 不使用显式似然的策略梯度方法的人形 sim-to-real。

### 5.3 操作微调

从 flow matching 行为克隆预训练的图像策略出发，用 FPO++ 微调 5 个操作任务：

| 任务 | FPO++ | Vanilla FPO | DPPO |
| --- | --- | --- | --- |
| Can | 最快收敛至 ~96% | 稳定但较慢 | 基策略质量低时失败 |
| Square | 最高成功率 ~60% | 较慢 | 较低 |
| Box Cleanup | ~75% | 接近 | 较低 |
| Tray Lift | ~75% | 接近 | 较低 |
| Threading | ~75% | 接近 | 较低 |

关键发现：
- FPO++ 一致达到最高成功率，收敛更快
- DPPO 方法对基策略质量敏感——当随机采样成功率低时，DPPO 在每个去噪步注入额外噪声进一步降低了到达目标的概率
- FPO++ 在微调场景中**不使用 ASPO**（因为预训练已提供良好初始化，ASPO 的熵保持反而引入不必要扰动）

### 5.4 消融实验

#### 逐样本裁剪的效果

在所有 4 个运动环境中，逐样本裁剪相比逐动作裁剪：
- 平均回报更高
- 种子间方差更小
- 效果在所有裁剪参数下一致

#### ASPO 信任域的效果

| 信任域 | Go2 | Spot | H1 | G1 |
| --- | --- | --- | --- | --- |
| PPO | 部分失败 | 部分失败 | 频繁失败 | 频繁失败 |
| SPO | 部分成功 | 部分成功 | 部分成功 | 部分成功 |
| **ASPO** | **稳定高回报** | **稳定高回报** | **稳定高回报** | **稳定高回报** |

ASPO 成功保持了策略熵：flow field 可视化显示 PPO clipping 训练的策略在收敛后出现明显的熵崩溃（动作分布收窄），而 ASPO 训练的策略即使在最终收敛状态仍保持宽广的探索分布。

#### ASPO 在微调中的局限

逐样本裁剪在所有任务中一致有益；但 ASPO 在操作微调中**降低**了性能。原因：
1. 微调任务中策略已有良好初始化，不需要额外的熵保持来促进探索
2. 预训练的 flow 策略已建模高质量动作分布，变分间隙的稳定化不那么关键

#### 梯度方差

逐样本裁剪和 ASPO 都显著提高了梯度间的余弦相似度，验证了它们降低梯度方差的机制。

---

## 六、类比总结

想象你是一个教练，在训练一队舞者。

**Gaussian PPO** 好比每个舞者只能独立地、对称地做动作（对角高斯）。你能让他们各自跳得不错，但很难编排出需要舞者之间紧密配合的协调舞步（维度间耦合）。

**Flow 策略** 好比舞者们可以通过一系列排练步骤（Euler 积分）从随机走位逐步精炼成优美队形。它能自然产生协调的编队——但有个麻烦：你（教练/PPO）需要计算"这个编队出现的概率有多大"来决定奖惩，而复杂编队的概率计算极其困难。

**FPO** 的巧妙之处在于说："我不需要算概率，只要比较新旧教练对同一个编队的'满意程度差异'（CFM 损失差值）就够了"。

**FPO++** 在此基础上做了两件事：
1. **逐样本裁剪**：不是一刀切地评价整个编队，而是对排练中的每一步（每个 Monte Carlo 样本）单独评价，有的步骤做过头了就限制住，其他步骤的反馈仍然保留
2. **ASPO**：对于教练想强化的好动作，照常鼓励；对于想弱化的坏动作，不是简单忽略（截断梯度），而是温和地"拉回来"——防止舞者因为被过度惩罚而变得畏首畏尾（熵崩溃）

---

## 七、局限性与未来方向

### 7.1 训练速度

FPO++ 的 wall-clock time 比 Gaussian PPO 更长。G1 运动任务中 Gaussian PPO 19 分钟达到 eval return 25，FPO++ 需要 23 分钟。动捕跟踪任务可达 3 倍差距。主要开销来自多步 Euler 积分和 Monte Carlo 采样。

### 7.2 动捕跟踪的 return 差距

与精心调优的 Gaussian PPO 基线相比，FPO++ 在动捕跟踪仿真中的总回报略低（但 episode 长度略长）。作者推测这源于 FPO++ 缺少显式熵正则化和 KL 自适应学习率。初步实验（Kozachenko-Leonenko 熵估计 + 近似 KL 自适应 LR）有所改善但仍有差距。

### 7.3 ASPO 的适用范围

ASPO 对 from-scratch 训练至关重要（防止熵崩溃），但在微调场景中可能有害。如何自适应地决定是否启用 ASPO 是开放问题。

### 7.4 步数蒸馏

目前训练使用 50-64 步积分，推理通过 zero-sampling 降至 5 步。进一步探索 few-step distillation 提升训练和推理效率是有价值的方向。

---

## 八、个人思考

### 8.1 与 SAC Flow 的互补

SAC Flow 和 FPO++ 都在解决"如何用 RL 训练 flow 策略"这一核心问题，但走了完全不同的路：

| 维度 | SAC Flow | FPO++ |
| --- | --- | --- |
| RL 范式 | Off-policy（SAC） | On-policy（PPO 风格） |
| 核心难题 | 梯度穿过 $K$ 步 Euler 积分爆炸 | 无法计算 flow 策略的动作似然 |
| 解决方案 | GRU/Transformer 重参数化速度网络 | CFM 损失差值近似似然比 |
| 密度计算 | 噪声增强 rollout 获得解析联合路径密度 | 完全绕过密度计算 |
| 应用场景 | MuJoCo from-scratch + offline-to-online | 四足运动 + 人形 sim-to-real + 操作微调 |

FPO++ 的优势在于**不需要修改速度网络架构**——任何已有的 flow 策略都可以直接用 FPO++ 训练。SAC Flow 的优势在于 off-policy 样本效率。两者可能在不同场景中各有优势。

### 8.2 对 VLA RL 后训练的启示

FPO++ 验证了一个重要命题：**显式似然对策略梯度方法不是必需的**。这对 VLA RL 后训练领域有直接意义——$\pi_0$ 系列等 VLA 模型使用 flow matching 生成动作，此前用 RL 微调这类模型需要解决密度计算问题（如 ReinFlow 的噪声注入、DPPO 的去噪 MDP）。FPO++ 提供了一条更简洁的路径：直接用 CFM 损失差值做 PPO 式更新。

### 8.3 Zero-sampling 的通用价值

Zero-sampling 在推理时将随机 flow 采样变为确定性的，这一简单技巧带来了巨大收益（成功率从 10% 到 70%）。这与 Diffusion Policy 社区最近关于"去噪初始化"的讨论一致——flow/diffusion 策略在训练时需要随机性探索，但推理时确定性采样通常更好。

### 8.4 ASPO 的设计哲学

ASPO 的非对称设计反映了一个深刻的不对称性：在 flow 策略的 RL 训练中，"让策略更喜欢好动作"和"让策略更不喜欢坏动作"有着不同的风险。后者如果过于激进会导致熵崩溃。这一洞察可能对其他生成式策略的 RL 训练也有参考价值。

---

## 九、参考

- McAllister et al., "Flow Matching Policy Gradients," arXiv 2507.21053, 2025. — FPO 原始论文
- Schulman et al., "Proximal Policy Optimization Algorithms," arXiv 1707.06347, 2017. — PPO
- Chi et al., "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion," IJRR, 2025. — Diffusion Policy
- Black et al., "$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control," 2024. — $\pi_0$ VLA
- Ren et al., "Diffusion Policy Policy Optimization," arXiv 2409.00588, 2024. — DPPO
- Zhang et al., "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning," arXiv 2505.22094, 2025. — ReinFlow
- Xie et al., "Simple Policy Optimization," arXiv 2401.16025, 2024. — SPO
- Liao et al., "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion," 2025. — BeyondMimic
