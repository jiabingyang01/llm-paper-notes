# GRAPE：通过偏好对齐提升 VLA 策略泛化性——原理详解

> 论文：*GRAPE: Generalizing Robot Policy via Preference Alignment*
> 机构：UNC-Chapel Hill、University of Washington、University of Chicago
> 发布时间：2024年11月（arXiv v1），2025年2月更新（v2）
> 🔗 [arXiv](https://arxiv.org/abs/2411.19309) | [PDF](https://arxiv.org/pdf/2411.19309) | [项目主页](https://grape-vla.github.io/) | [代码](https://github.com/aiming-lab/GRAPE)

---

## 一句话总结

GRAPE 提出**轨迹级偏好优化（TPO）**框架，通过 VLM 自动分解任务阶段、生成代价函数并合成偏好对，将 DPO 从单步扩展到全轨迹级别，以 plug-and-play 方式显著提升 VLA 模型在 unseen 任务上的泛化性，并支持安全、效率等多元对齐目标。

---

## 一、论文要解决什么问题？

### 1.1 行为克隆的泛化瓶颈

当前 VLA 模型（如 OpenVLA、Octo）的主流训练范式是**监督微调（SFT）**——在专家演示上做行为克隆。这种方式有两个根本问题：

- **只见成功、不见失败**：模型只学到"正确的动作是什么"，从未接触过"错误动作会导致什么后果"，缺乏对失败模式的理解
- **分布偏差**：不同专家在不同设置下采集的演示存在风格差异，模型很难从中泛化到未见过的任务和场景

结果是：VLA 模型在训练分布内的任务上表现尚可，但面对**新物体、新场景、新指令**等分布外（OOD）情况时性能急剧下降。

### 1.2 现有 RL 对齐方法的局限

自然的想法是用 RL 来弥补 SFT 的不足。LLM 领域的 RLHF/DPO 已经证明了偏好对齐的价值，但直接搬到机器人领域面临独特挑战：

- **逐步 DPO 的噪声问题**：标准 DPO 在每个 $(s_t, a_t)$ 步级别构建偏好对，但机器人操作中单步动作的好坏很难判断（一个中间步骤看起来"偏了"可能恰恰是为后续纠正做准备），步级偏好充满噪声
- **奖励设计困难**：机器人任务的奖励函数需要人工精心设计，不同任务需要不同的奖励，难以规模化
- **缺乏灵活性**：现有方法通常只优化任务成功率，无法同时兼顾安全性、效率等多元目标

### 1.3 GRAPE 的核心思路

GRAPE 的解决方案有三个关键创新：

1. **轨迹级偏好优化（TPO）**：将 DPO 从单步提升到整条轨迹级别，让模型从全局视角学习"好的轨迹 vs 坏的轨迹"，而非纠结于单步好坏
2. **VLM 驱动的自动偏好合成**：利用大型视觉语言模型自动分解任务阶段、标注关键点、生成代价函数，无需人工设计奖励
3. **多元对齐目标**：通过更换代价函数，同一框架可以对齐任务成功率、安全性、效率等不同目标

---

## 二、预备知识

### 2.1 VLA 的监督微调

VLA 模型将机器人控制建模为条件生成问题。给定视觉观测 $o$ 和语言指令 $l$，策略 $\pi_\theta$ 输出动作 $a$。监督微调的目标是最小化专家演示上的负对数似然：

$$\mathcal{L}_{\text{SFT}}(\theta) = -\mathbb{E}_{(o, l, a^*) \sim \mathcal{D}_{\text{demo}}} \left[\log \pi_\theta(a^* \mid o, l)\right]$$

这就是标准的行为克隆——模仿专家的每一步动作。

### 2.2 从 RL 视角看偏好对齐

偏好对齐的核心思想是：不直接定义奖励函数 $r$，而是通过对比"好的"和"差的"行为来隐式学习奖励。

标准的 RL 目标带有 KL 正则化约束：

$$\max_\theta \mathbb{E}_{\pi_\theta}\left[\sum_t r(o_t, a_t)\right] - \beta \cdot D_{\text{KL}}\left[\pi_\theta \| \pi_{\text{ref}}\right]$$

其中 $\pi_{\text{ref}}$ 是参考策略（通常是 SFT 后的模型），$\beta$ 控制偏离参考策略的程度。

DPO 的核心洞察是：最优策略与奖励函数之间存在**闭式解关系**，可以将奖励重参数化为策略的函数：

$$r(o, a) = \beta \log \frac{\pi_\theta(a \mid o)}{\pi_{\text{ref}}(a \mid o)} + \beta \log Z(o)$$

这样就不需要显式训练奖励模型了。

### 2.3 Bradley-Terry 偏好模型

给定一对轨迹 $(\zeta^w, \zeta^l)$（$w$ 表示 preferred，$l$ 表示 rejected），Bradley-Terry 模型假设人类偏好的概率为：

$$p(\zeta^w \succ \zeta^l) = \sigma\left(r(\zeta^w) - r(\zeta^l)\right)$$

其中 $\sigma$ 是 sigmoid 函数，$r(\zeta)$ 是整条轨迹的累积奖励。

---

## 三、方法论详解

GRAPE 的整体流程分为三个部分：

1. **轨迹级偏好优化（TPO）**——定义训练目标
2. **引导式代价偏好生成（GCPG）**——自动构建偏好数据
3. **迭代在线优化**——交替采样与训练

### 3.1 轨迹级偏好优化（TPO）

#### 3.1.1 从步级到轨迹级

标准 DPO 在每一步 $(o_t, a_t)$ 上构建偏好对，这对机器人操作来说噪声太大。GRAPE 的关键改进是将偏好提升到**整条轨迹**级别。

将 DPO 的奖励重参数化代入 Bradley-Terry 模型，并将轨迹奖励分解为各步奖励之和，得到 TPO 损失：

$$\mathcal{L}_{\text{TPO}}(\theta) = -\mathbb{E}_{(\zeta^w, \zeta^l)} \left[\log \sigma\left(\beta \sum_{t=0}^{T^w} \log \frac{\pi_\theta(a_t^w \mid o_t^w)}{\pi_{\text{ref}}(a_t^w \mid o_t^w)} - \beta \sum_{t=0}^{T^l} \log \frac{\pi_\theta(a_t^l \mid o_t^l)}{\pi_{\text{ref}}(a_t^l \mid o_t^l)}\right)\right]$$

其中 $T^w$ 和 $T^l$ 分别是 preferred 和 rejected 轨迹的长度。

#### 3.1.2 TPO 的三个优势

1. **全局决策能力**：梯度通过整条轨迹的所有 state-action 对反向传播，策略学到的是"什么样的整体行为模式更好"，而非"单步怎么做"
2. **利用失败经验**：rejected 轨迹中的失败信息被显式利用，策略同时学到"该做什么"和"不该做什么"
3. **抗噪声**：单步级别的偏好噪声在轨迹级别被平均化，信号更稳定

### 3.2 引导式代价偏好生成（GCPG）

核心问题：如何自动构建高质量的轨迹偏好对 $(\zeta^w, \zeta^l)$？

#### 3.2.1 任务阶段分解

复杂操作任务通常包含多个子阶段。GRAPE 利用 VLM（如 GPT-4o）自动将任务分解为 $M$ 个时序阶段 $\{S_1, S_2, \dots, S_M\}$。

例如，"把杯子放进柜子"可以分解为：
- $S_1$：接近并抓取杯子
- $S_2$：提起杯子移向柜子
- $S_3$：将杯子放入柜子并释放

同时，VLM 为每个阶段标注**空间关键点** $\{kp_1, kp_2, \dots\}$，作为评估该阶段完成度的参考锚点。

#### 3.2.2 代价函数生成

给定用户指定的对齐目标（如安全性、效率），GRAPE 提示 VLM 为每个阶段生成代价函数 $c_m$。代价值越低，表示与目标的一致性越高。

**外部奖励**使用指数衰减聚合各阶段的代价：

$$R_{\text{ext}}(\zeta) = \sum_{m=1}^{M} \exp(-\alpha \cdot c_m(\zeta, kp_m))$$

其中 $\alpha$ 控制代价的敏感度。指数衰减确保高代价阶段的贡献被大幅抑制——一个阶段做得很差会显著拉低整体评分。

#### 3.2.3 综合奖励

轨迹的最终排序分数由三个部分加权组成：

$$R_{\text{GCPG}}(\zeta) = \lambda_1 R_{\text{ext}}(\zeta) + \lambda_2 R_{\text{self}}(\zeta) + \lambda_3 \mathbb{I}_{\text{success}}(\zeta)$$

- **$R_{\text{ext}}$**：外部代价奖励，衡量轨迹与对齐目标的一致程度
- **$R_{\text{self}}$**：自评估奖励，利用 VLA 模型自身的对数似然对轨迹打分，反映策略自身的"信心"
- **$\mathbb{I}_{\text{success}}$**：二值成功指示器，任务是否最终完成

自评估奖励的直觉：如果策略在执行某条轨迹时一直很"自信"（对数似然高），说明这条轨迹在策略的高密度区域内；如果某条轨迹的对数似然很低，说明策略"不太确定"，这类轨迹更可能是探索性的或低质量的。

### 3.3 迭代在线优化

GRAPE 采用**迭代式**的训练流程，而非一次性优化：

**算法流程（共 $K$ 轮迭代）：**

每轮迭代包含四步：

1. **采样**：用当前策略 $\pi_\theta^{(k)}$ 在环境中 rollout，收集一批轨迹 $\{\zeta_1, \zeta_2, \dots, \zeta_N\}$
2. **评分与排序**：用 GCPG 奖励为每条轨迹打分 $R_{\text{GCPG}}(\zeta_i)$，按分数排序
3. **构建偏好对**：选取得分最高的 $m$ 条轨迹作为 preferred，得分最低的 $m$ 条作为 rejected，配对构成偏好数据
4. **TPO 优化**：用偏好数据训练策略，得到 $\pi_\theta^{(k+1)}$

每轮迭代后，策略变强 → 新的 rollout 质量更高 → 偏好对的区分度更大 → 训练信号更强，形成正向循环。

### 3.4 实现细节

- **骨干模型**：OpenVLA（7B 参数），使用 LoRA 微调
- **SFT 阶段**：学习率 $4 \times 10^{-5}$，batch size 16
- **TPO 阶段**：学习率 $2 \times 10^{-5}$，batch size 16
- **VLM 提示器**：GPT-4o，用于任务分解和代价函数生成
- **迭代轮数**：$K = 3$ 轮（实验表明 3 轮后基本收敛）
- **优化器**：AdamW

---

## 四、实验设置

### 4.1 评估环境

**仿真环境一：SimplerEnv（Google Robot）**

- 4 个 in-domain 任务（如 Pick Coke Can、Move Near、Open/Close Drawer）
- 3 类泛化测试：subject（新物体）、physical（新物理属性）、semantic（新语义指令）

**仿真环境二：LIBERO**

- 4 个任务套件：LIBERO-Spatial、LIBERO-Object、LIBERO-Goal、LIBERO-Long
- 每个套件包含 10 个任务，共 40 个任务

**真实世界实验**

- 300 次实验，覆盖 30 个任务
- 5 类 OOD 泛化测试：visual（视觉）、subject（物体）、action（动作）、semantic（语义）、language grounding（语言理解）

### 4.2 基线方法

- **Octo-SFT**：Octo 模型 + 监督微调
- **OpenVLA-SFT**：OpenVLA + 监督微调
- **OpenVLA-DPO**：OpenVLA + 步级 DPO（标准 DPO 基线）

### 4.3 评估指标

- 主要指标：**任务成功率**（每个任务 20-50 次 rollout 取平均）
- 附加指标：碰撞率（安全对齐）、平均步数（效率对齐）

---

## 五、实验结果

### 5.1 仿真环境主要结果

**SimplerEnv（Google Robot）：**

| 方法 | In-domain 平均 | Subject 泛化 | Physical 泛化 | Semantic 泛化 |
| --- | --- | --- | --- | --- |
| Octo-SFT | 较低基线 | — | — | — |
| OpenVLA-SFT | 中等基线 | — | — | — |
| **GRAPE** | **最高** | **最高** | **最高** | **最高** |

GRAPE 在 SimplerEnv 上相较 Octo-SFT 和 OpenVLA-SFT 分别提升 **131.72%** 和 **46.10%**。

**LIBERO：**

| 方法 | Spatial | Object | Goal | Long | 平均 |
| --- | --- | --- | --- | --- | --- |
| Octo-SFT | 78.2% | 80.6% | 68.2% | 50.6% | 69.4% |
| OpenVLA-SFT | 78.6% | 85.4% | 71.6% | 63.0% | 74.7% |
| OpenVLA-DPO | 69.8% | 79.4% | 64.6% | 50.2% | 66.0% |
| **GRAPE** | **84.6%** | **88.6%** | **76.8%** | **71.0%** | **80.2%** |

**关键观察：**

1. **步级 DPO 反而更差**：OpenVLA-DPO（66.0%）比 OpenVLA-SFT（74.7%）还低 8.7%，验证了步级偏好噪声对机器人任务的危害
2. **GRAPE 全面领先**：GRAPE（80.2%）比 OpenVLA-SFT 高 5.5%，比 OpenVLA-DPO 高 14.2%
3. **长时域任务提升最大**：LIBERO-Long 从 63.0% 提升到 71.0%（+8.0%），说明轨迹级偏好对长序列决策尤其有效

### 5.2 真实世界实验

| 方法 | In-domain | Visual | Subject | Action | Semantic | Language | 总平均 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Octo-SFT | 20.0% | 16.0% | 20.0% | 12.0% | 16.0% | 12.0% | 16.0% |
| OpenVLA-SFT | 45.0% | 40.0% | 36.0% | 28.0% | 24.0% | 20.0% | 32.2% |
| OpenVLA-DPO | 50.0% | 48.0% | 44.0% | 36.0% | 32.0% | 26.0% | 39.3% |
| **GRAPE** | **67.5%** | **56.0%** | **52.0%** | **48.0%** | **44.0%** | **34.0%** | **50.3%** |

**关键观察：**

1. **全类型泛化均提升**：GRAPE 在所有 5 类 OOD 泛化上都优于基线，总平均 50.3% 比 OpenVLA-DPO 高 11.0%
2. **In-domain 到 OOD 的衰减更小**：GRAPE 的 in-domain（67.5%）到最难的 language grounding（34.0%）衰减 33.5%，而 OpenVLA-SFT 衰减 25.0%（但起点低得多）
3. **轨迹级学习的泛化优势**：即使在训练时从未见过的泛化类型上，GRAPE 也展现出更强的迁移能力

### 5.3 多元对齐目标

GRAPE 的一大亮点是可以通过更换代价函数对齐不同目标：

| 对齐目标 | 碰撞率变化 | 平均步数变化 | 成功率 |
| --- | --- | --- | --- |
| 默认（任务完成） | 基线 | 基线 | 最高 |
| 安全优先 | **-37.44%** | +5.2% | 略降 |
| 效率优先 | +3.1% | **-11.15%** | 略降 |

这说明 GRAPE 的代价函数框架具备真正的灵活性——用户只需描述对齐目标，VLM 自动生成相应的代价函数，无需重新设计算法。

### 5.4 消融实验

#### 5.4.1 奖励组成部分的贡献

| 变体 | LIBERO 平均 | SimplerEnv 平均 |
| --- | --- | --- |
| 无 $R_{\text{ext}}$（去掉外部代价） | 下降显著 | 下降显著 |
| 无 $R_{\text{self}}$（去掉自评估） | 略有下降 | 略有下降 |
| 无 $\mathbb{I}_{\text{success}}$（去掉成功指示） | 下降中等 | 下降中等 |
| **完整 GRAPE** | **最高** | **最高** |

三个奖励组成部分都有贡献，其中外部代价奖励 $R_{\text{ext}}$ 最为关键。

#### 5.4.2 迭代优化的效果

| 迭代轮数 | 成功率 |
| --- | --- |
| $K = 0$（仅 SFT） | 基线 |
| $K = 1$ | 显著提升 |
| $K = 2$ | 进一步提升 |
| $K = 3$ | 接近收敛 |

第一轮迭代带来最大提升，后续迭代收益递减但仍有正贡献。

#### 5.4.3 TPO vs 步级 DPO

GRAPE（TPO）相比 OpenVLA-DPO（步级 DPO）在两个环境中平均提升 **33.14%**，直接证明了轨迹级偏好优化的优势。

---

## 六、用类比总结 GRAPE 的核心原理

想象你在学做一道复杂的菜——宫保鸡丁：

**纯行为克隆（SFT）：** 你只看了几遍大厨的录像，然后照着做。但你从没见过"油温过高该怎么补救"或者"切丁太大了后续怎么调整"，因为录像里大厨从不犯错。换一道菜你就懵了。

**步级 DPO：** 有人告诉你"这一刀切得好"、"那一刀切得差"，但每一刀的好坏取决于最终成品，单看一刀其实说不清。这些噪声反而让你更困惑。

**GRAPE 的做法：**
1. 首先请一位美食评论家（VLM）把做菜分成几个阶段：备料、炒制、调味、装盘
2. 每个阶段给出评判标准（代价函数）：备料看刀工是否均匀，炒制看火候是否合适
3. 你做了很多遍（rollout），评论家给每次**完整的做菜过程**打分
4. 对比得分最高和最低的几次尝试（偏好对），你一次性学到"整体流程该怎么组织"
5. 如果目标换成"做得快"（效率）或"不溅油"（安全），只需要换评判标准，不用改学习方法

---

## 七、局限性与未来方向

### 7.1 依赖强大的 VLM

GRAPE 的任务分解和代价函数生成依赖 GPT-4o 级别的 VLM。如果 VLM 对任务理解有偏差，生成的代价函数可能不准确，导致偏好信号有误。未来可以探索轻量级或专用的任务分解器。

### 7.2 仿真到真实的差距

论文的仿真实验在 SimplerEnv 和 LIBERO 上进行，真实世界实验虽然覆盖了 30 个任务，但都是桌面级别的操作。对于更复杂的双臂协调、柔性物体操作等场景，GRAPE 的效果有待验证。

### 7.3 迭代采样的计算成本

每轮迭代都需要大量的环境 rollout 来采集轨迹，在真实机器人上执行成本高昂。论文在仿真环境中进行迭代优化，但实际部署时如何高效地采集在线轨迹仍是开放问题。

### 7.4 骨干模型的局限

当前实验仅基于 OpenVLA（7B），GRAPE 在更大规模的 VLA（如 $\pi_0$、$\pi_{0.5}$）上的表现未知。由于 TPO 需要计算整条轨迹上每一步的对数似然，更大的模型意味着更高的计算开销。

### 7.5 代价函数的可解释性

VLM 自动生成的代价函数虽然方便，但用户难以直接验证其合理性。在安全关键场景中，代价函数的正确性至关重要。未来可以引入人类审核或形式化验证机制。

---

## 五、个人思考

### 与项目中其他论文的联系

1. **GRAPE vs RECAP / $\pi_{0.6}^*$**：两者都使用偏好/优势信号来改进 VLA，但方向不同。RECAP 用**优势条件化**（在推理时输入高优势 bin），GRAPE 用**轨迹级 DPO**直接优化策略参数。GRAPE 不需要在推理时做额外条件化，更加"透明"。

2. **GRAPE vs RISE**：RISE 在想象空间中做 RL（通过世界模型），GRAPE 在真实/仿真环境中做偏好优化。RISE 完全不需要环境交互，GRAPE 需要迭代采样。两者解决泛化问题的角度互补——RISE 侧重于解决 exposure bias，GRAPE 侧重于利用失败经验提升 OOD 泛化。

3. **GRAPE vs TGRPO**：TGRPO 也利用 LLM 生成奖励信号，但采用的是 GRPO 框架（组相对策略优化），GRAPE 采用 DPO 框架（偏好优化）。GRAPE 的"轨迹级"思路值得 GRPO 类方法借鉴。

4. **GRAPE vs SRPO**：两者都利用 VLA 模型自身的信号（GRAPE 的 $R_{\text{self}}$，SRPO 的自参照成功轨迹），但 SRPO 是在隐空间中进行，GRAPE 在动作空间中进行。

### 方法洞察

- **轨迹级 vs 步级是核心贡献**：GRAPE 最令人印象深刻的发现是步级 DPO 在机器人任务上甚至不如纯 SFT（LIBERO 上低了 8.7%），而轨迹级 TPO 则带来了显著提升。这揭示了机器人动作序列与语言 token 序列的本质差异——语言中每个 token 的偏好信号相对清晰，但机器人动作的好坏需要在更长的时间尺度上才能判断。

- **Plug-and-play 的实用价值**：GRAPE 作为 OpenVLA 的后训练方法，不修改模型架构，只需要在已有的 SFT 模型上额外做几轮偏好优化。这种即插即用的特性使其容易被社区采纳。

---

## 参考

- **OpenVLA**（Kim et al., 2024）：GRAPE 的骨干 VLA 模型，7B 参数的开源 VLA
- **DPO**（Rafailov et al., 2024）：直接偏好优化的基础方法，GRAPE 将其从步级扩展到轨迹级
- **$\pi_{0.6}^*$ / RECAP**（Physical Intelligence, 2025）：优势条件化离线 RL，另一种 VLA 偏好对齐方案
- **RISE**（OpenDriveLab, 2026）：基于世界模型的想象空间 RL，与 GRAPE 互补的泛化提升路线
- **Octo**（Ghosh et al., 2024）：另一个开源 VLA 基线模型
