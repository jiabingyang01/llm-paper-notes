# SRPO: Self-Referential Policy Optimization——自参照策略优化

> 论文：*SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models*
> 机构：复旦大学、同济大学、上海创新研究院
> 发布时间：2025年11月
> 🔗 [arXiv](https://arxiv.org/abs/2511.15605) | [GitHub](https://github.com/sii-research/siiRL) | [HuggingFace](https://huggingface.co/collections/Sylvest/srpo)

---

## 一句话总结

提出 Self-Referential Policy Optimization (SRPO)，利用模型自身成功轨迹作为参考、结合预训练世界模型的隐空间表征来为失败轨迹提供 progress-wise 奖励，消除对外部专家演示和任务特定奖励工程的依赖，仅 200 步 RL 即在 LIBERO 上达到 99.2% SOTA，泛化基准 LIBERO-Plus 提升 167%。

---

## 一、问题与动机

### 1.1 VLA-RL 的奖励稀疏困境

VLA 模型通过 RL 后训练来突破 SFT 的演示偏差上限已成共识，但当前 VLA-RL 方法面临严重的**奖励稀疏**问题：

- **GRPO 类方法**：依赖二值结果奖励（0/1 成功指示器），对失败轨迹的"部分正确"信息完全丢弃
- 机器人轨迹 rollout 计算成本远高于 LLM token 生成，失败轨迹的浪费代价更大
- 仅有结果奖励的学习信号太弱，训练效率低下

### 1.2 现有 process reward 的局限

为提供更密集的过程奖励，现有方法走了两条路，但各有缺陷：

| 方法 | 代表工作 | 局限性 |
| --- | --- | --- |
| 手工设计 PRM | TGRPO、GRAPE | 需要任务特定的阶段分解和领域知识，不可扩展 |
| 外部专家演示 | VLA-RL | 依赖高成本的专家数据，违背自主学习的目标 |

### 1.3 本文的核心洞察

> 模型自身在当前训练批次中生成的成功轨迹，可以作为"自参照"来评估失败轨迹的进度——从"如何获取专家标签"转变为"如何从自己的成功中提取 progress-wise 奖励"。

关键创新：利用预训练世界模型（V-JEPA 2）的**隐空间表征**来衡量行为相似度，而非像素级比较或通用视觉模型（ImageBind），实现任务无关、零样本可迁移的进度度量。

---

## 二、预备知识

### 2.1 问题建模

给定时刻 $t$ 的观测 $o_t$ 和目标描述 $l$，策略 $\pi_\theta(a_t | o_t, l)$ 输出动作 $a_t$。轨迹 rollout 过程：

$$o_t = O(z_t), \quad a_t \sim \pi_\theta(\cdot | o_t, l), \quad z_{t+1} \sim E(\cdot | z_t, a_t)$$

其中 $z_t$ 为环境状态（不可直接观测），$O$ 为观测函数，$E$ 为环境转移函数。

**奖励函数** $R(z_{0:T}, l)$ 评估轨迹的任务完成度，但通常只在 episode 结束时以 0/1 稀疏方式给出——这正是核心痛点。

### 2.2 GRPO 回顾

GRPO 在 trajectory level 做优势估计：对每个任务采样一组轨迹，用组内奖励的标准化（均值和方差）作为优势。但当奖励只有 0/1 时，优势估计非常粗糙——所有失败轨迹获得相同的负优势，无法区分"差一点成功"和"完全失败"。

### 2.3 OpenVLA* 基座

本文基于 OpenVLA 的改进版本 OpenVLA*，增加了：
- **Action Chunking**：预测 8 个动作 chunk
- **Parallel Decoding**：并行解码加速推理
- 保留 Llama 2 骨架和离散 action token 表示，确保可计算 action log-probability（对 policy gradient 至关重要）

---

## 三、核心方法：Self-Referential Policy Optimization

SRPO 的核心架构分为两部分：(1) 世界模型隐空间进度奖励建模；(2) 自参照策略优化。

### 3.1 世界模型进度奖励建模

#### 关键思路

利用预训练世界模型（V-JEPA 2）作为编码器 $\mathcal{W}$，将整条轨迹的观测序列编码为隐空间表征：

$$h_i = \mathcal{W}(o_{0:T}^{(i)})$$

这些隐表征自然捕获了跨环境的行为进度模式——不需要精确的环境重建，也不需要领域特定训练。

#### 成功轨迹聚类

对当前 batch 中的成功轨迹表征用 DBSCAN 聚类，得到代表性中心集合：

$$C = \text{DBSCAN}(\mathcal{S})$$

其中 $\mathcal{S} = \{o_{0:T}^{(i)}, R(z_{0:T}^{(i)}, l) = 1, \forall i\}$ 是成功轨迹的观测集合。

用聚类而非单条成功轨迹的原因：
- 一个任务可能有多种成功策略（如先放 A 再放 B，或反过来），失败轨迹应与最近的成功策略比较
- 聚类中心比单条轨迹更鲁棒，过滤掉成功轨迹中的噪声段（如抓取前的短暂偏离）

#### 进度奖励计算

每条轨迹的隐表征到最近聚类中心的 L2 距离：

$$d_i = \min\left(\left\{\|h_i - h_j\|^2; h_j \in C\right\}\right)$$

最终奖励：

$$g_i = \begin{cases} 1.0 & \text{成功轨迹} \\ \phi\left(\frac{d_i - \bar{d}}{\sigma_d}\right) & \text{失败轨迹} \end{cases}$$

其中 $\phi(\cdot)$ 为激活函数（sigmoid），将输出映射到 $(0, 1)$；$\bar{d}$ 和 $\sigma_d$ 分别为所有失败轨迹距离的均值和标准差。

用大白话说：失败轨迹在隐空间中越接近成功轨迹的聚类中心，得到的奖励越高——"差一点成功"和"完全失败"获得不同的信用分配。

### 3.2 自参照策略优化

SRPO 在 GRPO 框架上构建 clipped surrogate objective：

**概率比与优势估计**：

$$r_{i,t}(\theta) = \frac{\pi_\theta(a_t^{(i)} | o_t^{(i)}, l)}{\pi_{\theta_{old}}(a_t^{(i)} | o_t^{(i)}, l)}, \quad \hat{A}_i = \frac{g_i - \mu_g}{\sigma_g}$$

注意优势 $\hat{A}_i$ 是 trajectory-level 的（非 step-level），使用 progress-wise 奖励 $g_i$ 而非二值奖励计算组统计量。

**Clipped surrogate 目标**：

$$\mathcal{L}_{t,i}^{\text{CLIP}}(\theta) = \min(r_{i,t}(\theta) \hat{A}_i, \text{clip}(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_i)$$

**总优化目标**：

$$\mathcal{L}_{\text{SRPO}}(\theta) = \mathbb{E}_{t,i} \mathcal{L}_{t,i}^{\text{CLIP}}(\theta) + \omega(\theta)$$

其中 $\omega(\theta) = \beta D_{\text{KL}}(\pi_\theta \| \pi_{ref})$ 为 KL 正则化项，维持策略稳定性。

**组统计量**：

$$\mu_{\hat{R}} = \frac{1}{M} \sum_{j=1}^{M} \hat{R}_j, \quad \sigma_{\hat{R}} = \sqrt{\frac{1}{M} \sum_{j=1}^{M} (\hat{R}_j - \mu_{\hat{R}})^2 + \epsilon}$$

### 3.3 "自参照"的本质

SRPO 的核心设计是 **self-referential**——参照系来自模型自身：

- 参考轨迹 = 当前训练 batch 中的成功 rollout（非外部专家）
- 随着训练推进，模型能力增强，参考标准也在进化
- 这形成了一个良性循环：更好的策略 → 更多样的成功轨迹 → 更好的参考 → 更精确的进度奖励

对比外部参考方法的劣势：
- 固定的外部专家轨迹会限制策略的开放式探索
- 随着策略超越专家水平，静态参考无法提供有效的 step-by-step 进度评估

---

## 四、实验

### 4.1 实验设置

- **基准**：LIBERO（4 个 suite：Spatial、Object、Goal、Long，各 10 个任务）
- **泛化基准**：LIBERO-Plus（7 个扰动维度：相机、初始位姿、语言、光照、背景、噪声、布局）
- **基座模型**：OpenVLA*（增加 Action Chunking + Parallel Decoding）
- **基线**：SFT 方法（OpenVLA-OFT 等）、RL 方法（TGRPO、GRAPE、VLA-RL、SimpleVLA-RL、RIPT-VLA、RLinf）
- **世界模型编码器**：V-JEPA 2（大规模视频预训练的隐世界模型）
- **训练框架**：基于 SiiRL

### 4.2 LIBERO 主实验

| 方法 | Policy Input | Spatial | Object | Goal | Long | Avg |
| --- | --- | --- | --- | --- | --- | --- |
| OpenVLA*-One (1-shot SFT) | T+I | 63.6 | 54.9 | 59.6 | 17.3 | 48.9 |
| + Offline SRPO | T+I | 92.5 | 96.8 | 92.0 | 88.7 | 92.5 |
| **+ Online SRPO** | **T+I** | **98.8** | **100.0** | **99.4** | **98.6** | **99.2** |
| 对比 One-shot 提升 | | +35.2 | +45.1 | +39.8 | +81.3 | +50.3 |

关键发现：

- **在线后训练的强大效果**：仅用 one-shot SFT 初始化（48.9%），SRPO 将其提升到 99.2%，提升 103% 相对性能
- **奖励设计的优越性**：SRPO 超越仅依赖稀疏结果奖励的 SimpleVLA-RL（96.9%）和 RLinf（98.0%），也超越手工设计 process reward 的 TGRPO（80.7%）
- **有限输入下的 SOTA**：仅用第三人称视角 + 指令（T+I），超越使用多相机、本体感知、3D 数据等额外模态的方法

### 4.3 LIBERO-Plus 泛化评估

| 模型 | Camera | Robot-Init | Language | Light | Background | Noise | Layout | Total |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OpenVLA*-One | 3.2 | 14.0 | 27.6 | 25.7 | 32.7 | 6.4 | 26.4 | 19.4 |
| + Online SRPO (Zero-Shot) | 17.1 | 51.0 | 81.8 | 70.4 | 88.9 | 35.3 | 72.4 | 59.6 |
| 提升 | +13.9 | +37.0 | +54.2 | +44.7 | +56.2 | +28.9 | +46.0 | +40.2 |

Zero-Shot 场景下 SRPO 在 7 个扰动维度上全面超越基座模型，总分从 19.4% 提升到 59.6%（**+207% 相对提升**）。

使用增强数据训练后：

| 模型 | Camera | Robot-Init | Language | Light | Background | Noise | Layout | Total |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OpenVLA*-One | 12.8 | 23.0 | 30.0 | 42.0 | 49.6 | 23.3 | 34.5 | 30.7 |
| + Online SRPO | 83.4 | 62.0 | 73.6 | 97.2 | 97.7 | 85.7 | 75.2 | 82.1 |
| 提升 | +70.6 | +39.0 | +43.6 | +55.2 | +48.1 | +62.4 | +40.7 | +51.4 |

SRPO 甚至超越在 LIBERO-Plus 数据集上做 full-shot SFT 的 OpenVLA-OFT+（79.5%），说明**RL 探索的轨迹多样性优势可以弥补甚至超过更多样的输入模态**。

### 4.4 进度奖励质量评估

在 700 条成功 + 300 条失败轨迹上评估三种奖励方法：

| Method | SC | Mono | MMD | JS | SMD |
| --- | --- | --- | --- | --- | --- |
| Pixel-level | 0.125 | 0.498 | 0.274 | 0.548 | 2.100 |
| ImageBind | 0.957 | 0.837 | 0.356 | 0.408 | 18.111 |
| **SRPO (Ours)** | **0.998** | **0.992** | **0.615** | **0.572** | **188.799** |

- **SC (Spearman Correlation)**：时间相关性，SRPO 接近完美 (0.998)
- **Mono (Monotonicity)**：单调性，SRPO 0.992 远超 ImageBind 0.837
- **MMD、JS**：成功/失败分布可区分度，SRPO 全面领先
- **SMD (Standardized Mean Difference)**：188.799 vs 18.111，区分效果提升一个数量级

Pixel-level 方法仅关注最后一帧的像素变化，对长时域多子任务的进度估计失败；ImageBind 缺乏机器人物理直觉，产生波动的非单调进度信号。SRPO 通过嵌入完整轨迹到隐空间，自然捕获了物理进度规律。

### 4.5 训练效率

- LIBERO 四个 suite 的收敛步数：Spatial 79 步、Object 59 步、Goal 103 步、Long 219 步
- 相比 SFT 需要数万步训练，SRPO 仅需约 200 步 RL
- 与 GRPO 对比：SRPO 的效率斜率更陡，尤其在 Long suite 上优势明显（GRPO 效率斜率 0.003139 vs SRPO 0.003705）

### 4.6 轨迹多样性分析

SRPO 在线 RL 训练的策略相比 full-shot SFT 策略展现了两个关键优势：

1. **探索到之前不可达的区域**：末端执行器覆盖 SFT 未曾到达的空间位置
2. **更分散的轨迹分布**：倾向于空间探索而非仅拟合特定演示路径

这解释了为什么 one-shot SFT + SRPO 能超越 full-shot SFT——RL 的在线交互带来的轨迹多样性弥补了演示数量的不足。

### 4.7 真实世界实验

在 X-ARM 7 机器人上的 5 个任务（放苹果、放梨、叠毛巾、擦白板、选扑克牌），使用离线 RL（AWR + SRPO 奖励）：

- $\pi_0$ (diffusion VLA)：SFT 29.3% → + SRPO **54.7%**（+86.7%）
- $\pi_0$-FAST (autoregressive VLA)：SFT 26.7% → + SRPO **29.3%**（+9.7%）

两种不同架构（diffusion-based 和 autoregressive）上都获得一致改进，验证了方法的架构无关性和 sim-to-real 可迁移性。

---

## 五、深入分析

### 5.1 自参照 vs 外部参照消融

用固定的 50 条外部专家轨迹替换自参照机制：

- 初始阶段外部参照版本训练速度与完整 SRPO 相当（因为 progress-wise 奖励本身就比二值奖励好）
- 但后期性能增长放缓、需要 1.4 倍训练步数，最终性能仍低于自参照版本
- 原因：静态的外部参考无法为策略探索出的多样 rollout 提供精确的 step-by-step 进度评估

### 5.2 聚类 vs 最近邻消融

用单条最近成功轨迹代替聚类中心：

- 训练初期效率相当（成功策略少，聚类和单轨迹差异小）
- 训练后期差距拉大：策略探索出更多样的成功策略后，聚类能提炼出原型性的成功模式，提供更鲁棒的进度信号

### 5.3 超参数 $\alpha$ 分析

$\alpha$ 控制 progress reward 与 outcome reward 的权重平衡：

- $\alpha = 0$（纯结果奖励）：效果最差，纯结果奖励对复杂任务的顺序推理不足
- $\alpha = 0.3, 0.5$：逐步改善
- $\alpha = 0.8$（**最优**）：强但非独占地关注进度奖励
- $\alpha = 1.0$（纯进度奖励）：次优，过度强调进度会偏离最终目标

结论：进度感知和结果正确性需要平衡，80/20 分配最优。

---

## 六、局限性与未来方向

1. **世界模型质量依赖**：SRPO 的奖励质量取决于 V-JEPA 2 的表征质量。如果世界模型在特定域表征不佳，进度估计可能不准确
2. **需要 batch 内成功轨迹**：自参照机制要求当前 batch 中至少有成功轨迹。对于非常困难的任务（初始成功率极低），冷启动可能是问题
3. **离散 action token**：保留 OpenVLA 的离散化设计以获得 log-probability，但这可能牺牲连续控制的精度
4. **LIBERO 中心评估**：主要在 LIBERO 基准上验证，更多样的任务和机器人形态有待探索

---

## 七、个人思考

### 与 RLVLA 的互补视角

RLVLA (Liu et al., 2025) 系统性地证明了 PPO 是 VLA-RL 的最佳算法选择，而 SRPO 则深入解决了 PPO/GRPO 类方法共同面临的奖励稀疏问题。两者的发现高度互补：
- RLVLA 回答"用什么 RL 算法"——PPO 最优
- SRPO 回答"如何设计奖励"——自参照 progress reward 最优

### 与 VLA-RL 的对比

VLA-RL (Lu et al., 2025) 通过手工设计 Robotic PRM 解决稀疏奖励，SRPO 则完全消除了领域知识依赖。SRPO 的零样本可迁移世界模型奖励是一个更可扩展的方向——当任务种类增加时，不需要为每个任务设计 PRM。

### 与 pi0.6* 的设计理念差异

pi0.6* (Pertsch et al., 2025) 用 RECAP 做离线优势条件化 RL，其奖励也需要一定程度的工程设计（分布式价值函数 + rollout 数据分档）。SRPO 的自参照范式更优雅——"参照标准"自然从训练过程中涌现，不需要预设分档方式。

### 世界模型作为 reward model 的范式意义

SRPO 展示了一个重要趋势：**预训练世界模型不仅可以用于 imagination RL（如 RISE、WoVR），还可以作为通用的 reward model**。V-JEPA 2 的隐表征无需微调就能提供高质量的任务进度估计，这为 VLA-RL 的 reward shaping 提供了一条零工程成本的通用路径。

### 自参照的"内在动机"视角

SRPO 的自参照机制在哲学上类似于内在动机（intrinsic motivation）——模型通过与自身过去的成功比较来产生学习信号，而非依赖外部的绝对标准。这种设计天然适合持续学习场景：随着模型能力增长，"什么算进步"的标准也在同步进化。

---

## 参考

- Fei et al. 2025a. LIBERO-Plus: In-depth robustness analysis of VLA models.
- Liu et al. 2023. LIBERO: Benchmarking knowledge transfer for lifelong robot learning.
- Assran et al. 2025. V-JEPA 2: Self-supervised video models enable understanding, prediction and planning.
- Kim et al. 2024. OpenVLA: An open-source vision-language-action model.
- Shao et al. 2024. DeepSeekMath: Pushing the limits of mathematical reasoning (GRPO).
- Chen et al. 2025b. TGRPO: Fine-tuning VLA via trajectory-wise GRPO.
- Lu et al. 2025. VLA-RL: Towards masterful robotic manipulation with scalable RL.
- Liu et al. 2025. RLVLA: What can RL bring to VLA generalization?
